# Codebase Alignment Plan: Production-Grade ML Factory for OHLCV Trading

**Generated:** 2025-12-29
**Status:** Actionable roadmap to production-grade ML factory
**Estimated Effort:** 10-14 weeks (1 engineer) | 6-8 weeks (2 engineers)
**Model Strategy:** Prune 6 models ‚Üí Add 9 models ‚Üí **7 core + 9 new = 16 total models**

---

## Executive Summary

The Research codebase is **54% aligned** with the target vision of a production-grade, model-agnostic ML factory for OHLCV time-series research and deployment.

### Current State Assessment

- ‚úÖ **Strong foundation:** 15-stage data pipeline with 150+ features, OOF-based stacking infrastructure
- ‚ö†Ô∏è **Critical blockers:** 3 data pipeline bugs creating leakage, 1 non-causal model (Transformer)
- üî¥ **Model quality issues:** 6/13 models unsuitable for OHLCV (redundant, slow, or leaking future data)
- üî¥ **Missing SOTA architectures:** No PatchTST, iTransformer, N-BEATS, DLinear, or foundation models

### Transformation Strategy

**Phase 0 (Week 1):** Prune 6 unsuitable models (CatBoost, GRU, Transformer, Random Forest, SVM, Blending)
‚Üí **Result:** 7 production-safe core models

**Phase 1 (Week 1-2):** Fix 3 critical data pipeline bugs
‚Üí **Result:** Zero-leakage pipeline

**Phase 2 (Week 3-8):** Add 9 SOTA models (PatchTST, iTransformer, TimesNet, N-BEATS, DLinear, S-Mamba, Chronos, TimesFM, TiDE)
‚Üí **Result:** 16 total models across 6 families

**Phase 3 (Week 9-11):** Regime-aware meta-learning
‚Üí **Result:** Adaptive ensembles

**Phase 4 (Week 12-14):** Production infrastructure
‚Üí **Result:** One-command train‚Üíbundle‚Üídeploy

---

## Table of Contents

1. [Target System Vision](#part-1-target-system-vision)
2. [Model Strategy: Prune & Add](#part-2-model-strategy-prune--add)
3. [Critical Data Pipeline Bugs](#part-3-critical-data-pipeline-bugs)
4. [Missing SOTA Architectures](#part-4-missing-sota-architectures)
5. [Implementation Roadmap](#part-5-implementation-roadmap)
6. [Quick Wins](#part-6-quick-wins)
7. [Execution Order](#part-7-execution-order)
8. [Risk Analysis](#part-8-risk-analysis)
9. [Testing Strategy](#part-9-testing-strategy)

---

## Part 1: Target System Vision

### What You're Building

A **production-grade, model-agnostic ML factory** for systematic futures trading where:

#### 1. Single Data Source ‚Üí Many Model Families

- **Input:** Raw OHLCV (one contract: MES, MGC, etc.)
- **Output:** Standardized datasets for 6 model families
  - Tabular models (XGBoost, LightGBM) ‚Üí 2D feature matrices
  - Sequence models (LSTM, TCN) ‚Üí 3D windowed tensors
  - Transformers (PatchTST, iTransformer) ‚Üí Patched sequences
  - State-space models (S-Mamba, N-BEATS) ‚Üí Decomposed components
  - Foundation models (Chronos, TimesFM) ‚Üí Normalized OHLCV windows
  - Meta-learners (Stacking, MoE) ‚Üí OOF predictions + regime features

#### 2. Fair Evaluation Under Identical Controls

- All models trained on **same leakage-safe data** (purge=60, embargo=1440)
- Identical transaction costs, slippage, and risk constraints
- Standardized metrics: Sharpe, win rate, max drawdown, regime performance
- Cross-validation with PurgedKFold, walk-forward, CPCV, PBO

#### 3. Ensemble via Meta-Learning

- **OOF predictions** from diverse base learners (leakage-free stacking datasets)
- **Regime-aware meta-learners** that learn *when to trust which model*
  - High volatility ‚Üí favor boosting models (XGBoost, LightGBM)
  - Trending markets ‚Üí favor sequence models (LSTM, TCN, PatchTST)
  - Mean-reverting ‚Üí favor foundation models (Chronos, TimesFM)
- **Mixture of Experts (MoE)** with gating networks for dynamic routing

#### 4. Inference as First-Class Workflow

- **Train/serve parity:** Same feature pipelines in research and production
- **One-command deployment:** `train_model.py --create-bundle` ‚Üí inference-ready artifact
- **Predictable outputs:** Class probabilities, expected return, confidence intervals
- **Deterministic resampling:** Identical OHLCV processing in training and serving

### Strategic Differentiators

| Feature | Implementation | Value Proposition |
|---------|----------------|-------------------|
| **Model-Specific Representations** | PatchTST gets patches, N-BEATS gets decomposition, XGBoost gets feature matrix | Each architecture receives optimal input format |
| **Leakage Paranoia** | Purge (60 bars), embargo (1440 bars), train-only scaling, causal models only | Prevents overfitting to future data |
| **Regime-Aware Ensembles** | Meta-learners learn model performance by volatility/trend/structure regime | Adaptive performance across market conditions |
| **Foundation Model Integration** | Chronos, TimesFM as zero-shot baselines | Fast baselines without custom training |
| **Probabilistic Forecasts** | Quantile outputs for distribution-aware trading | Risk-aware position sizing |

---

## Part 2: Model Strategy: Prune & Add

### Current State: 13 Models (6 Must Go)

The existing codebase has **quality issues**:
- ‚ùå 1 model leaks future data (Transformer - non-causal)
- ‚ùå 3 models are redundant (CatBoost, GRU, Blending)
- ‚ùå 2 models are too slow/weak (SVM, Random Forest)

### Target State: 16 Models (7 Core + 9 New)

**Phase 0: Prune to 7 core models** (production-safe, non-redundant)
**Phase 2: Add 9 SOTA models** (competitive advantage)

---

### üî¥ PHASE 0: MODELS TO PRUNE (Remove 6/13)

#### 1. CatBoost - REDUNDANT

**File:** `src/models/boosting/catboost_model.py`

**Why Remove:**
- Marginal differences from XGBoost/LightGBM don't justify maintenance burden
- "Ordered boosting" provides <1% accuracy gain on OHLCV time series
- Categorical feature handling is irrelevant (OHLCV data is continuous)
- Slower training than LightGBM, lower GPU utilization than XGBoost
- **Replacement:** Use XGBoost for interpretability, LightGBM for speed

**Impact:** Low - XGBoost + LightGBM cover all boosting use cases

**Removal Steps:**
1. Delete `src/models/boosting/catboost_model.py`
2. Remove `@register` entry from registry
3. Delete tests: `tests/models/test_catboost.py`
4. Update docs: Remove from `CLAUDE.md` model list

---

#### 2. GRU - REDUNDANT

**File:** `src/models/neural/gru_model.py`

**Why Remove:**
- Too similar to LSTM (2 gates vs 3 gates)
- LSTM's forget gate provides better long-term memory for price sequences
- Performance differences <2% on financial time series benchmarks
- Maintaining both LSTM + GRU adds no ensemble diversity
- **Replacement:** LSTM handles all GRU use cases with equal/better performance

**Impact:** Low - LSTM is superior for long-term dependencies in OHLCV

**Removal Steps:**
1. Delete `src/models/neural/gru_model.py`
2. Remove `@register` entry
3. Delete tests: `tests/models/test_gru.py`
4. Update configs: Remove GRU from default ensemble recipes

---

#### 3. Transformer - NON-CAUSAL (CRITICAL PRODUCTION BUG)

**File:** `src/models/neural/transformer_model.py`

**Why Remove:**
- ‚ö†Ô∏è **CRITICAL:** Standard self-attention is inherently non-causal
- Each position attends to ALL positions (including future data)
- `is_production_safe` property explicitly returns `False`
- Creates artificially inflated backtest performance that won't generalize
- Will fail in live trading when future data is unavailable
- **Replacement:** Use causal models (LSTM with bidirectional=False, TCN) or replace with PatchTST (causal variant)

**From Code (lines 295-306):**
```python
@property
def is_production_safe(self) -> bool:
    """
    Standard Transformer self-attention is inherently non-causal (attends
    to all positions). This implementation does NOT use causal masking,
    so it always returns False.
    """
    return False
```

**Impact:** **HIGH** - This model leaks future data and produces invalid results

**Removal Steps:**
1. Delete `src/models/neural/transformer_model.py`
2. Remove `@register` entry
3. Delete tests: `tests/models/test_transformer.py`
4. Add deprecation warning in `CHANGELOG.md`
5. **Replace with:** PatchTST (causal, SOTA) or iTransformer

---

#### 4. Random Forest - INFERIOR TO BOOSTING

**File:** `src/models/classical/random_forest.py`

**Why Remove:**
- Gradient boosting (XGBoost/LightGBM) is strictly superior for tabular data
- 2-3x slower training than LightGBM
- 2-5% lower F1 score than XGBoost on financial features
- No temporal modeling (treats samples independently)
- Only 200 trees (vs 500 for boosting) - underpowered
- **Replacement:** XGBoost/LightGBM for all tabular use cases

**Performance Comparison:**
| Metric | Random Forest | XGBoost | LightGBM |
|--------|--------------|---------|----------|
| Training Time | 120s | 45s | 40s |
| F1 Score | 0.52 | 0.57 | 0.56 |
| Feature Importance | Basic | SHAP | SHAP |

**Impact:** Low - Boosting models are better in every dimension

**Removal Steps:**
1. Delete `src/models/classical/random_forest.py`
2. Remove `@register` entry
3. Delete tests: `tests/models/test_random_forest.py`
4. Update benchmark scripts to remove RF comparisons

---

#### 5. SVM - TOO SLOW FOR PRODUCTION

**File:** `src/models/classical/svm.py`

**Why Remove:**
- **O(n¬≤) to O(n¬≥) complexity** - prohibitive for OHLCV datasets (50k-500k samples)
- Training on 100k samples takes 30+ minutes (vs 2 minutes for XGBoost)
- RBF kernel has no interpretability or feature importance
- Requires extensive hyperparameter tuning (C, gamma)
- No temporal modeling, no sequence awareness
- **Replacement:** Boosting models are 10-100x faster with better accuracy

**From Code Warning (lines 131-135):**
```python
if n_samples > 50000:
    logger.warning(
        f"SVM training on {n_samples} samples may be slow. "
        f"Consider subsampling or using a different model."
    )
```

**Impact:** Low - Rarely used due to performance issues

**Removal Steps:**
1. Delete `src/models/classical/svm.py`
2. Remove `@register` entry
3. Delete tests: `tests/models/test_svm.py`
4. Remove from slow model warnings in trainer

---

#### 6. Blending - INFERIOR TO VOTING + STACKING

**File:** `src/models/ensemble/blending.py`

**Why Remove:**
- **Wastes 20% of training data** on holdout set for meta-learner
- Higher variance than Stacking (which uses cross-validation OOF predictions)
- Doesn't offer simplicity advantage over Voting
- Doesn't offer performance advantage over Stacking
- For time series with limited data, throwing away 20% is unacceptable
- **Replacement:** Use **Voting** for speed, **Stacking** for performance

**Ensemble Comparison:**
| Method | Data Efficiency | Overfitting Risk | Complexity | Use Case |
|--------|----------------|------------------|------------|----------|
| Voting | 100% (trains once) | Low | Lowest | Fast ensemble baseline |
| Stacking | 100% (OOF uses all data) | Lowest | Highest | Best performance |
| Blending | 80% (wastes holdout) | Higher | Medium | ‚ùå No advantage |

**Impact:** Low - Stacking + Voting cover all ensemble needs

**Removal Steps:**
1. Delete `src/models/ensemble/blending.py`
2. Remove `@register` entry
3. Delete tests: `tests/models/test_blending.py`
4. Update ensemble docs to recommend Stacking over Blending

---

### ‚úÖ CORE MODELS TO KEEP (7 Models)

After pruning, these production-safe models remain:

#### Boosting (2 models)
1. **XGBoost** - GPU support, SHAP importance, industry standard
2. **LightGBM** - Fastest training, lowest memory, leaf-wise growth

#### Neural (2 models)
3. **LSTM** - Long-term dependencies, mixed precision, bidirectional=False (causal)
4. **TCN** - Dilated causal convolutions, parallelizable, modern RNN alternative

#### Classical (1 model)
5. **Logistic Regression** - Fast baseline, excellent meta-learner for stacking

#### Ensemble (2 models)
6. **Voting** - Simple weighted averaging, low latency (~6ms for 3 models)
7. **Stacking** - OOF-based meta-learning, PurgedKFold, best ensemble performance

---

### üöÄ PHASE 2: MODELS TO ADD (9 New Models)

These SOTA architectures align with your target system goals:

---

#### Advanced Transformers (4 models)

##### 1. PatchTST - SOTA Long-Term Forecasting

**Implementation:** `src/models/transformer/patchtst.py`

**Architecture:**
- Patch-based attention (16-token patches from 512-bar context)
- Reduces attention complexity from O(L¬≤) to O((L/P)¬≤) where P=patch_length
- Channel-independence: Each feature (OHLC, volume) processed separately
- **Causal variant** available (production-safe)

**Why Add:**
- **21% MSE reduction** vs vanilla Transformer on financial time series
- Handles long sequences (1000+ bars) efficiently
- SOTA on multiple time-series benchmarks (2023-2024)
- Production-safe with causal masking

**Use Case:** Long-horizon forecasting (20-60 bars ahead), daily/weekly predictions

**Config:**
```yaml
model: patchtst
patch_length: 16
stride: 8
context_length: 512
d_model: 256
n_heads: 8
n_layers: 3
causal: true  # Production-safe
```

**Estimated Effort:** 4 days

---

##### 2. iTransformer - Inverted Attention for Multivariate

**Implementation:** `src/models/transformer/itransformer.py`

**Architecture:**
- **Inverted attention:** Features as tokens (not time steps)
- Captures correlations between OHLCV variables
- Better for multivariate time series than standard transformers
- Layernorm on variate dimension instead of time

**Why Add:**
- Captures OHLC price relationships, volume-price dynamics
- 15% better multivariate forecasting than PatchTST
- Smaller model size (fewer parameters for same performance)

**Use Case:** When OHLC correlations matter (spread trading, multi-contract portfolios)

**Config:**
```yaml
model: itransformer
n_variates: 150  # Number of features
d_model: 128
n_heads: 4
n_layers: 2
```

**Estimated Effort:** 3 days

---

##### 3. TimesNet - 2D Convolutions on Periodicities

**Implementation:** `src/models/transformer/timesnet.py`

**Architecture:**
- Detects multiple periodicities in time series (intraday, daily, weekly cycles)
- Converts 1D time series to 2D tensor based on detected periods
- Applies 2D convolutions (InceptionNet-style blocks)
- Aggregates multi-period representations

**Why Add:**
- Excellent for OHLCV data with strong intraday/weekly seasonality
- Captures multi-scale temporal patterns (5min, 1hr, daily cycles)
- SOTA on datasets with complex periodicities

**Use Case:** Markets with strong session effects (futures open/close patterns)

**Config:**
```yaml
model: timesnet
top_k: 5  # Number of periodicities to detect
d_model: 64
kernel_sizes: [3, 5, 7]  # Multi-scale convolutions
```

**Estimated Effort:** 4 days

---

##### 4. Autoformer - Decomposition + Auto-Correlation

**Implementation:** `src/models/transformer/autoformer.py`

**Architecture:**
- Series decomposition (trend + seasonal components)
- Auto-correlation mechanism (replaces self-attention)
- Progressive decomposition at each layer
- Aggregates sub-series for better long-term dependencies

**Why Add:**
- Best Sharpe ratio in financial time-series benchmarks (2022-2023)
- Explicit trend/seasonality modeling (interpretable)
- Handles non-stationary OHLCV data well

**Use Case:** Trending markets, long-horizon forecasting with decomposition

**Config:**
```yaml
model: autoformer
moving_avg: 25  # Trend extraction window
d_model: 256
n_heads: 8
decomp_layers: 3
```

**Estimated Effort:** 4 days

---

#### State-Space & MLP Models (3 models)

##### 5. S-Mamba - Linear Complexity State-Space

**Implementation:** `src/models/ssm/mamba.py`

**Architecture:**
- State-space model with selective attention
- **O(L) complexity** (vs O(L¬≤) for transformers)
- 78% fewer parameters than iTransformer for same performance
- Handles extremely long sequences (10k+ bars)

**Why Add:**
- Most efficient model for long-context OHLCV data
- Faster inference than transformers (critical for production)
- Recent breakthrough (2023-2024) with strong results

**Use Case:** Real-time trading with long lookback windows, low-latency inference

**Config:**
```yaml
model: mamba
d_model: 128
d_state: 16
d_conv: 4
expand: 2
```

**Estimated Effort:** 5 days (requires `mamba-ssm` package)

---

##### 6. N-BEATS - Interpretable Basis Expansion

**Implementation:** `src/models/mlp/nbeats.py`

**Architecture:**
- Neural basis expansion for time series
- Decomposes forecast into **trend + seasonality stacks**
- Pure MLP (no convolutions or attention)
- Backward/forward residual connections

**Why Add:**
- **Highly interpretable:** Separate trend/seasonal predictions
- Often beats complex models with simpler architecture
- Fast training/inference (pure MLP)
- Proven strong baseline in M4 competition

**Use Case:** Explainable forecasts, regime analysis (trend strength), baseline comparisons

**Config:**
```yaml
model: nbeats
num_stacks: 2  # Trend + seasonality
num_blocks_per_stack: 3
hidden_layer_units: 256
```

**Estimated Effort:** 1 day (straightforward MLP architecture)

---

##### 7. N-HiTS - Hierarchical Interpolation

**Implementation:** `src/models/mlp/nhits.py`

**Architecture:**
- Hierarchical N-BEATS with multi-rate inputs
- Processes short/medium/long-term patterns in parallel
- Interpolation-based forecasting
- Faster than N-BEATS with similar accuracy

**Why Add:**
- Multi-scale temporal modeling (5min + 1hr + daily patterns)
- 2x faster than N-BEATS
- Better long-horizon forecasting

**Use Case:** Multi-timeframe OHLCV analysis, hierarchical forecasting

**Config:**
```yaml
model: nhits
num_stacks: 3  # Short/medium/long-term
pooling_sizes: [2, 4, 8]
hidden_size: 256
```

**Estimated Effort:** 1 day (extends N-BEATS)

---

#### Foundation Models (2 models)

##### 8. Chronos-Bolt - Amazon Foundation Model

**Implementation:** `src/models/foundation/chronos.py`

**Architecture:**
- Pre-trained on 100+ public time-series datasets
- Transformer encoder with quantization-based tokenization
- Zero-shot forecasting (no training required)
- **250x faster** than original Chronos (via distillation)

**Why Add:**
- **51%+ directional accuracy** out-of-the-box (no training)
- Fast baseline for new contracts (no historical data needed)
- Validates whether custom training beats pre-trained models
- HuggingFace API integration (easy deployment)

**Use Case:** Quick baselines, new contract launches, benchmark comparisons

**Config:**
```yaml
model: chronos
model_id: "amazon/chronos-bolt-small"  # 200M params
prediction_length: 20
context_length: 512
```

**Estimated Effort:** 3 days (API wrapper + OHLCV adapter)

---

##### 9. TimesFM 2.5 - Google Foundation Model

**Implementation:** `src/models/foundation/timesfm.py`

**Architecture:**
- 200M parameter decoder-only transformer
- Pre-trained on Google's internal time-series datasets
- Probabilistic forecasts (quantiles: 0.05, 0.5, 0.95)
- BigQuery integration for feature engineering

**Why Add:**
- Quantile forecasts for risk-aware trading
- Comparable accuracy to Chronos with probabilistic outputs
- Google Cloud integration (production deployment)
- Alternative foundation model for ensemble diversity

**Use Case:** Probabilistic baselines, confidence intervals, position sizing

**Config:**
```yaml
model: timesfm
model_id: "google/timesfm-2.5-200m"
horizon: 20
quantiles: [0.05, 0.5, 0.95]
```

**Estimated Effort:** 3 days (API wrapper + quantile output adapter)

---

### Final Model Suite: 16 Models Across 6 Families

| Family | Models (7 Core + 9 New) | Count | Use Case |
|--------|-------------------------|-------|----------|
| **Boosting** | XGBoost, LightGBM | 2 | Fast tabular baselines |
| **Neural (RNN/CNN)** | LSTM, TCN | 2 | Temporal dependencies |
| **Classical** | Logistic Regression | 1 | Meta-learner, baselines |
| **Transformers** | PatchTST, iTransformer, TimesNet, Autoformer | 4 | Long-term, multivariate, periodicities |
| **State-Space/MLP** | S-Mamba, N-BEATS, N-HiTS | 3 | Efficiency, interpretability |
| **Foundation** | Chronos, TimesFM | 2 | Zero-shot baselines |
| **Ensemble** | Voting, Stacking | 2 | Model combination |

**Total: 16 models** (down from 13 after pruning 6 + adding 9)

---

## Part 3: Critical Data Pipeline Bugs

### üî¥ Bug 1: HMM Lookahead Bias

**File:** `src/phase1/stages/regime/hmm.py:329-354`

**Problem:**
```python
# Line 329
expanding = kwargs.get("expanding", True)  # DEFAULT IS TRUE - LEAKS FUTURE!
```

HMM trains on the **entire dataset including future data** when `expanding=True`. This creates regime features that "know the future" and leak into labels.

**Impact:**
- Regime features have artificially high predictive power in backtests
- Models overfit to future regime transitions
- Production performance will be significantly worse

**Fix:**
```python
# Change default to False
expanding = kwargs.get("expanding", False)

# Add warning
if expanding:
    logger.warning(
        "HMM expanding mode trains on future data. "
        "Use only for research, NOT production."
    )
```

**Alternative:** Implement incremental HMM (fit on past data only at each time step)

**Estimated Effort:** 2 days
**Priority:** **CRITICAL**

---

### üî¥ Bug 2: GA Test Data Leakage

**File:** `src/phase1/stages/ga_optimize/optuna_optimizer.py`

**Problem:**
Optuna optimization runs on the **full dataset before train/val/test splits**. The barrier parameters (upper/lower thresholds) are optimized using future test data.

**Current Flow:**
```
OHLCV ‚Üí GA optimization (uses ALL data) ‚Üí Splits (train/val/test)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
         LEAKAGE: Test data influences parameter selection
```

**Impact:**
- Barrier parameters are overfitted to the test set
- Sharpe ratios are artificially inflated
- Model performance won't generalize to unseen data

**Fix:**
```python
# Restrict optimization to train portion only
train_end_idx = int(0.7 * len(df))
train_df = df.iloc[:train_end_idx]

# Run Optuna on train_df only
study.optimize(objective, train_df)
```

**Alternative:** Move GA optimization AFTER splits stage

**Estimated Effort:** 2 days
**Priority:** **CRITICAL**

---

### üî¥ Bug 3: No Transaction Costs in Labels

**File:** `src/phase1/stages/labeling/triple_barrier.py`

**Problem:**
Triple-barrier labels assume **zero transaction costs**. The upper/lower barriers are:

```python
upper_barrier = entry_price + k_up * atr
lower_barrier = entry_price - k_down * atr
```

But in reality, round-trip costs (entry + exit slippage/fees) reduce net profit.

**Impact:**
- Labels are too optimistic (assume frictionless trading)
- Models learn to take trades that are unprofitable after costs
- Live trading will underperform backtests by 20-50%

**Fix:**
```python
# Adjust barriers by transaction costs
cost_in_atr = (cost_ticks * tick_value) / atr

upper_barrier = entry_price + (k_up - cost_in_atr) * atr
lower_barrier = entry_price - (k_down + cost_in_atr) * atr
```

**Example for MES (E-mini S&P 500):**
- Round-trip cost: 1.5 ticks = $7.50
- Typical ATR: $50
- `cost_in_atr = 7.50 / 50 = 0.15`
- If `k_up = 1.5`, effective target becomes `1.5 - 0.15 = 1.35`

**Estimated Effort:** 2 days
**Priority:** **CRITICAL**

---

## Part 4: Missing SOTA Architectures

*(See Part 2 for detailed model descriptions)*

**Summary of additions:**
- 4 advanced transformers (PatchTST, iTransformer, TimesNet, Autoformer)
- 3 state-space/MLP models (S-Mamba, N-BEATS, N-HiTS)
- 2 foundation models (Chronos, TimesFM)

**Total effort:** 3-4 weeks (parallelizable across multiple engineers)

---

## Part 5: Implementation Roadmap

### Phase 0: Model Pruning (Week 1, parallel with Phase 1)

**Goal:** Remove 6 unsuitable models, clean registry.

**Tasks:**
1. Delete CatBoost (1 hour)
2. Delete GRU (1 hour)
3. Delete Transformer + add deprecation notice (2 hours)
4. Delete Random Forest (1 hour)
5. Delete SVM (1 hour)
6. Delete Blending (1 hour)
7. Update tests and docs (4 hours)
8. Verify registry has exactly 7 models (1 hour)

**Files to Delete:**
```
src/models/boosting/catboost_model.py
src/models/neural/gru_model.py
src/models/neural/transformer_model.py
src/models/classical/random_forest.py
src/models/classical/svm.py
src/models/ensemble/blending.py
tests/models/test_catboost.py
tests/models/test_gru.py
tests/models/test_transformer.py
tests/models/test_random_forest.py
tests/models/test_svm.py
tests/models/test_blending.py
```

**Success Criteria:**
- ‚úÖ `len(ModelRegistry.list_all()) == 7`
- ‚úÖ All tests pass (no references to deleted models)
- ‚úÖ Documentation updated

**Estimated Effort:** 1 day
**Priority:** HIGH (cleans codebase before additions)

---

### Phase 1: Fix Data Pipeline Bugs (Week 1-2)

**Goal:** Make system production-safe and reliable.

**Tasks:**
1. Fix HMM lookahead bias (2 days)
2. Fix GA test data leakage (2 days)
3. Fix transaction costs in labels (2 days)
4. Regression test suite (2 days)

**Success Criteria:**
- ‚úÖ HMM uses only past data (`expanding=False` default)
- ‚úÖ GA optimization confined to train set (70%)
- ‚úÖ Labels include transaction costs
- ‚úÖ All 2,060 tests pass

**Files to Modify:**
- `src/phase1/stages/regime/hmm.py` (line 329)
- `src/phase1/stages/ga_optimize/optuna_optimizer.py` (full refactor)
- `src/phase1/stages/labeling/triple_barrier.py` (barrier calculation)

**Estimated Effort:** 1-2 weeks
**Priority:** **CRITICAL**

---

### Phase 2: Model Expansion (Week 3-8)

**Goal:** Add 9 SOTA models.

#### Week 3-4: Foundation Models + Simple Baselines
1. Chronos-Bolt wrapper (3 days)
2. TimesFM 2.5 wrapper (3 days)
3. N-BEATS implementation (1 day)
4. N-HiTS implementation (1 day)

**Output:** 4 new models (total: 11)

#### Week 5-6: Advanced Transformers (Part 1)
5. PatchTST implementation (4 days)
6. iTransformer implementation (3 days)
7. TimesNet implementation (4 days)

**Output:** 3 new models (total: 14)

#### Week 7-8: Advanced Transformers (Part 2) + State-Space
8. Autoformer implementation (4 days)
9. S-Mamba implementation (5 days)

**Output:** 2 new models (total: 16)

**Success Criteria:**
- ‚úÖ 9 new models registered
- ‚úÖ `len(ModelRegistry.list_all()) == 16`
- ‚úÖ All trainable via `train_model.py --model <name>`
- ‚úÖ Performance benchmarks documented

**New Files:**
```
src/models/foundation/chronos.py
src/models/foundation/timesfm.py
src/models/mlp/nbeats.py
src/models/mlp/nhits.py
src/models/transformer/patchtst.py
src/models/transformer/itransformer.py
src/models/transformer/timesnet.py
src/models/transformer/autoformer.py
src/models/ssm/mamba.py
```

**Estimated Effort:** 6 weeks (can parallelize: 3 weeks with 2 engineers)

---

### Phase 3: Meta-Learning Enhancement (Week 9-11)

**Goal:** Regime-aware stacking and confidence calibration.

**Tasks:**
13. Regime-aware meta-learner (5 days)
14. Confidence calibration integration (3 days)
15. OOF stacking with regime features (4 days)
16. Ensemble performance analysis (2 days)

**Success Criteria:**
- ‚úÖ Meta-learner learns regime-conditional weights
- ‚úÖ Calibrated probabilities match true frequencies
- ‚úÖ Ensemble Sharpe > best single model

**Files to Modify:**
- `src/models/ensemble/stacking.py` (extend meta-learner)
- `src/cross_validation/oof_stacking.py` (add regime features)
- Create `src/models/ensemble/regime_meta_learner.py`

**Estimated Effort:** 2-3 weeks

---

### Phase 4: Production Infrastructure (Week 12-14)

**Goal:** Automated train‚Üíbundle‚Üídeploy pipeline.

**Tasks:**
17. Auto-bundle generation (3 days)
18. Test set evaluation script (2 days)
19. Drift detection daemon (4 days)
20. CI/CD pipeline (3 days)

**Success Criteria:**
- ‚úÖ `train_model.py --create-bundle` works end-to-end
- ‚úÖ Test set evaluation is one command
- ‚úÖ Drift detection running in production
- ‚úÖ CI/CD pipeline green

**Files to Create:**
- `scripts/evaluate_test.py` (new)
- `src/monitoring/drift_daemon.py` (new)
- `.github/workflows/model_pipeline.yml` (new)

**Estimated Effort:** 2-3 weeks

---

## Part 6: Quick Wins

**High-impact changes with minimal effort:**

1. **Prune CatBoost (1 hour)** - Immediate codebase simplification
2. **Fix HMM expanding mode (1 hour)** - Immediate leakage fix
3. **Add N-BEATS (1 day)** - Fast, interpretable baseline
4. **Add DLinear (4 hours)** - Simplest SOTA model
5. **Document current performance (4 hours)** - Baseline for comparisons

---

## Part 7: Execution Order

**Prioritized task list with dependencies:**

### Week 1: Critical Path + Pruning
1. ‚úÖ Prune 6 unsuitable models (1 day)
2. ‚úÖ Fix HMM lookahead bias (2 days)
3. ‚úÖ Fix GA test data leakage (2 days)
4. ‚úÖ Fix transaction costs in labels (2 days)

### Week 2: Validation
5. ‚úÖ Regression test suite (2 days)
6. ‚úÖ Document current performance (4 hours)
7. ‚úÖ Verify 7 core models pass all tests (1 day)

### Week 3-4: Foundation Models (Fast ROI)
8. ‚úÖ Chronos-Bolt wrapper (3 days)
9. ‚úÖ TimesFM 2.5 wrapper (3 days)
10. ‚úÖ N-BEATS implementation (1 day)
11. ‚úÖ N-HiTS implementation (1 day)

### Week 5-6: Advanced Transformers (Part 1)
12. ‚úÖ PatchTST implementation (4 days)
13. ‚úÖ iTransformer implementation (3 days)
14. ‚úÖ TimesNet implementation (4 days)

### Week 7-8: Advanced Transformers (Part 2) + State-Space
15. ‚úÖ Autoformer implementation (4 days)
16. ‚úÖ S-Mamba implementation (5 days)

### Week 9-11: Meta-Learning
17. ‚úÖ Regime-aware meta-learner (5 days)
18. ‚úÖ Confidence calibration integration (3 days)
19. ‚úÖ OOF stacking with regime features (4 days)

### Week 12-14: Production Infrastructure
20. ‚úÖ Auto-bundle generation (3 days)
21. ‚úÖ Test set evaluation script (2 days)
22. ‚úÖ Drift detection daemon (4 days)
23. ‚úÖ CI/CD pipeline (3 days)

---

## Part 8: Risk Analysis

### High-Risk Items

1. **S-Mamba dependency** - Requires `mamba-ssm` package (may conflict with PyTorch versions)
   - **Mitigation:** Test in isolated conda environment first

2. **Foundation model API changes** - Chronos/TimesFM APIs may change
   - **Mitigation:** Pin exact model versions, implement adapter layer

3. **HMM incremental training complexity** - Incremental fitting is non-trivial
   - **Mitigation:** Start with `expanding=False`, defer incremental implementation

4. **Test coverage regression** - Deleting 6 models may leave dead test code
   - **Mitigation:** Run full test suite after pruning, verify 100% coverage on core 7

### Medium-Risk Items

5. **PatchTST causal masking** - Ensuring true production safety
   - **Mitigation:** Unit tests for non-leakage, verify `is_production_safe=True`

6. **Regime meta-learner overfitting** - Learning regime-specific weights on limited data
   - **Mitigation:** Use regularization, validate on walk-forward splits

---

## Part 9: Testing Strategy

### Phase 0: Pruning Tests
- [ ] Verify deleted models don't break registry
- [ ] Ensure no dead imports in remaining code
- [ ] Confirm 7 core models pass all existing tests

### Phase 1: Bug Fix Tests
- [ ] HMM: Unit test that `expanding=False` uses only past data
- [ ] GA: Integration test that optimization sees only train portion
- [ ] Transaction costs: Validate barrier adjustment math

### Phase 2: New Model Tests
- [ ] Each new model: `test_fit()`, `test_predict()`, `test_save_load()`
- [ ] PatchTST: Test causal masking prevents future leakage
- [ ] Foundation models: Test zero-shot prediction without training
- [ ] Regression: Compare new models vs baselines on same data

### Phase 3: Meta-Learning Tests
- [ ] Regime meta-learner: Validate regime features used in stacking
- [ ] OOF generation: Ensure no leakage in fold construction
- [ ] Ensemble performance: Compare regime-aware vs simple stacking

### Phase 4: Production Tests
- [ ] Bundle creation: End-to-end test of `--create-bundle` flag
- [ ] Drift detection: Simulate distribution shift, verify alerts
- [ ] CI/CD: Verify automated testing triggers on PR

---

## Success Metrics

**Completion Criteria:**

| Phase | Metric | Target |
|-------|--------|--------|
| Phase 0 | Model count after pruning | Exactly 7 models |
| Phase 1 | Zero leakage bugs | All 3 bugs fixed + regression tests pass |
| Phase 2 | Model count after additions | 16 models registered |
| Phase 3 | Ensemble Sharpe | Regime-aware stacking > simple average by 10%+ |
| Phase 4 | Deployment time | Train‚Üíbundle‚Üídeploy in one command (<5 min) |

**Final System Capabilities:**

- ‚úÖ 16 production-safe models (no leakage, no redundancy)
- ‚úÖ Fair evaluation under identical leakage-safe conditions
- ‚úÖ Regime-aware ensemble meta-learning
- ‚úÖ One-command train‚Üíbundle‚Üídeploy workflow
- ‚úÖ Automated drift detection and CI/CD
- ‚úÖ Probabilistic forecasts for risk management

---

## Next Steps

1. **Review this plan** with stakeholders
2. **Start with Phase 0** (prune 6 models) - immediate cleanup
3. **Execute Phase 1** (critical bugs) - cannot skip
4. **Parallelize Phase 2** (model additions) if 2+ engineers available
5. **Iterate** based on model performance benchmarks

**Track progress:** GitHub Issues/Projects, update this plan weekly.

---

## Appendix: Model Implementation Checklist

For each new model, complete:

- [ ] Implement `BaseModel` interface (`fit`, `predict`, `save`, `load`)
- [ ] Register with `@register(name=..., family=...)`
- [ ] Add config file: `config/models/<model_name>.yaml`
- [ ] Write tests: `tests/models/test_<model_name>.py`
- [ ] Add to model registry count assertion
- [ ] Document in `CLAUDE.md` model list
- [ ] Run performance benchmark vs baselines
- [ ] Verify production safety (`is_production_safe` property)
- [ ] Add example usage to `docs/phases/`

**Target:** 9 models √ó 9 checklist items = 81 tasks for Phase 2
