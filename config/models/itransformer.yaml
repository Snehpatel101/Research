# iTransformer Configuration
# Inverted Transformer - applies attention over features (channels) instead of time steps
# GPU: Optimized for CUDA acceleration with inverted attention mechanism

# Model identification
model:
  name: itransformer
  family: neural
  description: Inverted Transformer with channel-wise attention for multivariate time series

# Default hyperparameters
defaults:
  # Architecture
  d_model: 256
  n_heads: 8
  n_layers: 2
  d_ff: 512
  dropout: 0.2
  attention_dropout: 0.1
  use_instance_norm: true
  use_layer_norm: true
  activation: gelu

  # Input
  sequence_length: 60
  input_features: null         # Auto-detected from data

  # Learning
  learning_rate: 0.001
  weight_decay: 0.0001
  gradient_clip: 1.0

  # Optimizer
  optimizer: adamw
  scheduler: cosine
  warmup_epochs: 5

# Training settings
training:
  batch_size: 512
  max_epochs: 100
  early_stopping_patience: 15
  min_delta: 0.0001
  feature_set: neural_optimal
  random_seed: 42
  num_workers: 4
  pin_memory: true

# Device settings
device:
  default: auto                # auto, cuda, or cpu
  mixed_precision: true        # Use FP16 for faster training
