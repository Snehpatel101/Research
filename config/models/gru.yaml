# GRU Configuration
# Gated Recurrent Unit network (simpler than LSTM)
# GPU: Optimized for CUDA acceleration

# Model identification
model:
  name: gru
  family: neural
  description: Gated Recurrent Unit network - simpler and faster than LSTM

# Default hyperparameters
defaults:
  # Architecture
  hidden_size: 256
  num_layers: 2
  dropout: 0.3

  # Input
  sequence_length: 60
  input_features: null         # Auto-detected from data

  # Learning
  learning_rate: 0.001
  weight_decay: 0.0001
  gradient_clip: 1.0

  # Optimizer
  optimizer: adamw
  scheduler: cosine
  warmup_epochs: 5

# Training settings
training:
  batch_size: 512
  max_epochs: 100
  early_stopping_patience: 15
  min_delta: 0.0001
  feature_set: neural_optimal
  random_seed: 42
  num_workers: 4
  pin_memory: true

# Device settings
device:
  default: auto                # auto, cuda, or cpu
  mixed_precision: true        # Use FP16 for faster training
