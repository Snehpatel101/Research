# TFT Configuration
# Temporal Fusion Transformer - interpretable multi-horizon forecasting architecture
# GPU: Optimized for CUDA acceleration with variable selection networks

# Model identification
model:
  name: tft
  family: neural
  description: Temporal Fusion Transformer with interpretable attention and variable selection

# Default hyperparameters
defaults:
  # Architecture
  hidden_size: 256
  lstm_layers: 2
  attention_heads: 4
  dropout: 0.3
  hidden_dropout: 0.1
  attention_dropout: 0.1
  use_quantile_output: false
  num_quantiles: 3             # Only used if use_quantile_output is true
  use_static_covariates: false
  use_known_future: false

  # Variable Selection
  use_variable_selection: true
  variable_selection_dropout: 0.1

  # Gating
  use_glu: true                # Gated Linear Units

  # Input
  sequence_length: 60
  input_features: null         # Auto-detected from data

  # Learning
  learning_rate: 0.001
  weight_decay: 0.0001
  gradient_clip: 1.0

  # Optimizer
  optimizer: adamw
  scheduler: cosine
  warmup_epochs: 5

# Training settings
training:
  batch_size: 512
  max_epochs: 100
  early_stopping_patience: 15
  min_delta: 0.0001
  feature_set: neural_optimal
  random_seed: 42
  num_workers: 4
  pin_memory: true

# Device settings
device:
  default: auto                # auto, cuda, or cpu
  mixed_precision: true        # Use FP16 for faster training
