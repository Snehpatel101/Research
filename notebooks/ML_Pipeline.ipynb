{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Unified Pipeline\n",
    "\n",
    "**Single notebook for the entire ML pipeline.** Configure everything in Section 1, then run sequentially.\n",
    "\n",
    "## Pipeline Phases\n",
    "1. **Configuration** - All settings in one place\n",
    "2. **Environment Setup** - Auto-detects Colab vs Local\n",
    "3. **Phase 1: Data Pipeline** - Clean → Features → Labels → Splits → Scale\n",
    "4. **Phase 2: Model Training** - Train any model type\n",
    "5. **Phase 3: Cross-Validation** - Robust evaluation (optional)\n",
    "6. **Phase 4: Ensemble** - Combine models (optional)\n",
    "7. **Results & Export** - Summary and model export\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MASTER CONFIGURATION\n",
    "\n",
    "**Configure ALL settings here. No need to modify any other cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 1.1 Master Configuration Panel { display-mode: \"form\" }\n#@markdown ## Data Configuration\n#@markdown ---\n\n#@markdown ### Contract Selection\nSYMBOL = \"SI\"  #@param [\"SI\", \"MES\", \"MGC\", \"ES\", \"GC\", \"NQ\", \"CL\", \"HG\", \"ZB\", \"ZN\"]\n#@markdown Select ONE contract. Each contract is trained in complete isolation.\n#@markdown - **SI** = Silver, **MES** = Micro E-mini S&P, **MGC** = Micro Gold\n#@markdown - **ES** = E-mini S&P, **GC** = Gold, **NQ** = E-mini Nasdaq\n#@markdown - **CL** = Crude Oil, **HG** = Copper, **ZB/ZN** = Bonds\n\n#@markdown ### Data Source (Google Drive path relative to My Drive)\nDRIVE_DATA_PATH = \"research/data/raw\"  #@param {type: \"string\"}\n\n#@markdown ### Date Range\n#@markdown **Auto-detected from your parquet/CSV file.** No manual selection needed.\n#@markdown The pipeline will read the actual date range from your data.\n\n#@markdown ---\n#@markdown ## Pipeline Configuration\n\n#@markdown ### Label Horizons (bars)\nHORIZONS = \"5,10,15,20\"  #@param {type: \"string\"}\n#@markdown Comma-separated prediction horizons\n\n#@markdown ### Train/Val/Test Split Ratios\nTRAIN_RATIO = 0.70  #@param {type: \"number\"}\nVAL_RATIO = 0.15  #@param {type: \"number\"}\nTEST_RATIO = 0.15  #@param {type: \"number\"}\n\n#@markdown ### Leakage Prevention\nPURGE_BARS = 60  #@param {type: \"integer\"}\n#@markdown Bars to purge around train/val boundary (3x max horizon)\nEMBARGO_BARS = 1440  #@param {type: \"integer\"}\n#@markdown Embargo period after validation (~5 days at 5-min)\n\n#@markdown ---\n#@markdown ## Model Training Configuration\n\n#@markdown ### Training Horizon\nTRAINING_HORIZON = 20  #@param [5, 10, 15, 20]\n#@markdown Which horizon to train models on\n\n#@markdown ### Model Selection\nTRAIN_XGBOOST = True  #@param {type: \"boolean\"}\nTRAIN_LIGHTGBM = True  #@param {type: \"boolean\"}\nTRAIN_CATBOOST = True  #@param {type: \"boolean\"}\nTRAIN_RANDOM_FOREST = False  #@param {type: \"boolean\"}\nTRAIN_LOGISTIC = False  #@param {type: \"boolean\"}\nTRAIN_SVM = False  #@param {type: \"boolean\"}\nTRAIN_LSTM = False  #@param {type: \"boolean\"}\nTRAIN_GRU = False  #@param {type: \"boolean\"}\nTRAIN_TCN = False  #@param {type: \"boolean\"}\n\n#@markdown ### Neural Network Settings\nSEQUENCE_LENGTH = 60  #@param {type: \"slider\", min: 30, max: 120, step: 10}\nBATCH_SIZE = 256  #@param [64, 128, 256, 512, 1024]\nMAX_EPOCHS = 50  #@param {type: \"integer\"}\nEARLY_STOPPING_PATIENCE = 10  #@param {type: \"integer\"}\n\n#@markdown ### Boosting Settings\nN_ESTIMATORS = 500  #@param {type: \"integer\"}\nBOOSTING_EARLY_STOPPING = 50  #@param {type: \"integer\"}\n\n#@markdown ---\n#@markdown ## Optional Phases\n\n#@markdown ### Cross-Validation\nRUN_CROSS_VALIDATION = False  #@param {type: \"boolean\"}\nCV_N_SPLITS = 5  #@param {type: \"integer\"}\nCV_TUNE_HYPERPARAMS = False  #@param {type: \"boolean\"}\nCV_N_TRIALS = 20  #@param {type: \"integer\"}\n\n#@markdown ### Ensemble Training\nTRAIN_ENSEMBLE = False  #@param {type: \"boolean\"}\nENSEMBLE_TYPE = \"voting\"  #@param [\"voting\", \"stacking\", \"blending\"]\nENSEMBLE_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"random_forest\", \"xgboost\"]\n\n#@markdown ---\n#@markdown ## Execution Options\n\n#@markdown ### What to Run\nRUN_DATA_PIPELINE = True  #@param {type: \"boolean\"}\n#@markdown Run Phase 1 data pipeline\nRUN_MODEL_TRAINING = True  #@param {type: \"boolean\"}\n#@markdown Run Phase 2 model training\n\n#@markdown ### Memory Management\nSAFE_MODE = False  #@param {type: \"boolean\"}\n#@markdown Enable for low-memory environments (reduces batch size, limits iterations)\n\n# ============================================================\n# BUILD CONFIGURATION (DO NOT MODIFY BELOW)\n# ============================================================\n\nimport os\nfrom datetime import datetime\n\n# Parse horizons\nHORIZON_LIST = [int(h.strip()) for h in HORIZONS.split(',')]\n\n# Build model list\nMODELS_TO_TRAIN = []\nif TRAIN_XGBOOST: MODELS_TO_TRAIN.append('xgboost')\nif TRAIN_LIGHTGBM: MODELS_TO_TRAIN.append('lightgbm')\nif TRAIN_CATBOOST: MODELS_TO_TRAIN.append('catboost')\nif TRAIN_RANDOM_FOREST: MODELS_TO_TRAIN.append('random_forest')\nif TRAIN_LOGISTIC: MODELS_TO_TRAIN.append('logistic')\nif TRAIN_SVM: MODELS_TO_TRAIN.append('svm')\nif TRAIN_LSTM: MODELS_TO_TRAIN.append('lstm')\nif TRAIN_GRU: MODELS_TO_TRAIN.append('gru')\nif TRAIN_TCN: MODELS_TO_TRAIN.append('tcn')\n\n# Date range will be auto-detected from data file\nDATA_START = None  # Auto-detected\nDATA_END = None    # Auto-detected\n\n# Safe mode adjustments\nif SAFE_MODE:\n    BATCH_SIZE = min(BATCH_SIZE, 64)\n    N_ESTIMATORS = min(N_ESTIMATORS, 300)\n    SEQUENCE_LENGTH = min(SEQUENCE_LENGTH, 30)\n\n# Print configuration summary\nprint(\"=\" * 70)\nprint(\" ML PIPELINE CONFIGURATION\")\nprint(\"=\" * 70)\nprint(f\"\\n  Contract:        {SYMBOL}\")\nprint(f\"  Date Range:      Auto-detect from data file\")\nprint(f\"  Horizons:        {HORIZON_LIST}\")\nprint(f\"  Split Ratios:    {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")\nprint(f\"  Training Horizon: H{TRAINING_HORIZON}\")\nprint(f\"  Models:          {MODELS_TO_TRAIN if MODELS_TO_TRAIN else 'None selected'}\")\nprint(f\"\\n  Run Pipeline:    {RUN_DATA_PIPELINE}\")\nprint(f\"  Run Training:    {RUN_MODEL_TRAINING}\")\nprint(f\"  Cross-Validation: {RUN_CROSS_VALIDATION}\")\nprint(f\"  Ensemble:        {TRAIN_ENSEMBLE}\")\nprint(f\"  Safe Mode:       {SAFE_MODE}\")\nprint(\"=\" * 70)\nprint(\"\\nConfiguration complete! Run the next cells sequentially.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ENVIRONMENT SETUP\n",
    "\n",
    "Auto-detects Colab vs Local environment and sets up paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Environment Detection & Setup { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# ENVIRONMENT DETECTION\n",
    "# ============================================================\n",
    "IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\n[Environment] Google Colab detected\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone/update repository\n",
    "    REPO_PATH = Path('/content/research')\n",
    "    if not REPO_PATH.exists():\n",
    "        print(\"\\n[Setup] Cloning repository...\")\n",
    "        !git clone https://github.com/Snehpatel101/research.git /content/research\n",
    "    else:\n",
    "        print(\"\\n[Setup] Updating repository...\")\n",
    "        !cd /content/research && git pull --quiet\n",
    "    \n",
    "    # Set paths\n",
    "    PROJECT_ROOT = REPO_PATH\n",
    "    DRIVE_ROOT = Path('/content/drive/MyDrive')\n",
    "    RAW_DATA_DIR = DRIVE_ROOT / DRIVE_DATA_PATH\n",
    "    RESULTS_DIR = DRIVE_ROOT / 'research/experiments'\n",
    "    \n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[Environment] Local environment detected\")\n",
    "    \n",
    "    PROJECT_ROOT = Path('/Users/sneh/research')\n",
    "    DRIVE_ROOT = None\n",
    "    RAW_DATA_DIR = PROJECT_ROOT / 'data/raw'\n",
    "    RESULTS_DIR = PROJECT_ROOT / 'experiments'\n",
    "    \n",
    "    os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Add to Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Create output directories\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "EXPERIMENTS_DIR = RESULTS_DIR / 'runs'\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n  Project Root:  {PROJECT_ROOT}\")\n",
    "print(f\"  Raw Data:      {RAW_DATA_DIR}\")\n",
    "print(f\"  Splits:        {SPLITS_DIR}\")\n",
    "print(f\"  Experiments:   {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.2 Install Dependencies { display-mode: \"form\" }\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"[Dependencies] Installing packages...\")\n",
    "    !pip install -q xgboost lightgbm catboost optuna ta pywavelets scikit-learn pandas numpy matplotlib tqdm pyarrow numba\n",
    "    print(\"[Dependencies] Installation complete!\")\n",
    "else:\n",
    "    print(\"[Dependencies] Local environment - assuming packages installed.\")\n",
    "    print(\"  If needed: pip install xgboost lightgbm catboost optuna ta pywavelets\")\n",
    "\n",
    "# Verify imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"\\n  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.3 GPU Detection { display-mode: \"form\" }\n",
    "\n",
    "import torch\n",
    "\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_NAME = None\n",
    "GPU_MEMORY = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" HARDWARE DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    GPU_NAME = props.name\n",
    "    GPU_MEMORY = props.total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"\\n  GPU: {GPU_NAME}\")\n",
    "    print(f\"  Memory: {GPU_MEMORY:.1f} GB\")\n",
    "    print(f\"  Compute: {props.major}.{props.minor}\")\n",
    "    \n",
    "    # Adjust batch size based on GPU memory\n",
    "    if GPU_MEMORY >= 40:\n",
    "        RECOMMENDED_BATCH = 1024\n",
    "    elif GPU_MEMORY >= 15:\n",
    "        RECOMMENDED_BATCH = 512\n",
    "    else:\n",
    "        RECOMMENDED_BATCH = 256\n",
    "    \n",
    "    print(f\"  Recommended batch: {RECOMMENDED_BATCH}\")\n",
    "else:\n",
    "    print(\"\\n  GPU: Not available (using CPU)\")\n",
    "    print(\"  Tip: Runtime -> Change runtime type -> GPU\")\n",
    "    RECOMMENDED_BATCH = 128\n",
    "\n",
    "# Check for neural models without GPU\n",
    "NEURAL_MODELS = {'lstm', 'gru', 'tcn'}\n",
    "selected_neural = set(MODELS_TO_TRAIN) & NEURAL_MODELS\n",
    "if selected_neural and not GPU_AVAILABLE:\n",
    "    print(f\"\\n  [WARNING] Neural models selected but no GPU: {selected_neural}\")\n",
    "    print(\"  Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.4 Memory Utilities { display-mode: \"form\" }\n",
    "\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def print_memory_status(label: str = \"Current\"):\n",
    "    \"\"\"Print current RAM and GPU memory usage.\"\"\"\n",
    "    print(f\"\\n--- Memory: {label} ---\")\n",
    "    \n",
    "    # RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM: {ram.used/1e9:.1f}GB / {ram.total/1e9:.1f}GB ({ram.percent}%)\")\n",
    "    \n",
    "    # GPU\n",
    "    if GPU_AVAILABLE:\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"GPU: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear RAM and GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "print(\"Memory utilities loaded.\")\n",
    "print_memory_status(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. PHASE 1: DATA PIPELINE\n",
    "\n",
    "Processes raw OHLCV data into training-ready datasets.\n",
    "\n",
    "**Pipeline stages:**\n",
    "1. Load raw 1-minute data\n",
    "2. Clean and resample to 5-minute bars\n",
    "3. Generate 150+ technical features\n",
    "4. Apply triple-barrier labeling\n",
    "5. Create train/val/test splits with purge/embargo\n",
    "6. Scale features (train-only fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3.1 Verify Raw Data & Detect Date Range { display-mode: \"form\" }\n\nimport pandas as pd\nfrom pathlib import Path\n\nprint(\"=\" * 70)\nprint(\" RAW DATA VERIFICATION\")\nprint(\"=\" * 70)\nprint(f\"\\nLooking for {SYMBOL} data in: {RAW_DATA_DIR}\")\n\n# Find data file\nRAW_DATA_FILE = None\npatterns = [\n    f\"{SYMBOL}_1m.parquet\", f\"{SYMBOL}_1m.csv\",\n    f\"{SYMBOL}.parquet\", f\"{SYMBOL}.csv\",\n    f\"{SYMBOL}_1min.parquet\", f\"{SYMBOL}_1min.csv\",\n]\n\nfor pattern in patterns:\n    path = RAW_DATA_DIR / pattern\n    if path.exists():\n        RAW_DATA_FILE = path\n        break\n\nif RAW_DATA_FILE:\n    size_mb = RAW_DATA_FILE.stat().st_size / 1e6\n    print(f\"\\n  Found: {RAW_DATA_FILE.name} ({size_mb:.1f} MB)\")\n    \n    # Load and validate\n    if RAW_DATA_FILE.suffix == '.parquet':\n        df_raw = pd.read_parquet(RAW_DATA_FILE)\n    else:\n        df_raw = pd.read_csv(RAW_DATA_FILE)\n    \n    print(f\"  Rows: {len(df_raw):,}\")\n    print(f\"  Columns: {list(df_raw.columns)}\")\n    \n    # Validate OHLCV columns\n    required = {'open', 'high', 'low', 'close', 'volume'}\n    found = {c.lower() for c in df_raw.columns}\n    if required.issubset(found):\n        print(\"  OHLCV columns: OK\")\n    else:\n        missing = required - found\n        print(f\"  [ERROR] Missing columns: {missing}\")\n    \n    # ============================================================\n    # AUTO-DETECT DATE RANGE FROM DATA\n    # ============================================================\n    date_col = None\n    for c in df_raw.columns:\n        if 'date' in c.lower() or 'time' in c.lower():\n            date_col = c\n            break\n    \n    if date_col:\n        df_raw[date_col] = pd.to_datetime(df_raw[date_col])\n        \n        # Store globally for pipeline use\n        DATA_START = df_raw[date_col].min()\n        DATA_END = df_raw[date_col].max()\n        DATA_START_YEAR = DATA_START.year\n        DATA_END_YEAR = DATA_END.year\n        \n        print(f\"\\n  [AUTO-DETECTED DATE RANGE]\")\n        print(f\"  Start: {DATA_START} ({DATA_START_YEAR})\")\n        print(f\"  End:   {DATA_END} ({DATA_END_YEAR})\")\n        print(f\"  Span:  {(DATA_END - DATA_START).days} days ({DATA_END_YEAR - DATA_START_YEAR + 1} years)\")\n    else:\n        print(\"  [WARNING] No datetime column found - using index\")\n        DATA_START = None\n        DATA_END = None\n        DATA_START_YEAR = 2019\n        DATA_END_YEAR = 2024\n    \n    del df_raw\n    gc.collect()\n    \n    print(\"\\n  Data verified and ready for processing!\")\nelse:\n    print(f\"\\n  [ERROR] No data file found for {SYMBOL}!\")\n    print(f\"  Expected location: {RAW_DATA_DIR}\")\n    print(f\"  Expected files: {SYMBOL}_1m.csv or {SYMBOL}_1m.parquet\")\n    print(f\"\\n  Available files in directory:\")\n    if RAW_DATA_DIR.exists():\n        for f in RAW_DATA_DIR.iterdir():\n            if f.suffix in ['.csv', '.parquet']:\n                print(f\"    - {f.name}\")\n    else:\n        print(f\"    Directory does not exist!\")\n    \n    RAW_DATA_FILE = None\n    DATA_START = None\n    DATA_END = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3.2 Run Data Pipeline { display-mode: \"form\" }\n\nif not RUN_DATA_PIPELINE:\n    print(\"[Skipped] Data pipeline disabled in configuration.\")\n    print(\"Set RUN_DATA_PIPELINE = True in Section 1 to enable.\")\nelif RAW_DATA_FILE is None:\n    print(\"[Error] No raw data file found. Cannot run pipeline.\")\nelse:\n    import time\n    from datetime import datetime\n    \n    print(\"=\" * 70)\n    print(\" PHASE 1: DATA PIPELINE\")\n    print(\"=\" * 70)\n    print(f\"\\n  Symbol: {SYMBOL}\")\n    \n    # Use auto-detected date range\n    if DATA_START is not None and DATA_END is not None:\n        print(f\"  Date Range: {DATA_START.strftime('%Y-%m-%d')} to {DATA_END.strftime('%Y-%m-%d')} (auto-detected)\")\n        start_date_str = DATA_START.strftime('%Y-%m-%d')\n        end_date_str = DATA_END.strftime('%Y-%m-%d')\n    else:\n        print(f\"  Date Range: Full dataset (no filter)\")\n        start_date_str = None\n        end_date_str = None\n    \n    print(f\"  Horizons: {HORIZON_LIST}\")\n    \n    start_time = time.time()\n    \n    try:\n        from src.phase1.pipeline_config import PipelineConfig\n        from src.pipeline.runner import PipelineRunner\n        \n        # Configure pipeline with auto-detected dates\n        config = PipelineConfig(\n            symbols=[SYMBOL],\n            project_root=PROJECT_ROOT,\n            label_horizons=HORIZON_LIST,\n            train_ratio=TRAIN_RATIO,\n            val_ratio=VAL_RATIO,\n            test_ratio=TEST_RATIO,\n            purge_bars=PURGE_BARS,\n            embargo_bars=EMBARGO_BARS,\n            start_date=start_date_str,\n            end_date=end_date_str,\n            allow_batch_symbols=False,  # Single-contract architecture\n        )\n        \n        # Run pipeline\n        runner = PipelineRunner(config)\n        success = runner.run()\n        \n        elapsed = time.time() - start_time\n        \n        if success:\n            print(f\"\\n  Pipeline completed in {elapsed/60:.1f} minutes\")\n            \n            # Verify output\n            if (SPLITS_DIR / 'train_scaled.parquet').exists():\n                for split in ['train', 'val', 'test']:\n                    df = pd.read_parquet(SPLITS_DIR / f'{split}_scaled.parquet')\n                    print(f\"  {split}: {len(df):,} samples\")\n                    del df\n                gc.collect()\n                print(\"\\n  Data ready for training!\")\n        else:\n            print(\"\\n  [ERROR] Pipeline failed. Check logs above.\")\n        \n        del runner, config\n        clear_memory()\n        \n    except Exception as e:\n        print(f\"\\n  [ERROR] Pipeline failed: {e}\")\n        import traceback\n        traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.3 Verify Processed Data { display-mode: \"form\" }\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" PROCESSED DATA VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for pre-processed data (local) or pipeline output (Colab)\n",
    "if not IS_COLAB:\n",
    "    # Local: check pre-processed data\n",
    "    local_splits = PROJECT_ROOT / 'data/splits/final_correct/scaled'\n",
    "    if (local_splits / 'train_scaled.parquet').exists():\n",
    "        SPLITS_DIR = local_splits\n",
    "        print(f\"\\nUsing pre-processed data: {SPLITS_DIR}\")\n",
    "\n",
    "if (SPLITS_DIR / 'train_scaled.parquet').exists():\n",
    "    # Load metadata without keeping DataFrames\n",
    "    train_df = pd.read_parquet(SPLITS_DIR / 'train_scaled.parquet')\n",
    "    \n",
    "    FEATURE_COLS = [c for c in train_df.columns \n",
    "                   if not c.startswith(('label_', 'sample_weight', 'quality_', 'datetime', 'symbol'))]\n",
    "    LABEL_COLS = [c for c in train_df.columns if c.startswith('label_')]\n",
    "    TRAIN_LEN = len(train_df)\n",
    "    \n",
    "    # Label distribution\n",
    "    label_dists = {}\n",
    "    for col in LABEL_COLS:\n",
    "        label_dists[col] = train_df[col].value_counts().sort_index().to_dict()\n",
    "    \n",
    "    del train_df\n",
    "    \n",
    "    # Get val/test sizes\n",
    "    val_df = pd.read_parquet(SPLITS_DIR / 'val_scaled.parquet')\n",
    "    VAL_LEN = len(val_df)\n",
    "    del val_df\n",
    "    \n",
    "    test_df = pd.read_parquet(SPLITS_DIR / 'test_scaled.parquet')\n",
    "    TEST_LEN = len(test_df)\n",
    "    del test_df\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"  Train: {TRAIN_LEN:,} samples\")\n",
    "    print(f\"  Val:   {VAL_LEN:,} samples\")\n",
    "    print(f\"  Test:  {TEST_LEN:,} samples\")\n",
    "    print(f\"  Total: {TRAIN_LEN + VAL_LEN + TEST_LEN:,} samples\")\n",
    "    print(f\"\\n  Features: {len(FEATURE_COLS)}\")\n",
    "    print(f\"  Labels: {LABEL_COLS}\")\n",
    "    \n",
    "    print(f\"\\nLabel Distribution (train):\")\n",
    "    for col, dist in label_dists.items():\n",
    "        total = sum(dist.values())\n",
    "        long_pct = dist.get(1, 0) / total * 100\n",
    "        neutral_pct = dist.get(0, 0) / total * 100\n",
    "        short_pct = dist.get(-1, 0) / total * 100\n",
    "        print(f\"  {col}: Long={long_pct:.1f}% | Neutral={neutral_pct:.1f}% | Short={short_pct:.1f}%\")\n",
    "    \n",
    "    DATA_READY = True\n",
    "    print(\"\\n  Data verified and ready for training!\")\n",
    "else:\n",
    "    print(\"\\n[ERROR] Processed data not found!\")\n",
    "    print(f\"  Expected: {SPLITS_DIR}/train_scaled.parquet\")\n",
    "    print(\"  Run Section 3.2 to process raw data.\")\n",
    "    DATA_READY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. PHASE 2: MODEL TRAINING\n",
    "\n",
    "Train selected models on the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Train Models { display-mode: \"form\" }\n",
    "\n",
    "if not RUN_MODEL_TRAINING:\n",
    "    print(\"[Skipped] Model training disabled in configuration.\")\n",
    "elif not DATA_READY:\n",
    "    print(\"[Error] Data not ready. Run Section 3 first.\")\n",
    "elif not MODELS_TO_TRAIN:\n",
    "    print(\"[Error] No models selected. Enable models in Section 1.\")\n",
    "else:\n",
    "    import time\n",
    "    import json\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 2: MODEL TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n  Models: {MODELS_TO_TRAIN}\")\n",
    "    print(f\"  Horizon: H{TRAINING_HORIZON}\")\n",
    "    \n",
    "    TRAINING_RESULTS = {}\n",
    "    \n",
    "    try:\n",
    "        from src.models import ModelRegistry, Trainer, TrainerConfig\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        # Load data container\n",
    "        print(\"\\nLoading data...\")\n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        print(f\"  Train: {container.splits['train'].n_samples:,}\")\n",
    "        print(f\"  Val: {container.splits['val'].n_samples:,}\")\n",
    "        \n",
    "        # Train each model\n",
    "        for i, model_name in enumerate(MODELS_TO_TRAIN, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\" [{i}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            clear_memory()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Configure model\n",
    "            if model_name in ['lstm', 'gru', 'tcn']:\n",
    "                config = TrainerConfig(\n",
    "                    model_name=model_name,\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    sequence_length=SEQUENCE_LENGTH,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    max_epochs=MAX_EPOCHS,\n",
    "                    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                    output_dir=EXPERIMENTS_DIR,\n",
    "                    device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n",
    "                )\n",
    "            elif model_name == 'catboost':\n",
    "                config = TrainerConfig(\n",
    "                    model_name=model_name,\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    output_dir=EXPERIMENTS_DIR,\n",
    "                    model_config={\n",
    "                        \"iterations\": N_ESTIMATORS,\n",
    "                        \"early_stopping_rounds\": BOOSTING_EARLY_STOPPING,\n",
    "                        \"use_gpu\": False,\n",
    "                        \"task_type\": \"CPU\",\n",
    "                        \"verbose\": False,\n",
    "                    },\n",
    "                )\n",
    "            else:\n",
    "                config = TrainerConfig(\n",
    "                    model_name=model_name,\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    output_dir=EXPERIMENTS_DIR,\n",
    "                    model_config={\n",
    "                        \"n_estimators\": N_ESTIMATORS,\n",
    "                        \"early_stopping_rounds\": BOOSTING_EARLY_STOPPING,\n",
    "                    } if model_name in ['xgboost', 'lightgbm'] else None,\n",
    "                )\n",
    "            \n",
    "            # Train\n",
    "            trainer = Trainer(config)\n",
    "            results = trainer.run(container)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Store results\n",
    "            metrics = results.get('evaluation_metrics', {})\n",
    "            TRAINING_RESULTS[model_name] = {\n",
    "                'metrics': metrics,\n",
    "                'time': elapsed,\n",
    "                'run_id': results.get('run_id', 'unknown'),\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n  Accuracy: {metrics.get('accuracy', 0):.2%}\")\n",
    "            print(f\"  Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n",
    "            print(f\"  Time: {elapsed:.1f}s\")\n",
    "            \n",
    "            del trainer, config\n",
    "            clear_memory()\n",
    "        \n",
    "        # Save results\n",
    "        results_file = EXPERIMENTS_DIR / 'training_results.json'\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(TRAINING_RESULTS, f, indent=2)\n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "        \n",
    "        del container\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Compare Models { display-mode: \"form\" }\n",
    "\n",
    "if TRAINING_RESULTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" MODEL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Build comparison table\n",
    "    rows = []\n",
    "    for model, data in TRAINING_RESULTS.items():\n",
    "        metrics = data.get('metrics', {})\n",
    "        rows.append({\n",
    "            'Model': model,\n",
    "            'Accuracy': metrics.get('accuracy', 0),\n",
    "            'Macro F1': metrics.get('macro_f1', 0),\n",
    "            'Weighted F1': metrics.get('weighted_f1', 0),\n",
    "            'Time (s)': data.get('time', 0),\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(rows)\n",
    "    comparison_df = comparison_df.sort_values('Macro F1', ascending=False)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Best model\n",
    "    best_model = comparison_df.iloc[0]['Model']\n",
    "    best_f1 = comparison_df.iloc[0]['Macro F1']\n",
    "    print(f\"\\n  Best Model: {best_model} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # Visualization\n",
    "    if len(TRAINING_RESULTS) > 1:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        sorted_df = comparison_df.sort_values('Accuracy', ascending=True)\n",
    "        axes[0].barh(sorted_df['Model'], sorted_df['Accuracy'], color='steelblue')\n",
    "        axes[0].set_xlabel('Accuracy')\n",
    "        axes[0].set_title('Model Accuracy')\n",
    "        axes[0].set_xlim(0, 1)\n",
    "        \n",
    "        # Training time\n",
    "        sorted_df = comparison_df.sort_values('Time (s)', ascending=True)\n",
    "        axes[1].barh(sorted_df['Model'], sorted_df['Time (s)'], color='coral')\n",
    "        axes[1].set_xlabel('Training Time (seconds)')\n",
    "        axes[1].set_title('Training Time')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No training results available.\")\n",
    "    print(\"Run Section 4.1 to train models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. PHASE 3: CROSS-VALIDATION (Optional)\n",
    "\n",
    "Run purged K-fold cross-validation for robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Run Cross-Validation { display-mode: \"form\" }\n",
    "\n",
    "if not RUN_CROSS_VALIDATION:\n",
    "    print(\"[Skipped] Cross-validation disabled in configuration.\")\n",
    "    print(\"Set RUN_CROSS_VALIDATION = True in Section 1 to enable.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 3: CROSS-VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        from src.cross_validation import PurgedKFold, PurgedKFoldConfig\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        from sklearn.metrics import f1_score\n",
    "        \n",
    "        # Load data\n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        \n",
    "        X, y, _ = container.get_sklearn_arrays('train')\n",
    "        print(f\"\\nData: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "        \n",
    "        # Configure CV\n",
    "        cv_config = PurgedKFoldConfig(\n",
    "            n_splits=CV_N_SPLITS,\n",
    "            purge_bars=PURGE_BARS,\n",
    "            embargo_bars=EMBARGO_BARS,\n",
    "        )\n",
    "        cv = PurgedKFold(cv_config)\n",
    "        \n",
    "        print(f\"CV: {CV_N_SPLITS} folds, purge={PURGE_BARS}, embargo={EMBARGO_BARS}\")\n",
    "        \n",
    "        # Run CV for best model\n",
    "        best_model = TRAINING_RESULTS and max(TRAINING_RESULTS, key=lambda x: TRAINING_RESULTS[x]['metrics'].get('macro_f1', 0))\n",
    "        if not best_model:\n",
    "            best_model = 'xgboost'\n",
    "        \n",
    "        print(f\"\\nRunning CV for: {best_model}\")\n",
    "        \n",
    "        fold_scores = []\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(tqdm(cv.split(X, y), total=CV_N_SPLITS, desc=\"CV Folds\")):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            model = ModelRegistry.create(best_model, config={\n",
    "                'n_estimators': N_ESTIMATORS,\n",
    "                'early_stopping_rounds': BOOSTING_EARLY_STOPPING,\n",
    "            })\n",
    "            model.fit(X_train, y_train, X_val, y_val)\n",
    "            \n",
    "            predictions = model.predict(X_val)\n",
    "            f1 = f1_score(y_val, predictions.class_predictions, average='macro')\n",
    "            fold_scores.append(f1)\n",
    "            \n",
    "            del model\n",
    "            clear_memory()\n",
    "        \n",
    "        print(f\"\\nCV Results for {best_model}:\")\n",
    "        print(f\"  Mean F1: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})\")\n",
    "        print(f\"  Fold scores: {[f'{s:.4f}' for s in fold_scores]}\")\n",
    "        \n",
    "        del container, X, y\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Cross-validation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. PHASE 4: ENSEMBLE (Optional)\n",
    "\n",
    "Combine multiple models for improved predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Train Ensemble { display-mode: \"form\" }\n",
    "\n",
    "if not TRAIN_ENSEMBLE:\n",
    "    print(\"[Skipped] Ensemble training disabled in configuration.\")\n",
    "    print(\"Set TRAIN_ENSEMBLE = True in Section 1 to enable.\")\n",
    "elif len(TRAINING_RESULTS) < 2:\n",
    "    print(\"[Error] Need at least 2 trained models for ensemble.\")\n",
    "    print(f\"Currently trained: {list(TRAINING_RESULTS.keys())}\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\" PHASE 4: {ENSEMBLE_TYPE.upper()} ENSEMBLE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    base_models = list(TRAINING_RESULTS.keys())\n",
    "    print(f\"\\n  Base models: {base_models}\")\n",
    "    print(f\"  Meta-learner: {ENSEMBLE_META_LEARNER}\")\n",
    "    \n",
    "    try:\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        # Load data\n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        \n",
    "        # Configure ensemble\n",
    "        config = TrainerConfig(\n",
    "            model_name=ENSEMBLE_TYPE,\n",
    "            horizon=TRAINING_HORIZON,\n",
    "            output_dir=EXPERIMENTS_DIR,\n",
    "            model_config={\n",
    "                \"base_model_names\": base_models,\n",
    "                \"meta_learner\": ENSEMBLE_META_LEARNER,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Train ensemble\n",
    "        trainer = Trainer(config)\n",
    "        results = trainer.run(container)\n",
    "        \n",
    "        metrics = results.get('evaluation_metrics', {})\n",
    "        print(f\"\\nEnsemble Results:\")\n",
    "        print(f\"  Accuracy: {metrics.get('accuracy', 0):.2%}\")\n",
    "        print(f\"  Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n",
    "        \n",
    "        # Compare to best single model\n",
    "        best_single = max(TRAINING_RESULTS.values(), key=lambda x: x['metrics'].get('macro_f1', 0))\n",
    "        best_f1 = best_single['metrics'].get('macro_f1', 0)\n",
    "        ensemble_f1 = metrics.get('macro_f1', 0)\n",
    "        \n",
    "        improvement = (ensemble_f1 - best_f1) / best_f1 * 100 if best_f1 > 0 else 0\n",
    "        print(f\"\\n  Improvement over best single model: {improvement:+.1f}%\")\n",
    "        \n",
    "        del trainer, config, container\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Ensemble training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. RESULTS & EXPORT\n",
    "\n",
    "Summary of all results and export options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 7.1 Final Summary { display-mode: \"form\" }\n\nprint(\"=\" * 70)\nprint(\" PIPELINE SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\\n Configuration:\")\nprint(f\"   Symbol: {SYMBOL}\")\n\n# Show auto-detected date range\nif 'DATA_START' in dir() and DATA_START is not None:\n    print(f\"   Date Range: {DATA_START.strftime('%Y-%m-%d')} to {DATA_END.strftime('%Y-%m-%d')}\")\n    print(f\"   Years: {DATA_START_YEAR} - {DATA_END_YEAR}\")\nelse:\n    print(f\"   Date Range: Not detected (run Section 3.1)\")\n\nprint(f\"   Training Horizon: H{TRAINING_HORIZON}\")\n\nif 'TRAIN_LEN' in dir():\n    print(f\"\\n Data:\")\n    print(f\"   Train: {TRAIN_LEN:,} samples\")\n    print(f\"   Val: {VAL_LEN:,} samples\")\n    print(f\"   Test: {TEST_LEN:,} samples\")\n\nif 'TRAINING_RESULTS' in dir() and TRAINING_RESULTS:\n    print(f\"\\n Model Results:\")\n    for model, data in sorted(TRAINING_RESULTS.items(), \n                              key=lambda x: x[1]['metrics'].get('macro_f1', 0), \n                              reverse=True):\n        metrics = data['metrics']\n        print(f\"   {model}: Acc={metrics.get('accuracy', 0):.2%}, F1={metrics.get('macro_f1', 0):.4f}\")\n    \n    best = max(TRAINING_RESULTS, key=lambda x: TRAINING_RESULTS[x]['metrics'].get('macro_f1', 0))\n    print(f\"\\n Best Model: {best}\")\n\nprint(f\"\\n Saved Artifacts:\")\nprint(f\"   Data: {SPLITS_DIR}\")\nprint(f\"   Models: {EXPERIMENTS_DIR}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\" PIPELINE COMPLETE\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Export Best Model { display-mode: \"form\" }\n",
    "\n",
    "export_model = False  #@param {type: \"boolean\"}\n",
    "export_format = \"pickle\"  #@param [\"pickle\", \"joblib\", \"onnx\"]\n",
    "\n",
    "if export_model and TRAINING_RESULTS:\n",
    "    from pathlib import Path\n",
    "    import joblib\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(TRAINING_RESULTS, key=lambda x: TRAINING_RESULTS[x]['metrics'].get('macro_f1', 0))\n",
    "    best_run_id = TRAINING_RESULTS[best_model_name].get('run_id', 'unknown')\n",
    "    \n",
    "    print(f\"Exporting best model: {best_model_name}\")\n",
    "    print(f\"Run ID: {best_run_id}\")\n",
    "    \n",
    "    # Find model files\n",
    "    model_dir = EXPERIMENTS_DIR / best_run_id\n",
    "    if model_dir.exists():\n",
    "        export_dir = RESULTS_DIR / 'exports'\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        export_path = export_dir / f\"{best_model_name}_{SYMBOL}_H{TRAINING_HORIZON}.{export_format}\"\n",
    "        \n",
    "        # Copy model files\n",
    "        import shutil\n",
    "        shutil.copytree(model_dir, export_dir / best_run_id, dirs_exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nExported to: {export_dir}\")\n",
    "    else:\n",
    "        print(f\"Model directory not found: {model_dir}\")\n",
    "else:\n",
    "    print(\"Model export skipped. Enable checkbox above to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Quick Reference\n",
    "\n",
    "## Command Line Usage\n",
    "\n",
    "```bash\n",
    "# Train single model\n",
    "python scripts/train_model.py --model xgboost --horizon 20\n",
    "\n",
    "# Train neural model\n",
    "python scripts/train_model.py --model lstm --horizon 20 --seq-len 60\n",
    "\n",
    "# Run cross-validation\n",
    "python scripts/run_cv.py --models xgboost,lightgbm --horizons 20 --n-splits 5\n",
    "\n",
    "# Train ensemble\n",
    "python scripts/train_model.py --model voting --horizon 20\n",
    "\n",
    "# List all available models\n",
    "python scripts/train_model.py --list-models\n",
    "```\n",
    "\n",
    "## Model Families\n",
    "\n",
    "| Family | Models | Best For |\n",
    "|--------|--------|----------|\n",
    "| Boosting | XGBoost, LightGBM, CatBoost | Fast, accurate, tabular data |\n",
    "| Classical | Random Forest, Logistic, SVM | Baselines, interpretability |\n",
    "| Neural | LSTM, GRU, TCN | Sequential patterns, temporal dependencies |\n",
    "| Ensemble | Voting, Stacking, Blending | Combined predictions, robustness |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}