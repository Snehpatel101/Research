{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Unified Pipeline\n",
    "\n",
    "**Complete ML training pipeline for OHLCV time series with 13 models.**\n",
    "\n",
    "## \u2728 Features (Production Ready)\n",
    "\n",
    "### \ud83e\udd16 Model Support (13 Models)\n",
    "- **Boosting (3):** XGBoost, LightGBM, CatBoost\n",
    "- **Neural (4):** LSTM, GRU, TCN, Transformer\n",
    "- **Classical (3):** Random Forest, Logistic Regression, SVM\n",
    "- **Ensemble (3):** Voting, Stacking, Blending\n",
    "\n",
    "### \ud83d\udd2c Advanced Capabilities\n",
    "- **Transformer Support:** Self-attention with 8-head architecture + attention visualization\n",
    "- **Hyperparameter Tuning:** Optuna integration with 20+ trials per model\n",
    "- **Cross-Validation:** Purged K-fold for time series (prevents lookahead bias)\n",
    "- **Ensemble Intelligence:** Diversity analysis, contribution metrics, production recommendations\n",
    "- **Professional Export:** ONNX conversion, model cards, ZIP packages\n",
    "\n",
    "### \ud83d\udcca Rich Visualizations\n",
    "- Confusion matrices with class-wise accuracy\n",
    "- Feature importance (boosting models)\n",
    "- Learning curves (training/validation loss)\n",
    "- Prediction distribution analysis\n",
    "- Per-class precision/recall/F1 metrics\n",
    "- **Transformer attention heatmaps** (NEW)\n",
    "\n",
    "### \ud83d\udcc8 Evaluation & Analysis\n",
    "- Test set evaluation with generalization gap analysis\n",
    "- Out-of-fold predictions for stacking\n",
    "- Cross-validation with time series purge/embargo\n",
    "- Ensemble diversity metrics (disagreement, correlation, Q-statistic)\n",
    "- Production readiness scoring\n",
    "\n",
    "## Pipeline Phases\n",
    "1. **Configuration** - All settings in one place (13 model toggles)\n",
    "2. **Environment Setup** - Auto-detects Colab vs Local\n",
    "3. **Phase 1: Data Pipeline** - Clean \u2192 Features \u2192 Labels \u2192 Splits \u2192 Scale\n",
    "4. **Phase 2: Model Training** - Train any of 13 model types\n",
    "   - 4.1 Train Models\n",
    "   - 4.2 Training Summary\n",
    "   - 4.3 Visualizations (5 types)\n",
    "   - 4.4 Transformer Attention (NEW)\n",
    "   - 4.5 Test Set Performance\n",
    "5. **Phase 3: Cross-Validation** - Robust evaluation with tuning (optional)\n",
    "6. **Phase 4: Ensemble** - Combine models intelligently (optional)\n",
    "7. **Results & Export** - Professional packages with ONNX\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MASTER CONFIGURATION\n",
    "\n",
    "**Configure ALL settings here. No need to modify any other cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 Master Configuration Panel { display-mode: \"form\" }\n",
    "#@markdown ## Data Configuration\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ### Contract Selection\n",
    "SYMBOL = \"SI\"  #@param [\"SI\", \"MES\", \"MGC\", \"ES\", \"GC\", \"NQ\", \"CL\", \"HG\", \"ZB\", \"ZN\"]\n",
    "#@markdown Select ONE contract. Each contract is trained in complete isolation.\n",
    "#@markdown - **SI** = Silver, **MES** = Micro E-mini S&P, **MGC** = Micro Gold\n",
    "#@markdown - **ES** = E-mini S&P, **GC** = Gold, **NQ** = E-mini Nasdaq\n",
    "#@markdown - **CL** = Crude Oil, **HG** = Copper, **ZB/ZN** = Bonds\n",
    "\n",
    "#@markdown ### Date Range Selection\n",
    "DATE_RANGE = \"2019-2024\"  #@param [\"2019-2024\", \"2020-2024\", \"2021-2024\", \"2022-2024\", \"2023-2024\", \"Full Dataset\"]\n",
    "#@markdown Select the date range for your data\n",
    "\n",
    "#@markdown ### Data Source\n",
    "DRIVE_DATA_PATH = \"research/data/raw\"  #@param {type: \"string\"}\n",
    "#@markdown Google Drive path relative to My Drive\n",
    "\n",
    "#@markdown ### Custom Data File (optional)\n",
    "CUSTOM_DATA_FILE = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Leave empty for auto-detection, or specify exact filename (e.g., `si_historical_2019_2024.parquet`)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Pipeline Configuration\n",
    "\n",
    "#@markdown ### Label Horizons (bars)\n",
    "HORIZONS = \"5,10,15,20\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated prediction horizons\n",
    "\n",
    "#@markdown ### Train/Val/Test Split Ratios\n",
    "TRAIN_RATIO = 0.70  #@param {type: \"number\"}\n",
    "VAL_RATIO = 0.15  #@param {type: \"number\"}\n",
    "TEST_RATIO = 0.15  #@param {type: \"number\"}\n",
    "\n",
    "#@markdown ### Leakage Prevention\n",
    "PURGE_BARS = 60  #@param {type: \"integer\"}\n",
    "#@markdown Bars to purge around train/val boundary (3x max horizon)\n",
    "EMBARGO_BARS = 1440  #@param {type: \"integer\"}\n",
    "#@markdown Embargo period after validation (~5 days at 5-min)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Model Training Configuration\n",
    "\n",
    "#@markdown ### Training Horizon\n",
    "TRAINING_HORIZON = 20  #@param [5, 10, 15, 20]\n",
    "#@markdown Which horizon to train models on\n",
    "\n",
    "#@markdown ### Model Selection\n",
    "#@markdown #### Boosting Models\n",
    "TRAIN_XGBOOST = True  #@param {type: \"boolean\"}\n",
    "TRAIN_LIGHTGBM = True  #@param {type: \"boolean\"}\n",
    "TRAIN_CATBOOST = True  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Classical Models\n",
    "TRAIN_RANDOM_FOREST = False  #@param {type: \"boolean\"}\n",
    "TRAIN_LOGISTIC = False  #@param {type: \"boolean\"}\n",
    "TRAIN_SVM = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Neural Network Models\n",
    "TRAIN_LSTM = False  #@param {type: \"boolean\"}\n",
    "TRAIN_GRU = False  #@param {type: \"boolean\"}\n",
    "TRAIN_TCN = False  #@param {type: \"boolean\"}\n",
    "TRAIN_TRANSFORMER = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Ensemble Models\n",
    "TRAIN_VOTING = False  #@param {type: \"boolean\"}\n",
    "TRAIN_STACKING = False  #@param {type: \"boolean\"}\n",
    "TRAIN_BLENDING = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ### Neural Network Settings\n",
    "SEQUENCE_LENGTH = 60  #@param {type: \"slider\", min: 30, max: 120, step: 10}\n",
    "BATCH_SIZE = 256  #@param [64, 128, 256, 512, 1024]\n",
    "MAX_EPOCHS = 50  #@param {type: \"integer\"}\n",
    "EARLY_STOPPING_PATIENCE = 10  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Transformer Settings (when enabled)\n",
    "TRANSFORMER_SEQUENCE_LENGTH = 128  #@param {type: \"integer\"}\n",
    "TRANSFORMER_N_HEADS = 8  #@param [4, 8, 16]\n",
    "TRANSFORMER_N_LAYERS = 3  #@param [2, 3, 4, 6]\n",
    "TRANSFORMER_D_MODEL = 256  #@param [128, 256, 512]\n",
    "\n",
    "#@markdown ### Boosting Settings\n",
    "N_ESTIMATORS = 500  #@param {type: \"integer\"}\n",
    "BOOSTING_EARLY_STOPPING = 50  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Voting Ensemble Configuration (when enabled)\n",
    "VOTING_BASE_MODELS = \"xgboost,lightgbm,catboost\"  #@param {type: \"string\"}\n",
    "VOTING_WEIGHTS = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Leave weights empty for equal weighting\n",
    "\n",
    "#@markdown ### Stacking Ensemble Configuration (when enabled)\n",
    "STACKING_BASE_MODELS = \"xgboost,lightgbm,lstm\"  #@param {type: \"string\"}\n",
    "STACKING_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"xgboost\", \"random_forest\"]\n",
    "STACKING_N_FOLDS = 5  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Blending Ensemble Configuration (when enabled)\n",
    "BLENDING_BASE_MODELS = \"xgboost,lightgbm,random_forest\"  #@param {type: \"string\"}\n",
    "BLENDING_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"xgboost\", \"random_forest\"]\n",
    "BLENDING_HOLDOUT_RATIO = 0.2  #@param {type: \"number\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Optional Phases\n",
    "\n",
    "#@markdown ### Cross-Validation\n",
    "RUN_CROSS_VALIDATION = False  #@param {type: \"boolean\"}\n",
    "CV_N_SPLITS = 5  #@param {type: \"integer\"}\n",
    "CV_TUNE_HYPERPARAMS = False  #@param {type: \"boolean\"}\n",
    "CV_N_TRIALS = 20  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Ensemble Training\n",
    "TRAIN_ENSEMBLE = False  #@param {type: \"boolean\"}\n",
    "ENSEMBLE_TYPE = \"voting\"  #@param [\"voting\", \"stacking\", \"blending\"]\n",
    "ENSEMBLE_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"random_forest\", \"xgboost\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Execution Options\n",
    "\n",
    "#@markdown ### What to Run\n",
    "RUN_DATA_PIPELINE = True  #@param {type: \"boolean\"}\n",
    "#@markdown Run Phase 1 data pipeline\n",
    "RUN_MODEL_TRAINING = True  #@param {type: \"boolean\"}\n",
    "#@markdown Run Phase 2 model training\n",
    "\n",
    "#@markdown ### Memory Management\n",
    "SAFE_MODE = False  #@param {type: \"boolean\"}\n",
    "#@markdown Enable for low-memory environments (reduces batch size, limits iterations)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD CONFIGURATION (DO NOT MODIFY BELOW)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Parse horizons\n",
    "HORIZON_LIST = [int(h.strip()) for h in HORIZONS.split(',')]\n",
    "\n",
    "# Parse date range\n",
    "if DATE_RANGE == \"Full Dataset\":\n",
    "    YEAR_START = None\n",
    "    YEAR_END = None\n",
    "else:\n",
    "    years = DATE_RANGE.split('-')\n",
    "    YEAR_START = int(years[0])\n",
    "    YEAR_END = int(years[1])\n",
    "\n",
    "# Build model list\n",
    "MODELS_TO_TRAIN = []\n",
    "if TRAIN_XGBOOST: MODELS_TO_TRAIN.append('xgboost')\n",
    "if TRAIN_LIGHTGBM: MODELS_TO_TRAIN.append('lightgbm')\n",
    "if TRAIN_CATBOOST: MODELS_TO_TRAIN.append('catboost')\n",
    "if TRAIN_RANDOM_FOREST: MODELS_TO_TRAIN.append('random_forest')\n",
    "if TRAIN_LOGISTIC: MODELS_TO_TRAIN.append('logistic')\n",
    "if TRAIN_SVM: MODELS_TO_TRAIN.append('svm')\n",
    "if TRAIN_LSTM: MODELS_TO_TRAIN.append('lstm')\n",
    "if TRAIN_GRU: MODELS_TO_TRAIN.append('gru')\n",
    "if TRAIN_TCN: MODELS_TO_TRAIN.append('tcn')\n",
    "if TRAIN_TRANSFORMER: MODELS_TO_TRAIN.append('transformer')\n",
    "if TRAIN_VOTING: MODELS_TO_TRAIN.append('voting')\n",
    "if TRAIN_STACKING: MODELS_TO_TRAIN.append('stacking')\n",
    "if TRAIN_BLENDING: MODELS_TO_TRAIN.append('blending')\n",
    "\n",
    "# Date range will be auto-detected from data file\n",
    "DATA_START = None  # Auto-detected\n",
    "DATA_END = None    # Auto-detected\n",
    "\n",
    "# Safe mode adjustments\n",
    "if SAFE_MODE:\n",
    "    BATCH_SIZE = min(BATCH_SIZE, 64)\n",
    "    N_ESTIMATORS = min(N_ESTIMATORS, 300)\n",
    "    SEQUENCE_LENGTH = min(SEQUENCE_LENGTH, 30)\n",
    "    TRANSFORMER_SEQUENCE_LENGTH = min(TRANSFORMER_SEQUENCE_LENGTH, 64)\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=\" * 70)\n",
    "print(\" ML PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  Contract:        {SYMBOL}\")\n",
    "print(f\"  Date Range:      {DATE_RANGE}\")\n",
    "if CUSTOM_DATA_FILE:\n",
    "    print(f\"  Custom File:     {CUSTOM_DATA_FILE}\")\n",
    "print(f\"  Horizons:        {HORIZON_LIST}\")\n",
    "print(f\"  Split Ratios:    {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")\n",
    "print(f\"  Training Horizon: H{TRAINING_HORIZON}\")\n",
    "print(f\"  Models:          {MODELS_TO_TRAIN if MODELS_TO_TRAIN else 'None selected'}\")\n",
    "if MODELS_TO_TRAIN:\n",
    "    boosting_models = [m for m in MODELS_TO_TRAIN if m in ['xgboost', 'lightgbm', 'catboost']]\n",
    "    classical_models = [m for m in MODELS_TO_TRAIN if m in ['random_forest', 'logistic', 'svm']]\n",
    "    neural_models = [m for m in MODELS_TO_TRAIN if m in ['lstm', 'gru', 'tcn', 'transformer']]\n",
    "    ensemble_models = [m for m in MODELS_TO_TRAIN if m in ['voting', 'stacking', 'blending']]\n",
    "    if boosting_models:\n",
    "        print(f\"    Boosting:      {boosting_models}\")\n",
    "    if classical_models:\n",
    "        print(f\"    Classical:     {classical_models}\")\n",
    "    if neural_models:\n",
    "        print(f\"    Neural:        {neural_models}\")\n",
    "    if ensemble_models:\n",
    "        print(f\"    Ensemble:      {ensemble_models}\")\n",
    "print(f\"\\n  Run Pipeline:    {RUN_DATA_PIPELINE}\")\n",
    "print(f\"  Run Training:    {RUN_MODEL_TRAINING}\")\n",
    "print(f\"  Cross-Validation: {RUN_CROSS_VALIDATION}\")\n",
    "print(f\"  Ensemble:        {TRAIN_ENSEMBLE}\")\n",
    "print(f\"  Safe Mode:       {SAFE_MODE}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration complete! Run the next cells sequentially.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ENVIRONMENT SETUP\n",
    "\n",
    "Auto-detects Colab vs Local environment and sets up paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Environment Detection & Setup { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# ENVIRONMENT DETECTION\n",
    "# ============================================================\n",
    "IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\n[Environment] Google Colab detected\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone/update repository\n",
    "    REPO_PATH = Path('/content/research')\n",
    "    if not REPO_PATH.exists():\n",
    "        print(\"\\n[Setup] Cloning repository...\")\n",
    "        !git clone https://github.com/Snehpatel101/research.git /content/research\n",
    "    else:\n",
    "        print(\"\\n[Setup] Updating repository...\")\n",
    "        !cd /content/research && git pull --quiet\n",
    "    \n",
    "    # Set paths\n",
    "    PROJECT_ROOT = REPO_PATH\n",
    "    DRIVE_ROOT = Path('/content/drive/MyDrive')\n",
    "    RAW_DATA_DIR = DRIVE_ROOT / DRIVE_DATA_PATH\n",
    "    RESULTS_DIR = DRIVE_ROOT / 'research/experiments'\n",
    "    \n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[Environment] Local environment detected\")\n",
    "    \n",
    "    PROJECT_ROOT = Path('.')\n",
    "    DRIVE_ROOT = None\n",
    "    RAW_DATA_DIR = PROJECT_ROOT / 'data/raw'\n",
    "    RESULTS_DIR = PROJECT_ROOT / 'experiments'\n",
    "    \n",
    "    os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Add to Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Create output directories\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "EXPERIMENTS_DIR = RESULTS_DIR / 'runs'\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n  Project Root:  {PROJECT_ROOT}\")\n",
    "print(f\"  Raw Data:      {RAW_DATA_DIR}\")\n",
    "print(f\"  Splits:        {SPLITS_DIR}\")\n",
    "print(f\"  Experiments:   {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.2 Install Dependencies { display-mode: \"form\" }\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"[Dependencies] Installing packages...\")\n",
    "    !pip install -q torch xgboost lightgbm catboost optuna ta pywavelets scikit-learn pandas numpy matplotlib tqdm pyarrow numba psutil\n",
    "    print(\"[Dependencies] Installation complete!\")\n",
    "else:\n",
    "    print(\"[Dependencies] Local environment - assuming packages installed.\")\n",
    "    print(\"  If needed: pip install torch xgboost lightgbm catboost optuna ta pywavelets psutil\")\n",
    "\n",
    "# Verify imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"\\n  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.3 GPU Detection { display-mode: \"form\" }\n",
    "\n",
    "import torch\n",
    "\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_NAME = None\n",
    "GPU_MEMORY = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" HARDWARE DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    GPU_NAME = props.name\n",
    "    GPU_MEMORY = props.total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"\\n  GPU: {GPU_NAME}\")\n",
    "    print(f\"  Memory: {GPU_MEMORY:.1f} GB\")\n",
    "    print(f\"  Compute: {props.major}.{props.minor}\")\n",
    "    \n",
    "    # Adjust batch size based on GPU memory\n",
    "    if GPU_MEMORY >= 40:\n",
    "        RECOMMENDED_BATCH = 1024\n",
    "    elif GPU_MEMORY >= 15:\n",
    "        RECOMMENDED_BATCH = 512\n",
    "    else:\n",
    "        RECOMMENDED_BATCH = 256\n",
    "    \n",
    "    print(f\"  Recommended batch: {RECOMMENDED_BATCH}\")\n",
    "else:\n",
    "    print(\"\\n  GPU: Not available (using CPU)\")\n",
    "    print(\"  Tip: Runtime -> Change runtime type -> GPU\")\n",
    "    RECOMMENDED_BATCH = 128\n",
    "\n",
    "# Check for neural models without GPU\n",
    "NEURAL_MODELS = {'lstm', 'gru', 'tcn'}\n",
    "selected_neural = set(MODELS_TO_TRAIN) & NEURAL_MODELS\n",
    "if selected_neural and not GPU_AVAILABLE:\n",
    "    print(f\"\\n  [WARNING] Neural models selected but no GPU: {selected_neural}\")\n",
    "    print(\"  Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.4 Memory Utilities { display-mode: \"form\" }\n",
    "\n",
    "import psutil\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Ensure GPU_AVAILABLE is defined (in case cells run out of order)\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "def print_memory_status(label: str = \"Current\"):\n",
    "    \"\"\"Print current RAM and GPU memory usage.\"\"\"\n",
    "    print(f\"\\n--- Memory: {label} ---\")\n",
    "    \n",
    "    # RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM: {ram.used/1e9:.1f}GB / {ram.total/1e9:.1f}GB ({ram.percent}%)\")\n",
    "    \n",
    "    # GPU\n",
    "    if GPU_AVAILABLE:\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"GPU: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear RAM and GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "print(\"Memory utilities loaded.\")\n",
    "print_memory_status(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. PHASE 1: DATA PIPELINE\n",
    "\n",
    "Processes raw OHLCV data into training-ready datasets.\n",
    "\n",
    "**Pipeline stages:**\n",
    "1. Load raw 1-minute data\n",
    "2. Clean and resample to 5-minute bars\n",
    "3. Generate 150+ technical features\n",
    "4. Apply triple-barrier labeling\n",
    "5. Create train/val/test splits with purge/embargo\n",
    "6. Scale features (train-only fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Verify Raw Data & Detect Date Range { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined (in case cells run out of order)\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'RAW_DATA_DIR' not in dir():\n",
    "    if IS_COLAB:\n",
    "        RAW_DATA_DIR = Path('/content/drive/MyDrive') / DRIVE_DATA_PATH\n",
    "    else:\n",
    "        RAW_DATA_DIR = Path('./data/raw')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" RAW DATA VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nLooking for {SYMBOL} data in: {RAW_DATA_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# FLEXIBLE FILE DETECTION\n",
    "# ============================================================\n",
    "RAW_DATA_FILE = None\n",
    "\n",
    "# If custom file specified, use it directly\n",
    "if CUSTOM_DATA_FILE:\n",
    "    custom_path = RAW_DATA_DIR / CUSTOM_DATA_FILE\n",
    "    if custom_path.exists():\n",
    "        RAW_DATA_FILE = custom_path\n",
    "        print(f\"\\n  Using custom file: {CUSTOM_DATA_FILE}\")\n",
    "    else:\n",
    "        print(f\"\\n  [WARNING] Custom file not found: {CUSTOM_DATA_FILE}\")\n",
    "\n",
    "# Auto-detect file with flexible patterns\n",
    "if RAW_DATA_FILE is None and RAW_DATA_DIR.exists():\n",
    "    symbol_lower = SYMBOL.lower()\n",
    "    symbol_upper = SYMBOL.upper()\n",
    "    \n",
    "    # Build list of all matching files\n",
    "    matching_files = []\n",
    "    \n",
    "    for f in RAW_DATA_DIR.iterdir():\n",
    "        if f.suffix not in ['.parquet', '.csv']:\n",
    "            continue\n",
    "        \n",
    "        fname_lower = f.name.lower()\n",
    "        \n",
    "        # Check if file contains the symbol (case-insensitive)\n",
    "        if symbol_lower in fname_lower:\n",
    "            # Priority scoring: prefer files with date range matching config\n",
    "            priority = 0\n",
    "            \n",
    "            # Boost priority if filename contains the configured date range\n",
    "            if YEAR_START and YEAR_END:\n",
    "                date_pattern = f\"{YEAR_START}_{YEAR_END}|{YEAR_START}-{YEAR_END}\"\n",
    "                if re.search(date_pattern, fname_lower):\n",
    "                    priority += 10\n",
    "            \n",
    "            # Boost priority for common naming patterns\n",
    "            if '_1m' in fname_lower or '_1min' in fname_lower:\n",
    "                priority += 5\n",
    "            if 'historical' in fname_lower:\n",
    "                priority += 3\n",
    "            if f.suffix == '.parquet':\n",
    "                priority += 2  # Prefer parquet over CSV\n",
    "            \n",
    "            matching_files.append((priority, f))\n",
    "    \n",
    "    # Sort by priority (highest first) and pick best match\n",
    "    if matching_files:\n",
    "        matching_files.sort(key=lambda x: x[0], reverse=True)\n",
    "        RAW_DATA_FILE = matching_files[0][1]\n",
    "        \n",
    "        if len(matching_files) > 1:\n",
    "            print(f\"\\n  Found {len(matching_files)} matching files:\")\n",
    "            for pri, f in matching_files[:5]:\n",
    "                marker = \"\u2192\" if f == RAW_DATA_FILE else \" \"\n",
    "                print(f\"    {marker} {f.name} (priority: {pri})\")\n",
    "\n",
    "# ============================================================\n",
    "# VALIDATE AND LOAD DATA\n",
    "# ============================================================\n",
    "if RAW_DATA_FILE:\n",
    "    size_mb = RAW_DATA_FILE.stat().st_size / 1e6\n",
    "    print(f\"\\n  Selected: {RAW_DATA_FILE.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Load and validate\n",
    "    print(\"  Loading data...\")\n",
    "    if RAW_DATA_FILE.suffix == '.parquet':\n",
    "        df_raw = pd.read_parquet(RAW_DATA_FILE)\n",
    "    else:\n",
    "        df_raw = pd.read_csv(RAW_DATA_FILE)\n",
    "    \n",
    "    print(f\"  Rows: {len(df_raw):,}\")\n",
    "    print(f\"  Columns: {list(df_raw.columns)}\")\n",
    "    \n",
    "    # Validate OHLCV columns (case-insensitive)\n",
    "    required = {'open', 'high', 'low', 'close', 'volume'}\n",
    "    found = {c.lower() for c in df_raw.columns}\n",
    "    if required.issubset(found):\n",
    "        print(\"  OHLCV columns: \u2713 OK\")\n",
    "    else:\n",
    "        missing = required - found\n",
    "        print(f\"  [ERROR] Missing columns: {missing}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # AUTO-DETECT DATE RANGE FROM DATA\n",
    "    # ============================================================\n",
    "    date_col = None\n",
    "    for c in df_raw.columns:\n",
    "        if 'date' in c.lower() or 'time' in c.lower():\n",
    "            date_col = c\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        df_raw[date_col] = pd.to_datetime(df_raw[date_col])\n",
    "        \n",
    "        # Store globally for pipeline use\n",
    "        DATA_START = df_raw[date_col].min()\n",
    "        DATA_END = df_raw[date_col].max()\n",
    "        DATA_START_YEAR = DATA_START.year\n",
    "        DATA_END_YEAR = DATA_END.year\n",
    "        \n",
    "        print(f\"\\n  [DATE RANGE DETECTED]\")\n",
    "        print(f\"  Start: {DATA_START.strftime('%Y-%m-%d %H:%M')} ({DATA_START_YEAR})\")\n",
    "        print(f\"  End:   {DATA_END.strftime('%Y-%m-%d %H:%M')} ({DATA_END_YEAR})\")\n",
    "        print(f\"  Span:  {(DATA_END - DATA_START).days:,} days ({DATA_END_YEAR - DATA_START_YEAR + 1} years)\")\n",
    "        \n",
    "        # Validate against configured date range\n",
    "        if YEAR_START and YEAR_END:\n",
    "            if DATA_START_YEAR <= YEAR_START and DATA_END_YEAR >= YEAR_END:\n",
    "                print(f\"  Config Match: \u2713 Data covers {DATE_RANGE}\")\n",
    "            else:\n",
    "                print(f\"  [WARNING] Data range ({DATA_START_YEAR}-{DATA_END_YEAR}) differs from config ({DATE_RANGE})\")\n",
    "    else:\n",
    "        print(\"  [WARNING] No datetime column found - using index\")\n",
    "        DATA_START = None\n",
    "        DATA_END = None\n",
    "        DATA_START_YEAR = YEAR_START or 2019\n",
    "        DATA_END_YEAR = YEAR_END or 2024\n",
    "    \n",
    "    del df_raw\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\n  \u2713 Data verified and ready for processing!\")\n",
    "else:\n",
    "    print(f\"\\n  [ERROR] No data file found for {SYMBOL}!\")\n",
    "    print(f\"  Expected location: {RAW_DATA_DIR}\")\n",
    "    print(f\"\\n  Tried patterns matching '{SYMBOL}' (case-insensitive)\")\n",
    "    print(f\"\\n  Available files in directory:\")\n",
    "    if RAW_DATA_DIR.exists():\n",
    "        for f in sorted(RAW_DATA_DIR.iterdir()):\n",
    "            if f.suffix in ['.csv', '.parquet']:\n",
    "                print(f\"    - {f.name}\")\n",
    "    else:\n",
    "        print(f\"    Directory does not exist!\")\n",
    "    \n",
    "    print(f\"\\n  [FIX] Set CUSTOM_DATA_FILE in Section 1 to your exact filename\")\n",
    "    \n",
    "    RAW_DATA_FILE = None\n",
    "    DATA_START = None\n",
    "    DATA_END = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 Run Data Pipeline { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'RUN_DATA_PIPELINE' not in dir():\n",
    "    RUN_DATA_PIPELINE = True\n",
    "if 'RAW_DATA_FILE' not in dir():\n",
    "    RAW_DATA_FILE = None\n",
    "\n",
    "if not RUN_DATA_PIPELINE:\n",
    "    print(\"[Skipped] Data pipeline disabled in configuration.\")\n",
    "    print(\"Set RUN_DATA_PIPELINE = True in Section 1 to enable.\")\n",
    "elif RAW_DATA_FILE is None:\n",
    "    print(\"[Error] No raw data file found. Cannot run pipeline.\")\n",
    "    print(\"Run Section 3.1 first to detect data files.\")\n",
    "else:\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 1: DATA PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n  Symbol: {SYMBOL}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # COPY DATA FROM DRIVE TO PROJECT (Colab only)\n",
    "    # ============================================================\n",
    "    if IS_COLAB and RAW_DATA_FILE is not None:\n",
    "        # The pipeline expects data in PROJECT_ROOT/data/raw/\n",
    "        project_raw_dir = PROJECT_ROOT / 'data/raw'\n",
    "        project_raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create standardized filename for pipeline: SYMBOL_1m.parquet\n",
    "        target_filename = f\"{SYMBOL}_1m{RAW_DATA_FILE.suffix}\"\n",
    "        target_path = project_raw_dir / target_filename\n",
    "        \n",
    "        # Only copy if source is not already in project directory\n",
    "        source_in_project = str(RAW_DATA_FILE).startswith(str(PROJECT_ROOT))\n",
    "        if not source_in_project:\n",
    "            if not target_path.exists() or target_path.stat().st_size != RAW_DATA_FILE.stat().st_size:\n",
    "                print(f\"\\n  [Setup] Copying data from Drive to project...\")\n",
    "                print(f\"    From: {RAW_DATA_FILE}\")\n",
    "                print(f\"    To:   {target_path}\")\n",
    "                shutil.copy2(RAW_DATA_FILE, target_path)\n",
    "                print(f\"    Done! ({target_path.stat().st_size / 1e6:.1f} MB)\")\n",
    "            else:\n",
    "                print(f\"\\n  [Setup] Data already in project: {target_path.name}\")\n",
    "        else:\n",
    "            print(f\"\\n  [Setup] Data already in project directory\")\n",
    "    \n",
    "    # Use auto-detected date range\n",
    "    if 'DATA_START' in dir() and DATA_START is not None:\n",
    "        print(f\"  Date Range: {DATA_START.strftime('%Y-%m-%d')} to {DATA_END.strftime('%Y-%m-%d')} (auto-detected)\")\n",
    "        start_date_str = DATA_START.strftime('%Y-%m-%d')\n",
    "        end_date_str = DATA_END.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        print(f\"  Date Range: Full dataset (no filter)\")\n",
    "        start_date_str = None\n",
    "        end_date_str = None\n",
    "    \n",
    "    print(f\"  Horizons: {HORIZON_LIST}\")\n",
    "    print(f\"  Purge: {PURGE_BARS} bars, Embargo: {EMBARGO_BARS} bars\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        from src.phase1.pipeline_config import PipelineConfig\n",
    "        from src.pipeline.runner import PipelineRunner\n",
    "        \n",
    "        # Configure pipeline with auto-detected dates\n",
    "        # NOTE: auto_scale_purge_embargo=False uses our explicit PURGE_BARS/EMBARGO_BARS\n",
    "        config = PipelineConfig(\n",
    "            symbols=[SYMBOL],\n",
    "            project_root=PROJECT_ROOT,\n",
    "            label_horizons=HORIZON_LIST,\n",
    "            train_ratio=TRAIN_RATIO,\n",
    "            val_ratio=VAL_RATIO,\n",
    "            test_ratio=TEST_RATIO,\n",
    "            purge_bars=PURGE_BARS,\n",
    "            embargo_bars=EMBARGO_BARS,\n",
    "            start_date=start_date_str,\n",
    "            end_date=end_date_str,\n",
    "            allow_batch_symbols=False,  # Single-contract architecture\n",
    "            auto_scale_purge_embargo=False,  # Use explicit purge/embargo values\n",
    "        )\n",
    "        \n",
    "        # Run pipeline\n",
    "        runner = PipelineRunner(config)\n",
    "        success = runner.run()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\n  Pipeline completed in {elapsed/60:.1f} minutes\")\n",
    "            \n",
    "            # Verify output\n",
    "            if (SPLITS_DIR / 'train_scaled.parquet').exists():\n",
    "                for split in ['train', 'val', 'test']:\n",
    "                    df = pd.read_parquet(SPLITS_DIR / f'{split}_scaled.parquet')\n",
    "                    print(f\"  {split}: {len(df):,} samples\")\n",
    "                    del df\n",
    "                gc.collect()\n",
    "                print(\"\\n  Data ready for training!\")\n",
    "        else:\n",
    "            print(\"\\n  [ERROR] Pipeline failed. Check logs above.\")\n",
    "        \n",
    "        del runner, config\n",
    "        if 'clear_memory' in dir():\n",
    "            clear_memory()\n",
    "        else:\n",
    "            gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  [ERROR] Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.3 Verify Processed Data { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined (in case cells run out of order)\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" PROCESSED DATA VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for pre-processed data (local) or pipeline output (Colab)\n",
    "if not IS_COLAB:\n",
    "    # Local: check pre-processed data\n",
    "    local_splits = PROJECT_ROOT / 'data/splits/final_correct/scaled'\n",
    "    if (local_splits / 'train_scaled.parquet').exists():\n",
    "        SPLITS_DIR = local_splits\n",
    "        print(f\"\\nUsing pre-processed data: {SPLITS_DIR}\")\n",
    "\n",
    "if (SPLITS_DIR / 'train_scaled.parquet').exists():\n",
    "    # Load metadata without keeping DataFrames\n",
    "    train_df = pd.read_parquet(SPLITS_DIR / 'train_scaled.parquet')\n",
    "    \n",
    "    FEATURE_COLS = [c for c in train_df.columns \n",
    "                   if not c.startswith(('label_', 'sample_weight', 'quality_', 'datetime', 'symbol'))]\n",
    "    LABEL_COLS = [c for c in train_df.columns if c.startswith('label_')]\n",
    "    TRAIN_LEN = len(train_df)\n",
    "    \n",
    "    # Label distribution with safety check\n",
    "    label_dists = {}\n",
    "    for col in LABEL_COLS:\n",
    "        label_dists[col] = train_df[col].value_counts().sort_index().to_dict()\n",
    "    \n",
    "    del train_df\n",
    "    \n",
    "    # Get val/test sizes\n",
    "    val_df = pd.read_parquet(SPLITS_DIR / 'val_scaled.parquet')\n",
    "    VAL_LEN = len(val_df)\n",
    "    del val_df\n",
    "    \n",
    "    test_df = pd.read_parquet(SPLITS_DIR / 'test_scaled.parquet')\n",
    "    TEST_LEN = len(test_df)\n",
    "    del test_df\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"  Train: {TRAIN_LEN:,} samples\")\n",
    "    print(f\"  Val:   {VAL_LEN:,} samples\")\n",
    "    print(f\"  Test:  {TEST_LEN:,} samples\")\n",
    "    print(f\"  Total: {TRAIN_LEN + VAL_LEN + TEST_LEN:,} samples\")\n",
    "    print(f\"\\n  Features: {len(FEATURE_COLS)}\")\n",
    "    print(f\"  Labels: {LABEL_COLS}\")\n",
    "    \n",
    "    print(f\"\\nLabel Distribution (train):\")\n",
    "    for col, dist in label_dists.items():\n",
    "        total = sum(dist.values())\n",
    "        if total == 0:\n",
    "            print(f\"  {col}: No valid samples!\")\n",
    "            continue\n",
    "        long_pct = dist.get(1, 0) / total * 100\n",
    "        neutral_pct = dist.get(0, 0) / total * 100\n",
    "        short_pct = dist.get(-1, 0) / total * 100\n",
    "        print(f\"  {col}: Long={long_pct:.1f}% | Neutral={neutral_pct:.1f}% | Short={short_pct:.1f}%\")\n",
    "    \n",
    "    # Validate TRAINING_HORIZON is in available labels\n",
    "    if 'TRAINING_HORIZON' in dir() and 'HORIZON_LIST' in dir():\n",
    "        if TRAINING_HORIZON not in HORIZON_LIST:\n",
    "            print(f\"\\n  [WARNING] TRAINING_HORIZON={TRAINING_HORIZON} not in HORIZON_LIST={HORIZON_LIST}\")\n",
    "            print(f\"  Model training may fail. Update TRAINING_HORIZON in Section 1.\")\n",
    "    \n",
    "    DATA_READY = True\n",
    "    print(\"\\n  Data verified and ready for training!\")\n",
    "else:\n",
    "    print(\"\\n[ERROR] Processed data not found!\")\n",
    "    print(f\"  Expected: {SPLITS_DIR}/train_scaled.parquet\")\n",
    "    print(\"  Run Section 3.2 to process raw data.\")\n",
    "    DATA_READY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. PHASE 2: MODEL TRAINING\n",
    "\n",
    "Train selected models on the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Train Models { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'RUN_MODEL_TRAINING' not in dir():\n",
    "    RUN_MODEL_TRAINING = True\n",
    "if 'DATA_READY' not in dir():\n",
    "    DATA_READY = (SPLITS_DIR / 'train_scaled.parquet').exists()\n",
    "if 'MODELS_TO_TRAIN' not in dir():\n",
    "    MODELS_TO_TRAIN = ['xgboost', 'lightgbm', 'catboost']\n",
    "if 'HORIZON_LIST' not in dir():\n",
    "    HORIZON_LIST = [5, 10, 15, 20]\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "# Define clear_memory if not available\n",
    "if 'clear_memory' not in dir():\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        if GPU_AVAILABLE:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Validate training horizon before starting\n",
    "horizon_valid = True\n",
    "if 'TRAINING_HORIZON' in dir() and TRAINING_HORIZON not in HORIZON_LIST:\n",
    "    print(f\"[ERROR] TRAINING_HORIZON={TRAINING_HORIZON} not in processed horizons {HORIZON_LIST}\")\n",
    "    print(f\"  Update TRAINING_HORIZON in Section 1 to one of: {HORIZON_LIST}\")\n",
    "    horizon_valid = False\n",
    "\n",
    "if not RUN_MODEL_TRAINING:\n",
    "    print(\"[Skipped] Model training disabled in configuration.\")\n",
    "elif not DATA_READY:\n",
    "    print(\"[Error] Data not ready. Run Section 3 first.\")\n",
    "elif not MODELS_TO_TRAIN:\n",
    "    print(\"[Error] No models selected. Enable models in Section 1.\")\n",
    "elif not horizon_valid:\n",
    "    print(\"[Error] Invalid training horizon. See error above.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 2: MODEL TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n  Models: {MODELS_TO_TRAIN}\")\n",
    "    print(f\"  Horizon: H{TRAINING_HORIZON}\")\n",
    "    \n",
    "    # Initialize results dict before training loop\n",
    "    TRAINING_RESULTS = {}\n",
    "    \n",
    "    try:\n",
    "        from src.models import ModelRegistry, Trainer, TrainerConfig\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        # Load data container\n",
    "        print(\"\\nLoading data...\")\n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        print(f\"  Train: {container.splits['train'].n_samples:,}\")\n",
    "        print(f\"  Val: {container.splits['val'].n_samples:,}\")\n",
    "        \n",
    "        # Train each model with per-model error handling\n",
    "        for i, model_name in enumerate(MODELS_TO_TRAIN, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\" [{i}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            clear_memory()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Configure model\n",
    "                if model_name in ['lstm', 'gru', 'tcn']:\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        sequence_length=SEQUENCE_LENGTH,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        max_epochs=MAX_EPOCHS,\n",
    "                        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n",
    "                    )\n",
    "                elif model_name == 'catboost':\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        model_config={\n",
    "                            \"iterations\": N_ESTIMATORS,\n",
    "                            \"early_stopping_rounds\": BOOSTING_EARLY_STOPPING,\n",
    "                            \"use_gpu\": False,\n",
    "                            \"task_type\": \"CPU\",\n",
    "                            \"verbose\": False,\n",
    "                        },\n",
    "                    )\n",
    "                else:\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        model_config={\n",
    "                            \"n_estimators\": N_ESTIMATORS,\n",
    "                            \"early_stopping_rounds\": BOOSTING_EARLY_STOPPING,\n",
    "                        } if model_name in ['xgboost', 'lightgbm'] else None,\n",
    "                    )\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(config)\n",
    "                results = trainer.run(container)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Store results\n",
    "                metrics = results.get('evaluation_metrics', {})\n",
    "                TRAINING_RESULTS[model_name] = {\n",
    "                    'metrics': metrics,\n",
    "                    'time': elapsed,\n",
    "                    'run_id': results.get('run_id', 'unknown'),\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n  Accuracy: {metrics.get('accuracy', 0):.2%}\")\n",
    "                print(f\"  Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n",
    "                print(f\"  Time: {elapsed:.1f}s\")\n",
    "                \n",
    "                del trainer, config\n",
    "                \n",
    "            except Exception as model_error:\n",
    "                # Per-model error handling - continue to next model\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"\\n  [ERROR] {model_name} training failed: {model_error}\")\n",
    "                TRAINING_RESULTS[model_name] = {\n",
    "                    'metrics': {},\n",
    "                    'time': elapsed,\n",
    "                    'run_id': 'failed',\n",
    "                    'error': str(model_error),\n",
    "                }\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            clear_memory()\n",
    "        \n",
    "        # Save results\n",
    "        results_file = EXPERIMENTS_DIR / 'training_results.json'\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(TRAINING_RESULTS, f, indent=2)\n",
    "        \n",
    "        # Summary\n",
    "        successful = [m for m, r in TRAINING_RESULTS.items() if r.get('run_id') != 'failed']\n",
    "        failed = [m for m, r in TRAINING_RESULTS.items() if r.get('run_id') == 'failed']\n",
    "        print(f\"\\n  Completed: {len(successful)}/{len(MODELS_TO_TRAIN)} models\")\n",
    "        if failed:\n",
    "            print(f\"  Failed: {failed}\")\n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "        \n",
    "        del container\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Training setup failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Compare Models { display-mode: \"form\" }\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure TRAINING_RESULTS is defined\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "if TRAINING_RESULTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" MODEL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Build comparison table\n",
    "    rows = []\n",
    "    for model, data in TRAINING_RESULTS.items():\n",
    "        metrics = data.get('metrics', {})\n",
    "        rows.append({\n",
    "            'Model': model,\n",
    "            'Accuracy': metrics.get('accuracy', 0),\n",
    "            'Macro F1': metrics.get('macro_f1', 0),\n",
    "            'Weighted F1': metrics.get('weighted_f1', 0),\n",
    "            'Time (s)': data.get('time', 0),\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(rows)\n",
    "    comparison_df = comparison_df.sort_values('Macro F1', ascending=False)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Best model\n",
    "    best_model = comparison_df.iloc[0]['Model']\n",
    "    best_f1 = comparison_df.iloc[0]['Macro F1']\n",
    "    print(f\"\\n  Best Model: {best_model} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # Visualization\n",
    "    if len(TRAINING_RESULTS) > 1:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        sorted_df = comparison_df.sort_values('Accuracy', ascending=True)\n",
    "        axes[0].barh(sorted_df['Model'], sorted_df['Accuracy'], color='steelblue')\n",
    "        axes[0].set_xlabel('Accuracy')\n",
    "        axes[0].set_title('Model Accuracy')\n",
    "        axes[0].set_xlim(0, 1)\n",
    "        \n",
    "        # Training time\n",
    "        sorted_df = comparison_df.sort_values('Time (s)', ascending=True)\n",
    "        axes[1].barh(sorted_df['Model'], sorted_df['Time (s)'], color='coral')\n",
    "        axes[1].set_xlabel('Training Time (seconds)')\n",
    "        axes[1].set_title('Training Time')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No training results available.\")\n",
    "    print(\"Run Section 4.1 to train models.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 4.3 Visualize Training Results { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "\n",
    "# Visualization toggles\n",
    "show_confusion_matrix = True  #@param {type: \"boolean\"}\n",
    "show_feature_importance = True  #@param {type: \"boolean\"}\n",
    "show_learning_curves = True  #@param {type: \"boolean\"}\n",
    "show_prediction_dist = True  #@param {type: \"boolean\"}\n",
    "show_per_class_metrics = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Check if we have training results\n",
    "if not TRAINING_RESULTS:\n",
    "    print(\"No training results available.\")\n",
    "    print(\"Run Section 4.1 to train models first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" TRAINING VISUALIZATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Filter successful models only\n",
    "    successful_models = {\n",
    "        name: data for name, data in TRAINING_RESULTS.items()\n",
    "        if data.get('run_id') != 'failed' and data.get('metrics')\n",
    "    }\n",
    "    \n",
    "    if not successful_models:\n",
    "        print(\"\\nNo successful models to visualize.\")\n",
    "        print(\"All models failed during training.\")\n",
    "    else:\n",
    "        print(f\"\\nVisualizing {len(successful_models)} models: {list(successful_models.keys())}\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 1. CONFUSION MATRICES\n",
    "        # ============================================================\n",
    "        if show_confusion_matrix:\n",
    "            print(\"\\n[1/5] Generating confusion matrices...\")\n",
    "            \n",
    "            n_models = len(successful_models)\n",
    "            n_cols = min(3, n_models)\n",
    "            n_rows = (n_models + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "            if n_models == 1:\n",
    "                axes = np.array([axes])\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for idx, (model_name, data) in enumerate(successful_models.items()):\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_true = np.array(pred_data.get('y_true', []))\n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_true) > 0 and len(y_pred) > 0:\n",
    "                        cm = confusion_matrix(y_true, y_pred, labels=[-1, 0, 1])\n",
    "                        disp = ConfusionMatrixDisplay(\n",
    "                            confusion_matrix=cm,\n",
    "                            display_labels=['Short', 'Neutral', 'Long']\n",
    "                        )\n",
    "                        disp.plot(ax=axes[idx], cmap='Blues', values_format='d')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}', fontweight='bold')\n",
    "                    else:\n",
    "                        axes[idx].text(0.5, 0.5, 'No predictions available',\n",
    "                                     ha='center', va='center')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}')\n",
    "                else:\n",
    "                    axes[idx].text(0.5, 0.5, 'Predictions not found',\n",
    "                                 ha='center', va='center')\n",
    "                    axes[idx].set_title(f'{model_name.upper()}')\n",
    "            \n",
    "            # Hide extra subplots\n",
    "            for idx in range(n_models, len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # ============================================================\n",
    "        # 2. FEATURE IMPORTANCE (Top 20)\n",
    "        # ============================================================\n",
    "        if show_feature_importance:\n",
    "            print(\"\\n[2/5] Generating feature importance plots...\")\n",
    "            \n",
    "            # Models that support feature importance\n",
    "            boosting_models = ['xgboost', 'lightgbm', 'catboost']\n",
    "            classical_models = ['random_forest']\n",
    "            \n",
    "            fi_models = {\n",
    "                name: data for name, data in successful_models.items()\n",
    "                if name in boosting_models + classical_models\n",
    "            }\n",
    "            \n",
    "            if fi_models:\n",
    "                n_models = len(fi_models)\n",
    "                n_cols = min(2, n_models)\n",
    "                n_rows = (n_models + n_cols - 1) // n_cols\n",
    "                \n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(10*n_cols, 6*n_rows))\n",
    "                if n_models == 1:\n",
    "                    axes = np.array([axes])\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for idx, (model_name, data) in enumerate(fi_models.items()):\n",
    "                    run_id = data.get('run_id', 'unknown')\n",
    "                    fi_file = EXPERIMENTS_DIR / run_id / 'feature_importance.json'\n",
    "                    \n",
    "                    if fi_file.exists():\n",
    "                        with open(fi_file, 'r') as f:\n",
    "                            fi_data = json.load(f)\n",
    "                        \n",
    "                        # Convert to DataFrame and sort\n",
    "                        fi_df = pd.DataFrame(list(fi_data.items()),\n",
    "                                            columns=['feature', 'importance'])\n",
    "                        fi_df = fi_df.sort_values('importance', ascending=False).head(20)\n",
    "                        \n",
    "                        # Plot\n",
    "                        axes[idx].barh(range(len(fi_df)), fi_df['importance'], color='steelblue')\n",
    "                        axes[idx].set_yticks(range(len(fi_df)))\n",
    "                        axes[idx].set_yticklabels(fi_df['feature'], fontsize=8)\n",
    "                        axes[idx].invert_yaxis()\n",
    "                        axes[idx].set_xlabel('Importance', fontweight='bold')\n",
    "                        axes[idx].set_title(f'{model_name.upper()} - Top 20 Features',\n",
    "                                          fontweight='bold')\n",
    "                        axes[idx].grid(axis='x', alpha=0.3)\n",
    "                    else:\n",
    "                        axes[idx].text(0.5, 0.5, 'Feature importance not available',\n",
    "                                     ha='center', va='center')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}')\n",
    "                \n",
    "                # Hide extra subplots\n",
    "                for idx in range(n_models, len(axes)):\n",
    "                    axes[idx].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No models with feature importance (boosting/classical models only)\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 3. LEARNING CURVES (Neural Models)\n",
    "        # ============================================================\n",
    "        if show_learning_curves:\n",
    "            print(\"\\n[3/5] Generating learning curves...\")\n",
    "            \n",
    "            # Neural models that have training history\n",
    "            neural_models = ['lstm', 'gru', 'tcn', 'transformer']\n",
    "            \n",
    "            lc_models = {\n",
    "                name: data for name, data in successful_models.items()\n",
    "                if name in neural_models\n",
    "            }\n",
    "            \n",
    "            if lc_models:\n",
    "                n_models = len(lc_models)\n",
    "                \n",
    "                fig, axes = plt.subplots(n_models, 2, figsize=(12, 4*n_models))\n",
    "                if n_models == 1:\n",
    "                    axes = axes.reshape(1, -1)\n",
    "                \n",
    "                for idx, (model_name, data) in enumerate(lc_models.items()):\n",
    "                    run_id = data.get('run_id', 'unknown')\n",
    "                    history_file = EXPERIMENTS_DIR / run_id / 'training_history.json'\n",
    "                    \n",
    "                    if history_file.exists():\n",
    "                        with open(history_file, 'r') as f:\n",
    "                            history = json.load(f)\n",
    "                        \n",
    "                        epochs = range(1, len(history.get('train_loss', [])) + 1)\n",
    "                        \n",
    "                        # Loss plot\n",
    "                        axes[idx, 0].plot(epochs, history.get('train_loss', []),\n",
    "                                        label='Train', linewidth=2, color='steelblue')\n",
    "                        axes[idx, 0].plot(epochs, history.get('val_loss', []),\n",
    "                                        label='Val', linewidth=2, color='coral')\n",
    "                        axes[idx, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "                        axes[idx, 0].set_ylabel('Loss', fontweight='bold')\n",
    "                        axes[idx, 0].set_title(f'{model_name.upper()} - Loss',\n",
    "                                              fontweight='bold')\n",
    "                        axes[idx, 0].legend()\n",
    "                        axes[idx, 0].grid(alpha=0.3)\n",
    "                        \n",
    "                        # Accuracy plot\n",
    "                        axes[idx, 1].plot(epochs, history.get('train_acc', []),\n",
    "                                        label='Train', linewidth=2, color='steelblue')\n",
    "                        axes[idx, 1].plot(epochs, history.get('val_acc', []),\n",
    "                                        label='Val', linewidth=2, color='coral')\n",
    "                        axes[idx, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "                        axes[idx, 1].set_ylabel('Accuracy', fontweight='bold')\n",
    "                        axes[idx, 1].set_title(f'{model_name.upper()} - Accuracy',\n",
    "                                              fontweight='bold')\n",
    "                        axes[idx, 1].legend()\n",
    "                        axes[idx, 1].grid(alpha=0.3)\n",
    "                    else:\n",
    "                        for col in [0, 1]:\n",
    "                            axes[idx, col].text(0.5, 0.5, 'History not available',\n",
    "                                              ha='center', va='center')\n",
    "                            axes[idx, col].set_title(f'{model_name.upper()}')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No neural models with training history\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 4. PREDICTION DISTRIBUTION\n",
    "        # ============================================================\n",
    "        if show_prediction_dist:\n",
    "            print(\"\\n[4/5] Generating prediction distribution...\")\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Prepare data for stacked bar chart\n",
    "            model_names = []\n",
    "            long_counts = []\n",
    "            neutral_counts = []\n",
    "            short_counts = []\n",
    "            \n",
    "            for model_name, data in successful_models.items():\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_pred) > 0:\n",
    "                        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "                        count_dict = dict(zip(unique, counts))\n",
    "                        total = len(y_pred)\n",
    "                        \n",
    "                        model_names.append(model_name)\n",
    "                        short_counts.append(count_dict.get(-1, 0) / total * 100)\n",
    "                        neutral_counts.append(count_dict.get(0, 0) / total * 100)\n",
    "                        long_counts.append(count_dict.get(1, 0) / total * 100)\n",
    "            \n",
    "            if model_names:\n",
    "                x = np.arange(len(model_names))\n",
    "                width = 0.6\n",
    "                \n",
    "                p1 = ax.bar(x, short_counts, width, label='Short (-1)', color='#d62728')\n",
    "                p2 = ax.bar(x, neutral_counts, width, bottom=short_counts,\n",
    "                           label='Neutral (0)', color='#7f7f7f')\n",
    "                p3 = ax.bar(x, long_counts, width,\n",
    "                           bottom=np.array(short_counts) + np.array(neutral_counts),\n",
    "                           label='Long (1)', color='#2ca02c')\n",
    "                \n",
    "                ax.set_ylabel('Percentage (%)', fontweight='bold')\n",
    "                ax.set_title('Prediction Distribution by Model', fontweight='bold', fontsize=14)\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                ax.legend(loc='upper right')\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                # Add percentage labels\n",
    "                for i, (s, n, l) in enumerate(zip(short_counts, neutral_counts, long_counts)):\n",
    "                    if s > 5:\n",
    "                        ax.text(i, s/2, f'{s:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                    if n > 5:\n",
    "                        ax.text(i, s + n/2, f'{n:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                    if l > 5:\n",
    "                        ax.text(i, s + n + l/2, f'{l:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No predictions available for distribution plot\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 5. PER-CLASS METRICS\n",
    "        # ============================================================\n",
    "        if show_per_class_metrics:\n",
    "            print(\"\\n[5/5] Generating per-class metrics...\")\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            model_names = []\n",
    "            short_precision = []\n",
    "            neutral_precision = []\n",
    "            long_precision = []\n",
    "            short_recall = []\n",
    "            neutral_recall = []\n",
    "            long_recall = []\n",
    "            short_f1 = []\n",
    "            neutral_f1 = []\n",
    "            long_f1 = []\n",
    "            \n",
    "            for model_name, data in successful_models.items():\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_true = np.array(pred_data.get('y_true', []))\n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_true) > 0 and len(y_pred) > 0:\n",
    "                        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                            y_true, y_pred, labels=[-1, 0, 1], average=None, zero_division=0\n",
    "                        )\n",
    "                        \n",
    "                        model_names.append(model_name)\n",
    "                        short_precision.append(precision[0])\n",
    "                        neutral_precision.append(precision[1])\n",
    "                        long_precision.append(precision[2])\n",
    "                        short_recall.append(recall[0])\n",
    "                        neutral_recall.append(recall[1])\n",
    "                        long_recall.append(recall[2])\n",
    "                        short_f1.append(f1[0])\n",
    "                        neutral_f1.append(f1[1])\n",
    "                        long_f1.append(f1[2])\n",
    "            \n",
    "            if model_names:\n",
    "                x = np.arange(len(model_names))\n",
    "                width = 0.25\n",
    "                \n",
    "                # Precision\n",
    "                axes[0].bar(x - width, short_precision, width, label='Short', color='#d62728')\n",
    "                axes[0].bar(x, neutral_precision, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[0].bar(x + width, long_precision, width, label='Long', color='#2ca02c')\n",
    "                axes[0].set_ylabel('Precision', fontweight='bold')\n",
    "                axes[0].set_title('Precision by Class', fontweight='bold')\n",
    "                axes[0].set_xticks(x)\n",
    "                axes[0].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(axis='y', alpha=0.3)\n",
    "                axes[0].set_ylim(0, 1)\n",
    "                \n",
    "                # Recall\n",
    "                axes[1].bar(x - width, short_recall, width, label='Short', color='#d62728')\n",
    "                axes[1].bar(x, neutral_recall, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[1].bar(x + width, long_recall, width, label='Long', color='#2ca02c')\n",
    "                axes[1].set_ylabel('Recall', fontweight='bold')\n",
    "                axes[1].set_title('Recall by Class', fontweight='bold')\n",
    "                axes[1].set_xticks(x)\n",
    "                axes[1].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[1].legend()\n",
    "                axes[1].grid(axis='y', alpha=0.3)\n",
    "                axes[1].set_ylim(0, 1)\n",
    "                \n",
    "                # F1 Score\n",
    "                axes[2].bar(x - width, short_f1, width, label='Short', color='#d62728')\n",
    "                axes[2].bar(x, neutral_f1, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[2].bar(x + width, long_f1, width, label='Long', color='#2ca02c')\n",
    "                axes[2].set_ylabel('F1 Score', fontweight='bold')\n",
    "                axes[2].set_title('F1 Score by Class', fontweight='bold')\n",
    "                axes[2].set_xticks(x)\n",
    "                axes[2].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[2].legend()\n",
    "                axes[2].grid(axis='y', alpha=0.3)\n",
    "                axes[2].set_ylim(0, 1)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No predictions available for per-class metrics\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" VISUALIZATIONS COMPLETE\")\n",
    "        print(\"=\" * 70)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.4 Transformer Attention Visualization { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Visualization settings\n",
    "visualize_attention = True  #@param {type: \"boolean\"}\n",
    "sample_index = 0  #@param {type: \"integer\"}\n",
    "#@markdown Sample index from validation set to visualize\n",
    "layer_to_visualize = -1  #@param {type: \"integer\"}\n",
    "#@markdown Layer index (-1 for last layer, 0 for first)\n",
    "head_to_visualize = 0  #@param {type: \"integer\"}\n",
    "#@markdown Head index to visualize in detail (0-7)\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "if IS_COLAB:\n",
    "    PROJECT_ROOT = Path('/content/Research')\n",
    "else:\n",
    "    PROJECT_ROOT = Path.home() / 'Research'\n",
    "\n",
    "EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments' / 'runs'\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data' / 'splits' / 'scaled'\n",
    "\n",
    "if not visualize_attention:\n",
    "    print(\"\u2713 Attention visualization disabled.\")\n",
    "    print(\"  Enable 'visualize_attention' to see transformer attention patterns.\")\n",
    "elif 'TRAINING_RESULTS' not in dir() or not TRAINING_RESULTS:\n",
    "    print(\"\u26a0 No training results found.\")\n",
    "    print(\"  Run Section 4.1 (Model Training) first.\")\n",
    "elif 'transformer' not in TRAINING_RESULTS:\n",
    "    print(\"\u26a0 Transformer model not trained.\")\n",
    "    print(\"  Enable TRAIN_TRANSFORMER in Section 1 and run Section 4.1.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"=\"*80)\n",
    "        print(\"TRANSFORMER ATTENTION VISUALIZATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Get transformer run ID\n",
    "        run_id = TRAINING_RESULTS['transformer']['run_id']\n",
    "        print(f\"\\n[Loading Model]\")\n",
    "        print(f\"  Run ID: {run_id}\")\n",
    "        \n",
    "        # Load container\n",
    "        from src.phase1.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        \n",
    "        print(f\"  Horizon: {TRAINING_HORIZON}\")\n",
    "        print(f\"  Validation samples: {len(container.val_X)}\")\n",
    "        \n",
    "        # Load trained transformer\n",
    "        model_path = EXPERIMENTS_DIR / run_id / 'checkpoints'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            print(f\"\\n\u26a0 Model checkpoint not found at {model_path}\")\n",
    "            print(\"  The model may not have been saved during training.\")\n",
    "        else:\n",
    "            # Import transformer model\n",
    "            from src.models import ModelRegistry\n",
    "            from src.models.config import TrainerConfig\n",
    "            import torch\n",
    "            \n",
    "            # Create model instance with same config\n",
    "            config = TrainerConfig(\n",
    "                model_type='transformer',\n",
    "                horizon=TRAINING_HORIZON,\n",
    "                seq_len=TRANSFORMER_SEQ_LEN,\n",
    "                d_model=TRANSFORMER_D_MODEL,\n",
    "                n_heads=TRANSFORMER_N_HEADS,\n",
    "                n_layers=TRANSFORMER_N_LAYERS,\n",
    "                dropout=0.1\n",
    "            )\n",
    "            \n",
    "            model = ModelRegistry.create('transformer', config=config.to_dict())\n",
    "            \n",
    "            # Load trained weights\n",
    "            checkpoint_file = list(model_path.glob('*.pt'))\n",
    "            if checkpoint_file:\n",
    "                model.load(model_path)\n",
    "                print(f\"  \u2713 Model loaded from {checkpoint_file[0].name}\")\n",
    "            else:\n",
    "                print(f\"\\n\u26a0 No .pt checkpoint files found in {model_path}\")\n",
    "                raise FileNotFoundError(\"Model checkpoint not found\")\n",
    "            \n",
    "            # Get validation sample\n",
    "            print(f\"\\n[Extracting Sample]\")\n",
    "            print(f\"  Sample index: {sample_index}\")\n",
    "            \n",
    "            if sample_index >= len(container.val_X):\n",
    "                print(f\"  \u26a0 Sample index {sample_index} out of range (max: {len(container.val_X)-1})\")\n",
    "                print(f\"  Using index 0 instead.\")\n",
    "                sample_index = 0\n",
    "            \n",
    "            # Prepare sample\n",
    "            X_val = container.val_X.iloc[[sample_index]]\n",
    "            y_val = container.val_y.iloc[sample_index]\n",
    "            \n",
    "            # Convert to torch tensor\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            # Reshape for transformer: (batch, seq_len, features)\n",
    "            seq_len = config.seq_len\n",
    "            n_features = X_val.shape[1] // seq_len\n",
    "            \n",
    "            X_tensor = torch.FloatTensor(X_val.values).reshape(1, seq_len, n_features).to(device)\n",
    "            \n",
    "            print(f\"  Input shape: {X_tensor.shape}\")\n",
    "            print(f\"  True label: {y_val}\")\n",
    "            \n",
    "            # Get model prediction and attention weights\n",
    "            model.model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Forward pass to get attention\n",
    "                if hasattr(model.model, 'get_attention_weights'):\n",
    "                    attention_weights, prediction = model.model.get_attention_weights(X_tensor, layer_idx=layer_to_visualize)\n",
    "                else:\n",
    "                    # Fallback: hook into transformer layers\n",
    "                    print(\"  \u26a0 Model doesn't have get_attention_weights method\")\n",
    "                    print(\"  Attention visualization requires model modifications\")\n",
    "                    raise NotImplementedError(\"Attention extraction not implemented\")\n",
    "            \n",
    "            # Convert to numpy\n",
    "            attention_weights = attention_weights.cpu().numpy()  # Shape: (batch, n_heads, seq_len, seq_len)\n",
    "            attention_weights = attention_weights[0]  # Remove batch dim: (n_heads, seq_len, seq_len)\n",
    "            prediction = prediction.cpu().numpy()[0]\n",
    "            \n",
    "            print(f\"\\n[Model Output]\")\n",
    "            print(f\"  Prediction: {prediction.argmax()}\")\n",
    "            print(f\"  Confidence: {prediction.max():.2%}\")\n",
    "            print(f\"  Attention shape: {attention_weights.shape}\")\n",
    "            \n",
    "            # Visualize attention heatmaps for all heads\n",
    "            print(f\"\\n[Visualizing Attention Patterns]\")\n",
    "            \n",
    "            n_heads = attention_weights.shape[0]\n",
    "            n_rows = 2\n",
    "            n_cols = (n_heads + n_rows - 1) // n_rows  # Ceiling division\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 8))\n",
    "            if n_heads == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif n_rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for head_idx in range(n_heads):\n",
    "                ax = axes[head_idx]\n",
    "                \n",
    "                # Plot heatmap\n",
    "                sns.heatmap(\n",
    "                    attention_weights[head_idx],\n",
    "                    cmap='viridis',\n",
    "                    ax=ax,\n",
    "                    cbar=True,\n",
    "                    square=True,\n",
    "                    vmin=0,\n",
    "                    vmax=attention_weights[head_idx].max(),\n",
    "                    cbar_kws={'label': 'Attention Weight'}\n",
    "                )\n",
    "                \n",
    "                ax.set_title(f'Head {head_idx+1}', fontsize=12, fontweight='bold')\n",
    "                ax.set_xlabel('Key Position (Source)')\n",
    "                ax.set_ylabel('Query Position (Target)')\n",
    "                \n",
    "                # Add grid for readability\n",
    "                ax.grid(False)\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for idx in range(n_heads, len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "            \n",
    "            layer_name = f\"Layer {layer_to_visualize}\" if layer_to_visualize >= 0 else \"Final Layer\"\n",
    "            plt.suptitle(\n",
    "                f'Transformer Attention Weights - {layer_name}\\nSample {sample_index} | True: {y_val} | Pred: {prediction.argmax()}',\n",
    "                fontsize=14,\n",
    "                fontweight='bold',\n",
    "                y=1.02\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Detailed analysis for selected head\n",
    "            print(f\"\\n[Attention Analysis - Head {head_to_visualize + 1}]\")\n",
    "            \n",
    "            if head_to_visualize >= n_heads:\n",
    "                print(f\"  \u26a0 Head {head_to_visualize} not available (max: {n_heads-1})\")\n",
    "                head_to_visualize = 0\n",
    "            \n",
    "            head_attention = attention_weights[head_to_visualize]\n",
    "            \n",
    "            # Average attention per position (what positions are attended to)\n",
    "            avg_attention_received = head_attention.mean(axis=0)  # Average over queries\n",
    "            avg_attention_given = head_attention.mean(axis=1)     # Average over keys\n",
    "            \n",
    "            print(f\"\\n  Most attended positions (received):\")\n",
    "            top_positions = avg_attention_received.argsort()[-5:][::-1]\n",
    "            for pos in top_positions:\n",
    "                print(f\"    Position {pos:3d}: {avg_attention_received[pos]:.4f}\")\n",
    "            \n",
    "            print(f\"\\n  Most attentive positions (given):\")\n",
    "            top_giving = avg_attention_given.argsort()[-5:][::-1]\n",
    "            for pos in top_giving:\n",
    "                print(f\"    Position {pos:3d}: {avg_attention_given[pos]:.4f}\")\n",
    "            \n",
    "            # Attention entropy (uniformity)\n",
    "            attention_entropy = [entropy(head_attention[i]) for i in range(len(head_attention))]\n",
    "            avg_entropy = np.mean(attention_entropy)\n",
    "            \n",
    "            print(f\"\\n  Attention entropy: {avg_entropy:.4f}\")\n",
    "            print(f\"    (Higher = more uniform, Lower = more focused)\")\n",
    "            \n",
    "            # Interpretability insights\n",
    "            print(f\"\\n[Interpretability Insights]\")\n",
    "            \n",
    "            # Check recency bias\n",
    "            recent_positions = seq_len // 10  # Last 10% of sequence\n",
    "            recent_attention = avg_attention_received[-recent_positions:].sum()\n",
    "            \n",
    "            if recent_attention > 0.3:  # >30% on recent bars\n",
    "                print(f\"  \u2192 Strong recency bias ({recent_attention:.1%} on last {recent_positions} positions)\")\n",
    "                print(f\"     Model relies heavily on most recent observations\")\n",
    "            \n",
    "            # Check long-range dependencies\n",
    "            early_positions = seq_len // 10  # First 10% of sequence\n",
    "            early_attention = avg_attention_received[:early_positions].sum()\n",
    "            \n",
    "            if early_attention > 0.15:  # >15% on early bars\n",
    "                print(f\"  \u2192 Long-range context ({early_attention:.1%} on first {early_positions} positions)\")\n",
    "                print(f\"     Model uses historical information beyond recent bars\")\n",
    "            \n",
    "            # Check attention focus vs spread\n",
    "            if avg_entropy < 2.0:\n",
    "                print(f\"  \u2192 Focused attention (entropy={avg_entropy:.2f})\")\n",
    "                print(f\"     Model concentrates on specific positions\")\n",
    "            elif avg_entropy > 4.0:\n",
    "                print(f\"  \u2192 Distributed attention (entropy={avg_entropy:.2f})\")\n",
    "                print(f\"     Model spreads attention broadly across sequence\")\n",
    "            \n",
    "            # Diagonal attention (position attends to itself)\n",
    "            self_attention = np.diag(head_attention).mean()\n",
    "            if self_attention > 0.2:\n",
    "                print(f\"  \u2192 Self-attention ({self_attention:.1%} average)\")\n",
    "                print(f\"     Positions attend to themselves (local context)\")\n",
    "            \n",
    "            print(f\"\\n\u2713 Attention visualization complete\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n\u26a0 Error: {e}\")\n",
    "        print(\"  The transformer model checkpoint was not found.\")\n",
    "        print(\"  Make sure the model completed training in Section 4.1.\")\n",
    "        \n",
    "    except NotImplementedError as e:\n",
    "        print(f\"\\n\u26a0 {e}\")\n",
    "        print(\"  The transformer model needs modifications to extract attention weights.\")\n",
    "        print(\"  Add a 'get_attention_weights' method to the transformer model class.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u26a0 Error during attention visualization:\")\n",
    "        print(f\"  {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 4.5 Test Set Performance { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Configuration options\n",
    "run_test_evaluation = True  #@param {type: \"boolean\"}\n",
    "show_sample_predictions = True  #@param {type: \"boolean\"}\n",
    "n_samples_to_show = 20  #@param {type: \"integer\"}\n",
    "show_generalization_gap = True  #@param {type: \"boolean\"}\n",
    "save_test_predictions = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Check if we can run test evaluation\n",
    "if not run_test_evaluation:\n",
    "    print(\"[Skipped] Test evaluation disabled. Enable checkbox above to run.\")\n",
    "elif not TRAINING_RESULTS:\n",
    "    print(\"[Error] No training results available.\")\n",
    "    print(\"Run Section 4.1 to train models first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" TEST SET PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Filter successful models only\n",
    "    successful_models = {\n",
    "        name: data for name, data in TRAINING_RESULTS.items()\n",
    "        if data.get('run_id') != 'failed' and data.get('metrics')\n",
    "    }\n",
    "    \n",
    "    if not successful_models:\n",
    "        print(\"\\n[Error] No successful models to evaluate.\")\n",
    "        print(\"All models failed during training.\")\n",
    "    else:\n",
    "        try:\n",
    "            # ============================================================\n",
    "            # LOAD TEST DATA\n",
    "            # ============================================================\n",
    "            print(f\"\\n[1/5] Loading test data...\")\n",
    "            \n",
    "            from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "            \n",
    "            container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "                path=SPLITS_DIR,\n",
    "                horizon=TRAINING_HORIZON\n",
    "            )\n",
    "            \n",
    "            # Get test data\n",
    "            test_split = container.splits.get('test')\n",
    "            if test_split is None:\n",
    "                print(\"  [ERROR] Test split not found in container!\")\n",
    "                raise ValueError(\"Test data not available\")\n",
    "            \n",
    "            X_test = test_split.features\n",
    "            y_test = test_split.labels\n",
    "            \n",
    "            print(f\"  Test samples: {len(X_test):,}\")\n",
    "            print(f\"  Features: {X_test.shape[1]}\")\n",
    "            \n",
    "            # ============================================================\n",
    "            # RUN PREDICTIONS ON TEST SET\n",
    "            # ============================================================\n",
    "            print(f\"\\n[2/5] Running predictions on test set...\")\n",
    "            \n",
    "            TEST_RESULTS = {}\n",
    "            \n",
    "            for model_name, train_data in successful_models.items():\n",
    "                print(f\"\\n  Evaluating: {model_name.upper()}\")\n",
    "                \n",
    "                try:\n",
    "                    run_id = train_data.get('run_id', 'unknown')\n",
    "                    model_dir = EXPERIMENTS_DIR / run_id\n",
    "                    \n",
    "                    # Load model from checkpoints\n",
    "                    checkpoint_dir = model_dir / 'checkpoints'\n",
    "                    \n",
    "                    # Try different model file formats\n",
    "                    model_loaded = False\n",
    "                    model = None\n",
    "                    \n",
    "                    # Method 1: Try pickle format\n",
    "                    pickle_path = checkpoint_dir / 'model.pkl'\n",
    "                    if pickle_path.exists():\n",
    "                        with open(pickle_path, 'rb') as f:\n",
    "                            model = pickle.load(f)\n",
    "                        model_loaded = True\n",
    "                        print(f\"    Loaded from: {pickle_path.name}\")\n",
    "                    \n",
    "                    # Method 2: Try joblib format\n",
    "                    if not model_loaded:\n",
    "                        joblib_path = checkpoint_dir / 'model.joblib'\n",
    "                        if joblib_path.exists():\n",
    "                            model = joblib.load(joblib_path)\n",
    "                            model_loaded = True\n",
    "                            print(f\"    Loaded from: {joblib_path.name}\")\n",
    "                    \n",
    "                    # Method 3: Try PyTorch format (for neural models)\n",
    "                    if not model_loaded and model_name in ['lstm', 'gru', 'tcn', 'transformer']:\n",
    "                        torch_path = checkpoint_dir / 'model.pt'\n",
    "                        if torch_path.exists():\n",
    "                            import torch\n",
    "                            from src.models import ModelRegistry\n",
    "                            \n",
    "                            # Recreate model architecture\n",
    "                            model = ModelRegistry.create(model_name, config={\n",
    "                                'input_size': X_test.shape[1],\n",
    "                                'hidden_size': 128,\n",
    "                                'num_layers': 2,\n",
    "                            })\n",
    "                            \n",
    "                            # Load weights\n",
    "                            state_dict = torch.load(torch_path, map_location='cpu')\n",
    "                            model.model.load_state_dict(state_dict)\n",
    "                            model.model.eval()\n",
    "                            model_loaded = True\n",
    "                            print(f\"    Loaded from: {torch_path.name}\")\n",
    "                    \n",
    "                    if not model_loaded:\n",
    "                        print(f\"    [WARNING] Model file not found in {checkpoint_dir}\")\n",
    "                        print(f\"    Skipping {model_name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    if hasattr(model, 'predict'):\n",
    "                        # Sklearn-style models\n",
    "                        if model_name in ['xgboost', 'lightgbm', 'catboost', 'random_forest', 'logistic', 'svm']:\n",
    "                            y_pred = model.predict(X_test)\n",
    "                        else:\n",
    "                            # Neural models - may need special handling\n",
    "                            pred_result = model.predict(X_test)\n",
    "                            if hasattr(pred_result, 'class_predictions'):\n",
    "                                y_pred = pred_result.class_predictions\n",
    "                            else:\n",
    "                                y_pred = pred_result\n",
    "                    else:\n",
    "                        print(f\"    [WARNING] Model has no predict method\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate test metrics\n",
    "                    test_acc = accuracy_score(y_test, y_pred)\n",
    "                    test_macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    test_weighted_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                    test_precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    test_recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    # Per-class metrics\n",
    "                    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    per_class_precision = precision_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    per_class_recall = recall_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    \n",
    "                    # Confusion matrix\n",
    "                    cm = confusion_matrix(y_test, y_pred, labels=[-1, 0, 1])\n",
    "                    \n",
    "                    # Store results\n",
    "                    val_metrics = train_data.get('metrics', {})\n",
    "                    \n",
    "                    TEST_RESULTS[model_name] = {\n",
    "                        'test_metrics': {\n",
    "                            'accuracy': test_acc,\n",
    "                            'macro_f1': test_macro_f1,\n",
    "                            'weighted_f1': test_weighted_f1,\n",
    "                            'precision': test_precision,\n",
    "                            'recall': test_recall,\n",
    "                            'per_class_f1': per_class_f1.tolist(),\n",
    "                            'per_class_precision': per_class_precision.tolist(),\n",
    "                            'per_class_recall': per_class_recall.tolist(),\n",
    "                            'confusion_matrix': cm.tolist(),\n",
    "                        },\n",
    "                        'val_metrics': val_metrics,\n",
    "                        'predictions': y_pred.tolist() if hasattr(y_pred, 'tolist') else list(y_pred),\n",
    "                        'run_id': run_id,\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"    Test Accuracy: {test_acc:.2%}\")\n",
    "                    print(f\"    Test Macro F1: {test_macro_f1:.4f}\")\n",
    "                    \n",
    "                    # Save predictions if requested\n",
    "                    if save_test_predictions:\n",
    "                        test_pred_file = model_dir / 'test_predictions.json'\n",
    "                        with open(test_pred_file, 'w') as f:\n",
    "                            json.dump({\n",
    "                                'y_true': y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test),\n",
    "                                'y_pred': y_pred.tolist() if hasattr(y_pred, 'tolist') else list(y_pred),\n",
    "                                'test_metrics': TEST_RESULTS[model_name]['test_metrics'],\n",
    "                            }, f, indent=2)\n",
    "                    \n",
    "                    del model\n",
    "                    \n",
    "                except Exception as model_error:\n",
    "                    print(f\"    [ERROR] Failed to evaluate {model_name}: {model_error}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            # ============================================================\n",
    "            # DISPLAY COMPARISON TABLE\n",
    "            # ============================================================\n",
    "            if TEST_RESULTS:\n",
    "                print(f\"\\n[3/5] Model Performance Comparison\")\n",
    "                print(\"=\" * 70)\n",
    "                \n",
    "                # Build comparison DataFrame\n",
    "                comparison_data = []\n",
    "                for model_name, results in TEST_RESULTS.items():\n",
    "                    val_metrics = results['val_metrics']\n",
    "                    test_metrics = results['test_metrics']\n",
    "                    \n",
    "                    val_acc = val_metrics.get('accuracy', 0)\n",
    "                    test_acc = test_metrics.get('accuracy', 0)\n",
    "                    val_f1 = val_metrics.get('macro_f1', 0)\n",
    "                    test_f1 = test_metrics.get('macro_f1', 0)\n",
    "                    \n",
    "                    # Calculate generalization gap\n",
    "                    acc_gap = (test_acc - val_acc) * 100\n",
    "                    f1_gap = (test_f1 - val_f1) * 100\n",
    "                    \n",
    "                    comparison_data.append({\n",
    "                        'Model': model_name,\n",
    "                        'Val Acc': f\"{val_acc:.2%}\",\n",
    "                        'Test Acc': f\"{test_acc:.2%}\",\n",
    "                        'Val F1': f\"{val_f1:.4f}\",\n",
    "                        'Test F1': f\"{test_f1:.4f}\",\n",
    "                        'Acc Gap (%)': f\"{acc_gap:+.2f}\",\n",
    "                        'F1 Gap (%)': f\"{f1_gap:+.2f}\",\n",
    "                    })\n",
    "                \n",
    "                comparison_df = pd.DataFrame(comparison_data)\n",
    "                \n",
    "                # Sort by test F1 score\n",
    "                comparison_df = comparison_df.sort_values(\n",
    "                    by='Test F1',\n",
    "                    ascending=False,\n",
    "                    key=lambda x: x.str.replace('%', '').astype(float) if x.dtype == 'object' else x\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\")\n",
    "                print(comparison_df.to_string(index=False))\n",
    "                \n",
    "                # Best performing model\n",
    "                best_model_name = comparison_df.iloc[0]['Model']\n",
    "                best_test_f1 = comparison_df.iloc[0]['Test F1']\n",
    "                print(f\"\\n  Best Model on Test Set: {best_model_name} (F1: {best_test_f1})\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # GENERALIZATION ANALYSIS\n",
    "                # ============================================================\n",
    "                if show_generalization_gap:\n",
    "                    print(f\"\\n[4/5] Generalization Analysis\")\n",
    "                    print(\"=\" * 70)\n",
    "                    \n",
    "                    for model_name, results in TEST_RESULTS.items():\n",
    "                        val_metrics = results['val_metrics']\n",
    "                        test_metrics = results['test_metrics']\n",
    "                        \n",
    "                        val_f1 = val_metrics.get('macro_f1', 0)\n",
    "                        test_f1 = test_metrics.get('macro_f1', 0)\n",
    "                        \n",
    "                        gap_pct = ((test_f1 - val_f1) / val_f1 * 100) if val_f1 > 0 else 0\n",
    "                        \n",
    "                        # Color-code based on gap\n",
    "                        if abs(gap_pct) < 2:\n",
    "                            status = \"\u2713 EXCELLENT\"\n",
    "                            color = \"green\"\n",
    "                        elif abs(gap_pct) < 5:\n",
    "                            status = \"~ GOOD\"\n",
    "                            color = \"yellow\"\n",
    "                        else:\n",
    "                            status = \"\u26a0 POOR\"\n",
    "                            color = \"red\"\n",
    "                        \n",
    "                        print(f\"\\n  {model_name.upper()}:\")\n",
    "                        print(f\"    Val F1:  {val_f1:.4f}\")\n",
    "                        print(f\"    Test F1: {test_f1:.4f}\")\n",
    "                        print(f\"    Gap:     {gap_pct:+.2f}% [{status}]\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # SAMPLE PREDICTIONS\n",
    "                # ============================================================\n",
    "                if show_sample_predictions and n_samples_to_show > 0:\n",
    "                    print(f\"\\n[5/5] Sample Predictions (first {n_samples_to_show})\")\n",
    "                    print(\"=\" * 70)\n",
    "                    \n",
    "                    # Show actual labels\n",
    "                    sample_actual = y_test[:n_samples_to_show]\n",
    "                    print(f\"\\n  Actual:     {list(sample_actual)}\")\n",
    "                    \n",
    "                    # Show predictions for each model\n",
    "                    for model_name, results in TEST_RESULTS.items():\n",
    "                        predictions = results['predictions']\n",
    "                        sample_pred = predictions[:n_samples_to_show]\n",
    "                        \n",
    "                        # Calculate accuracy for this sample\n",
    "                        matches = sum(1 for a, p in zip(sample_actual, sample_pred) if a == p)\n",
    "                        sample_acc = matches / len(sample_actual) * 100\n",
    "                        \n",
    "                        print(f\"  {model_name:12s}: {sample_pred} ({sample_acc:.1f}% match)\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 70)\n",
    "                print(\" TEST EVALUATION COMPLETE\")\n",
    "                print(\"=\" * 70)\n",
    "                \n",
    "                print(f\"\\n  Evaluated: {len(TEST_RESULTS)} models\")\n",
    "                print(f\"  Test samples: {len(X_test):,}\")\n",
    "                \n",
    "                if save_test_predictions:\n",
    "                    print(f\"  Predictions saved to: {EXPERIMENTS_DIR}/[run_id]/test_predictions.json\")\n",
    "            else:\n",
    "                print(\"\\n[WARNING] No test results generated.\")\n",
    "                print(\"All models failed to load or predict.\")\n",
    "            \n",
    "            # Clean up\n",
    "            del container, X_test, y_test\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] Test evaluation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. PHASE 3: CROSS-VALIDATION (Optional)\n",
    "\n",
    "Run purged K-fold cross-validation for robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Run Cross-Validation { display-mode: \"form\" }import osimport gcimport numpy as npimport pandas as pdfrom pathlib import Pathfrom tqdm.auto import tqdm# Ensure environment variables are definedif 'IS_COLAB' not in dir():    IS_COLAB = os.path.exists('/content')if 'PROJECT_ROOT' not in dir():    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()if 'SPLITS_DIR' not in dir():    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'if 'RUN_CROSS_VALIDATION' not in dir():    RUN_CROSS_VALIDATION = Falseif 'TRAINING_RESULTS' not in dir():    TRAINING_RESULTS = {}if 'GPU_AVAILABLE' not in dir():    import torch    GPU_AVAILABLE = torch.cuda.is_available()if 'CV_TUNE_HYPERPARAMS' not in dir():    CV_TUNE_HYPERPARAMS = Falseif 'CV_N_TRIALS' not in dir():    CV_N_TRIALS = 20# Define clear_memory if not availableif 'clear_memory' not in dir():    def clear_memory():        gc.collect()        if GPU_AVAILABLE:            import torch            torch.cuda.empty_cache()# Initialize global results dictionariesif 'CV_RESULTS' not in dir():    CV_RESULTS = {}if 'TUNING_RESULTS' not in dir():    TUNING_RESULTS = {}if not RUN_CROSS_VALIDATION:    print(\"[Skipped] Cross-validation disabled in configuration.\")    print(\"Set RUN_CROSS_VALIDATION = True in Section 1 to enable.\")else:    print(\"=\" * 70)    print(\" PHASE 3: CROSS-VALIDATION\")    print(\"=\" * 70)        try:        from src.cross_validation import PurgedKFold, PurgedKFoldConfig        from src.cross_validation.cv_runner import TimeSeriesOptunaTuner        from src.cross_validation.param_spaces import get_param_space        from src.phase1.stages.datasets.container import TimeSeriesDataContainer        from src.models import ModelRegistry        from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score                # Load data        container = TimeSeriesDataContainer.from_parquet_dir(            path=SPLITS_DIR,            horizon=TRAINING_HORIZON        )                X, y, _ = container.get_sklearn_arrays('train')        print(f\"\\nData: {X.shape[0]:,} samples, {X.shape[1]} features\")                # Configure CV        cv_config = PurgedKFoldConfig(            n_splits=CV_N_SPLITS,            purge_bars=PURGE_BARS,            embargo_bars=EMBARGO_BARS,        )        cv = PurgedKFold(cv_config)                print(f\"CV: {CV_N_SPLITS} folds, purge={PURGE_BARS}, embargo={EMBARGO_BARS}\")                # Get list of successfully trained models        successful_models = [m for m in TRAINING_RESULTS.keys() if TRAINING_RESULTS[m].get('status') == 'success']                if not successful_models:            print(\"\\n[Warning] No trained models found. Train models first in Section 4.\")        else:            print(f\"\\nRunning CV for {len(successful_models)} models: {', '.join(successful_models)}\")                        # Run CV for ALL trained models            cv_summary_data = []                        for model_name in tqdm(successful_models, desc=\"Cross-Validation\"):                print(f\"\\n{'='*50}\")                print(f\"Model: {model_name}\")                print(f\"{'='*50}\")                                # Hyperparameter tuning (if enabled)                tuned_params = {}                if CV_TUNE_HYPERPARAMS:                    param_space = get_param_space(model_name)                                        if param_space:                        print(f\"  Tuning hyperparameters ({CV_N_TRIALS} trials)...\")                        try:                            tuner = TimeSeriesOptunaTuner(                                model_name=model_name,                                cv=cv,                                n_trials=CV_N_TRIALS,                                direction=\"maximize\",                                metric=\"f1\"                            )                                                        tuning_result = tuner.tune(                                X=pd.DataFrame(X),                                y=pd.Series(y),                                sample_weights=None,                                param_space=param_space                            )                                                        if not tuning_result.get('skipped', False):                                tuned_params = tuning_result.get('best_params', {})                                best_value = tuning_result.get('best_value', 0.0)                                                                TUNING_RESULTS[model_name] = {                                    'best_params': tuned_params,                                    'best_value': best_value,                                    'n_trials': tuning_result.get('n_trials', 0)                                }                                                                print(f\"    Best F1: {best_value:.4f}\")                                print(f\"    Best params: {tuned_params}\")                            else:                                print(f\"    [Skipped] No tuning support or Optuna not installed\")                                                        except Exception as e:                            print(f\"    [Warning] Tuning failed: {e}\")                    else:                        print(f\"  [Skipped] No param space defined for {model_name}\")                                # Get model config (use tuned params if available)                try:                    default_config = ModelRegistry.get_model_info(model_name).get('default_config', {})                except:                    default_config = {                        'n_estimators': N_ESTIMATORS,                        'early_stopping_rounds': BOOSTING_EARLY_STOPPING,                    }                                model_config = {**default_config, **tuned_params}                                # Run cross-validation                print(f\"  Running {CV_N_SPLITS}-fold CV...\")                fold_scores = []                fold_details = []                                for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):                    X_train, X_val = X[train_idx], X[val_idx]                    y_train, y_val = y[train_idx], y[val_idx]                                        # Train model                    model = ModelRegistry.create(model_name, config=model_config)                    model.fit(X_train, y_train, X_val, y_val)                                        # Evaluate                    predictions = model.predict(X_val)                    y_pred = predictions.class_predictions                                        f1 = f1_score(y_val, y_pred, average='macro')                    acc = accuracy_score(y_val, y_pred)                    prec = precision_score(y_val, y_pred, average='macro', zero_division=0)                    rec = recall_score(y_val, y_pred, average='macro', zero_division=0)                                        fold_scores.append(f1)                    fold_details.append({                        'fold': fold_idx,                        'f1': f1,                        'accuracy': acc,                        'precision': prec,                        'recall': rec,                        'train_size': len(train_idx),                        'val_size': len(val_idx)                    })                                        del model                    clear_memory()                                # Calculate CV statistics                mean_f1 = np.mean(fold_scores)                std_f1 = np.std(fold_scores)                best_f1 = np.max(fold_scores)                                # Stability grading                if std_f1 < 0.01:                    stability = \"Excellent\"                elif std_f1 < 0.02:                    stability = \"Good\"                elif std_f1 < 0.04:                    stability = \"Fair\"                else:                    stability = \"Poor\"                                # Store results                CV_RESULTS[model_name] = {                    'mean_f1': mean_f1,                    'std_f1': std_f1,                    'best_f1': best_f1,                    'fold_scores': fold_scores,                    'fold_details': fold_details,                    'stability': stability,                    'tuned_params': tuned_params                }                                cv_summary_data.append({                    'Model': model_name,                    'CV Mean F1': mean_f1,                    'CV Std': std_f1,                    'Best F1': best_f1,                    'Stability': stability                })                                print(f\"  Mean F1: {mean_f1:.4f} (+/- {std_f1:.4f})\")                print(f\"  Best F1: {best_f1:.4f}\")                print(f\"  Stability: {stability}\")                        # Display CV summary table            print(f\"\\n{'='*70}\")            print(\" CROSS-VALIDATION SUMMARY\")            print(f\"{'='*70}\\n\")                        cv_summary_df = pd.DataFrame(cv_summary_data)            cv_summary_df = cv_summary_df.sort_values('CV Mean F1', ascending=False)                        # Format for display            print(cv_summary_df.to_string(index=False))                        print(f\"\\n{'='*70}\")            print(f\"Cross-validation complete for {len(successful_models)} models\")            print(f\"{'='*70}\")                del container, X, y        clear_memory()            except Exception as e:        print(f\"\\n[ERROR] Cross-validation failed: {e}\")        import traceback        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.2 Hyperparameter Tuning Results { display-mode: \"form\" }#@markdown Display hyperparameter tuning results and recommendations.show_retrain_recommendation = True  #@param {type: \"boolean\"}show_optimization_plots = False  #@param {type: \"boolean\"}import osimport pandas as pdimport numpy as npfrom pathlib import Path# Ensure environment variables are definedif 'IS_COLAB' not in dir():    IS_COLAB = os.path.exists('/content')if 'TUNING_RESULTS' not in dir():    TUNING_RESULTS = {}if 'TRAINING_RESULTS' not in dir():    TRAINING_RESULTS = {}if 'CV_TUNE_HYPERPARAMS' not in dir():    CV_TUNE_HYPERPARAMS = False# Check if tuning was runif not CV_TUNE_HYPERPARAMS:    print(\"=\" * 70)    print(\" HYPERPARAMETER TUNING RESULTS\")    print(\"=\" * 70)    print(\"\\n[Skipped] Hyperparameter tuning not enabled.\")    print(\"\\nTo enable tuning:\")    print(\"  1. Set CV_TUNE_HYPERPARAMS = True in Section 1\")    print(\"  2. Run Cross-Validation (Section 5.1)\")    print(f\"  3. Configure CV_N_TRIALS (currently: {globals().get('CV_N_TRIALS', 20)})\")elif 'TUNING_RESULTS' not in dir() or not TUNING_RESULTS:    print(\"=\" * 70)    print(\" HYPERPARAMETER TUNING RESULTS\")    print(\"=\" * 70)    print(\"\\n[No Data] No tuning results available.\")    print(\"\\nPossible reasons:\")    print(\"  - Cross-validation hasn't been run yet\")    print(\"  - No models have param spaces defined\")    print(\"  - Optuna is not installed\")    print(\"\\nRun Section 5.1 (Cross-Validation) first.\")else:    print(\"=\" * 70)    print(\" HYPERPARAMETER TUNING RESULTS\")    print(\"=\" * 70)        if not TUNING_RESULTS:        print(\"\\n[No Results] Tuning enabled but no models were tuned.\")        print(\"\\nModels with tuning support:\")        from src.cross_validation.param_spaces import PARAM_SPACES        supported_models = list(PARAM_SPACES.keys())        print(f\"  {', '.join(supported_models)}\")    else:        print(f\"\\nTuned {len(TUNING_RESULTS)} model(s)\")        print(f\"Trials per model: {globals().get('CV_N_TRIALS', 20)}\\n\")                # Display results for each model        for model_name, results in TUNING_RESULTS.items():            print(f\"\\n{'='*60}\")            print(f\" {model_name.upper()}\")            print(f\"{'='*60}\")                        best_params = results.get('best_params', {})            best_value = results.get('best_value', 0.0)            n_trials = results.get('n_trials', 0)                        print(f\"\\nOptimization Summary:\")            print(f\"  Trials completed: {n_trials}\")            print(f\"  Best F1 score:    {best_value:.4f}\")                        if best_params:                print(f\"\\n  Best Parameters:\")                                # Get default params for comparison                try:                    from src.models import ModelRegistry                    model_info = ModelRegistry.get_model_info(model_name)                    default_config = model_info.get('default_config', {})                except:                    default_config = {}                                # Create parameter comparison table                param_data = []                for param_name, tuned_value in best_params.items():                    default_value = default_config.get(param_name, None)                                        # Calculate change                    if default_value is not None:                        if isinstance(tuned_value, (int, float)) and isinstance(default_value, (int, float)):                            change_pct = ((tuned_value - default_value) / default_value * 100) if default_value != 0 else 0                            change_str = f\"{change_pct:+.1f}%\"                        else:                            change_str = \"changed\"                    else:                        change_str = \"new\"                                        param_data.append({                        'Parameter': param_name,                        'Default': str(default_value) if default_value is not None else 'N/A',                        'Tuned': str(tuned_value),                        'Change': change_str                    })                                if param_data:                    param_df = pd.DataFrame(param_data)                    print(\"\\n\" + param_df.to_string(index=False))                else:                    for param_name, value in best_params.items():                        print(f\"    {param_name}: {value}\")                        # Calculate improvement over default            if model_name in TRAINING_RESULTS:                default_f1 = TRAINING_RESULTS[model_name].get('metrics', {}).get('macro_f1', 0.0)                improvement = ((best_value - default_f1) / default_f1 * 100) if default_f1 > 0 else 0                                print(f\"\\n  Improvement Analysis:\")                print(f\"    Default F1:     {default_f1:.4f}\")                print(f\"    Tuned F1:       {best_value:.4f}\")                print(f\"    Improvement:    {improvement:+.2f}%\")                        print()                # Show retrain recommendations        if show_retrain_recommendation:            print(f\"\\n{'='*70}\")            print(\" RETRAIN RECOMMENDATIONS\")            print(f\"{'='*70}\\n\")                        recommendations = []            for model_name, results in TUNING_RESULTS.items():                best_value = results.get('best_value', 0.0)                                # Calculate improvement                if model_name in TRAINING_RESULTS:                    default_f1 = TRAINING_RESULTS[model_name].get('metrics', {}).get('macro_f1', 0.0)                    improvement = ((best_value - default_f1) / default_f1 * 100) if default_f1 > 0 else 0                                        recommendations.append({                        'Model': model_name,                        'Default F1': default_f1,                        'Tuned F1': best_value,                        'Improvement': improvement,                        'Action': 'RETRAIN' if improvement > 2.0 else 'Optional'                    })                        if recommendations:                rec_df = pd.DataFrame(recommendations)                rec_df = rec_df.sort_values('Improvement', ascending=False)                                # Format for display                rec_df['Default F1'] = rec_df['Default F1'].apply(lambda x: f\"{x:.4f}\")                rec_df['Tuned F1'] = rec_df['Tuned F1'].apply(lambda x: f\"{x:.4f}\")                rec_df['Improvement'] = rec_df['Improvement'].apply(lambda x: f\"{x:+.2f}%\")                                print(rec_df.to_string(index=False))                                # Highlight high-priority retrains                high_priority = [r for r in recommendations if r['Improvement'] > 2.0]                if high_priority:                    print(f\"\\n\u26a0 HIGH PRIORITY: {len(high_priority)} model(s) show >2% improvement:\")                    for rec in high_priority:                        print(f\"  - {rec['Model']}: {rec['Improvement']:+.2f}% improvement\")                    print(\"\\n  Recommendation: Retrain these models with tuned parameters\")                else:                    print(\"\\n\u2713 All models performing near-optimally with default parameters\")            else:                print(\"No comparison data available (models not trained with defaults)\")                # Optimization plots (optional)        if show_optimization_plots:            print(f\"\\n{'='*70}\")            print(\" OPTIMIZATION HISTORY\")            print(f\"{'='*70}\\n\")            print(\"[Info] Optimization plots require Optuna visualization.\")            print(\"      In Colab, install: !pip install optuna plotly\")            print(\"      Then re-run this cell to see optimization history.\")                        try:                import optuna                print(\"\\n\u2713 Optuna available - plots can be generated\")                print(\"  (Full plot integration coming in next update)\")            except ImportError:                print(\"\\n\u2717 Optuna not installed - plots unavailable\")                # Save tuning results        try:            if 'EXPERIMENTS_DIR' in dir():                tuning_results_path = EXPERIMENTS_DIR / 'tuning_results.json'                import json                with open(tuning_results_path, 'w') as f:                    json.dump(TUNING_RESULTS, f, indent=2, default=str)                print(f\"\\n[Saved] Tuning results: {tuning_results_path}\")        except Exception as e:            pass  # Silently skip if can't save                print(f\"\\n{'='*70}\")        print(f\"Hyperparameter tuning analysis complete\")        print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. PHASE 4: ENSEMBLE (Optional)\n",
    "\n",
    "Combine multiple models for improved predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Train Ensemble { display-mode: \"form\" }import osimport gcfrom pathlib import Pathimport numpy as npimport pandas as pd#@markdown ### Ensemble Training Optionsshow_base_model_validation = True  #@param {type: \"boolean\"}filter_by_cv_stability = False  #@param {type: \"boolean\"}show_ensemble_comparison = True  #@param {type: \"boolean\"}min_diversity_threshold = 0.1  #@param {type: \"number\"}# Ensure environment variables are definedif 'IS_COLAB' not in dir():    IS_COLAB = os.path.exists('/content')if 'PROJECT_ROOT' not in dir():    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()if 'SPLITS_DIR' not in dir():    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'if 'EXPERIMENTS_DIR' not in dir():    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'if 'TRAINING_RESULTS' not in dir():    TRAINING_RESULTS = {}if 'GPU_AVAILABLE' not in dir():    import torch    GPU_AVAILABLE = torch.cuda.is_available()# Define clear_memory if not availableif 'clear_memory' not in dir():    def clear_memory():        gc.collect()        if GPU_AVAILABLE:            import torch            torch.cuda.empty_cache()# Initialize ensemble results dictENSEMBLE_RESULTS = {}# Filter out failed models from ensemble base modelssuccessful_models = [    model for model, data in TRAINING_RESULTS.items()    if data.get('run_id') != 'failed' and data.get('metrics')]# Check if any ensemble is enabledany_ensemble_enabled = TRAIN_VOTING or TRAIN_STACKING or TRAIN_BLENDINGif not any_ensemble_enabled:    print(\"[Skipped] No ensemble training enabled.\")    print(\"Enable TRAIN_VOTING, TRAIN_STACKING, or TRAIN_BLENDING in Section 1.\")elif len(successful_models) < 2:    print(\"[Error] Need at least 2 successfully trained models for ensemble.\")    print(f\"Successfully trained: {successful_models}\")    if len(TRAINING_RESULTS) > len(successful_models):        failed = [m for m in TRAINING_RESULTS if m not in successful_models]        print(f\"Failed models (excluded): {failed}\")else:    print(\"=\" * 70)    print(\" PHASE 4: ENSEMBLE TRAINING\")    print(\"=\" * 70)        # Helper function to parse base models and validate    def parse_and_validate_base_models(base_models_str, ensemble_name):        \"\"\"Parse comma-separated base models and validate availability.\"\"\"        # Parse base models        base_model_names = [m.strip() for m in base_models_str.split(',') if m.strip()]                if show_base_model_validation:            print(f\"\\n[{ensemble_name.upper()}] Base Model Validation:\")            print(f\"  Requested: {base_model_names}\")                # Validate: only use successfully trained models        valid_base_models = [            m for m in base_model_names             if m in successful_models        ]                invalid_models = [m for m in base_model_names if m not in successful_models]        if invalid_models:            print(f\"  \u26a0 Skipped (not trained/failed): {invalid_models}\")                # Optionally filter by CV stability        if filter_by_cv_stability and 'CV_RESULTS' in dir() and CV_RESULTS:            stable_models = [                m for m in valid_base_models                if m in CV_RESULTS and CV_RESULTS[m].get('stability') in ['Excellent', 'Good']            ]            if len(stable_models) < len(valid_base_models):                unstable = [m for m in valid_base_models if m not in stable_models]                print(f\"  \u26a0 Filtered (low CV stability): {unstable}\")                valid_base_models = stable_models                if show_base_model_validation:            print(f\"  \u2713 Valid base models: {valid_base_models}\")                return valid_base_models        # Helper function to parse weights    def parse_weights(weights_str):        \"\"\"Parse comma-separated weights string into list of floats.\"\"\"        if not weights_str or not weights_str.strip():            return None        try:            weights = [float(w.strip()) for w in weights_str.split(',')]            return weights        except ValueError:            print(f\"  \u26a0 Invalid weights format: {weights_str}\")            return None        try:        from src.phase1.stages.datasets.container import TimeSeriesDataContainer        from src.models.trainer import Trainer        from src.models.config import TrainerConfig                # Load data container        print(f\"\\n[Data Loading]\")        print(f\"  Splits directory: {SPLITS_DIR}\")        print(f\"  Horizon: {TRAINING_HORIZON}\")                container = TimeSeriesDataContainer.load(SPLITS_DIR, TRAINING_HORIZON)        print(f\"  \u2713 Loaded: {container.X_train.shape[0]:,} train samples\")                # ===================================================================        # TRAIN VOTING ENSEMBLE        # ===================================================================        if TRAIN_VOTING:            print(\"\\n\" + \"=\" * 70)            print(\" VOTING ENSEMBLE\")            print(\"=\" * 70)                        valid_voting_models = parse_and_validate_base_models(                VOTING_BASE_MODELS,                 \"voting\"            )                        if len(valid_voting_models) < 2:                print(f\"  \u2717 Need at least 2 valid base models (got {len(valid_voting_models)})\")                print(\"  Skipping Voting ensemble.\")            else:                # Parse weights if provided                weights = parse_weights(VOTING_WEIGHTS) if VOTING_WEIGHTS else None                if weights and len(weights) != len(valid_voting_models):                    print(f\"  \u26a0 Weights count ({len(weights)}) != models count ({len(valid_voting_models)})\")                    print(\"  Using equal weights instead.\")                    weights = None                                # Create config                voting_config = TrainerConfig(                    model_name='voting',                    horizon=TRAINING_HORIZON,                    model_config={                        'base_model_names': valid_voting_models,                        'voting_type': 'soft',  # Soft voting (avg probabilities)                        'weights': weights                    },                    device='cuda' if GPU_AVAILABLE else 'cpu'                )                                print(f\"\\n  Base models: {valid_voting_models}\")                if weights:                    print(f\"  Weights: {weights}\")                else:                    print(f\"  Weights: Equal (1/{len(valid_voting_models)})\")                print(f\"  Voting type: soft\")                                # Train                trainer = Trainer(voting_config)                print(\"\\n  Training Voting ensemble...\")                results = trainer.run(container)                                # Store results                ENSEMBLE_RESULTS['voting'] = results                                # Display metrics                metrics = results['metrics']                print(f\"\\n  \u2713 Voting Ensemble Results:\")                print(f\"     Accuracy:  {metrics['accuracy']:.2%}\")                print(f\"     Macro F1:  {metrics['macro_f1']:.4f}\")                print(f\"     Precision: {metrics['macro_precision']:.4f}\")                print(f\"     Recall:    {metrics['macro_recall']:.4f}\")                                clear_memory()                # ===================================================================        # TRAIN STACKING ENSEMBLE        # ===================================================================        if TRAIN_STACKING:            print(\"\\n\" + \"=\" * 70)            print(\" STACKING ENSEMBLE\")            print(\"=\" * 70)                        valid_stacking_models = parse_and_validate_base_models(                STACKING_BASE_MODELS,                 \"stacking\"            )                        if len(valid_stacking_models) < 2:                print(f\"  \u2717 Need at least 2 valid base models (got {len(valid_stacking_models)})\")                print(\"  Skipping Stacking ensemble.\")            else:                # Create config                stacking_config = TrainerConfig(                    model_name='stacking',                    horizon=TRAINING_HORIZON,                    model_config={                        'base_model_names': valid_stacking_models,                        'meta_learner': STACKING_META_LEARNER,                        'n_folds': STACKING_N_FOLDS,                        'use_probas': True  # Use class probabilities                    },                    device='cuda' if GPU_AVAILABLE else 'cpu'                )                                print(f\"\\n  Base models: {valid_stacking_models}\")                print(f\"  Meta-learner: {STACKING_META_LEARNER}\")                print(f\"  CV folds: {STACKING_N_FOLDS}\")                                # Train                trainer = Trainer(stacking_config)                print(\"\\n  Training Stacking ensemble...\")                print(\"  (Generating out-of-fold predictions...)\")                results = trainer.run(container)                                # Store results                ENSEMBLE_RESULTS['stacking'] = results                                # Display metrics                metrics = results['metrics']                print(f\"\\n  \u2713 Stacking Ensemble Results:\")                print(f\"     Accuracy:  {metrics['accuracy']:.2%}\")                print(f\"     Macro F1:  {metrics['macro_f1']:.4f}\")                print(f\"     Precision: {metrics['macro_precision']:.4f}\")                print(f\"     Recall:    {metrics['macro_recall']:.4f}\")                                clear_memory()                # ===================================================================        # TRAIN BLENDING ENSEMBLE        # ===================================================================        if TRAIN_BLENDING:            print(\"\\n\" + \"=\" * 70)            print(\" BLENDING ENSEMBLE\")            print(\"=\" * 70)                        valid_blending_models = parse_and_validate_base_models(                BLENDING_BASE_MODELS,                 \"blending\"            )                        if len(valid_blending_models) < 2:                print(f\"  \u2717 Need at least 2 valid base models (got {len(valid_blending_models)})\")                print(\"  Skipping Blending ensemble.\")            else:                # Create config                blending_config = TrainerConfig(                    model_name='blending',                    horizon=TRAINING_HORIZON,                    model_config={                        'base_model_names': valid_blending_models,                        'meta_learner': BLENDING_META_LEARNER,                        'holdout_ratio': BLENDING_HOLDOUT_RATIO                    },                    device='cuda' if GPU_AVAILABLE else 'cpu'                )                                print(f\"\\n  Base models: {valid_blending_models}\")                print(f\"  Meta-learner: {BLENDING_META_LEARNER}\")                print(f\"  Holdout ratio: {BLENDING_HOLDOUT_RATIO:.0%}\")                                # Train                trainer = Trainer(blending_config)                print(\"\\n  Training Blending ensemble...\")                print(\"  (Using holdout set for meta-learner...)\")                results = trainer.run(container)                                # Store results                ENSEMBLE_RESULTS['blending'] = results                                # Display metrics                metrics = results['metrics']                print(f\"\\n  \u2713 Blending Ensemble Results:\")                print(f\"     Accuracy:  {metrics['accuracy']:.2%}\")                print(f\"     Macro F1:  {metrics['macro_f1']:.4f}\")                print(f\"     Precision: {metrics['macro_precision']:.4f}\")                print(f\"     Recall:    {metrics['macro_recall']:.4f}\")                                clear_memory()                # ===================================================================        # ENSEMBLE vs BASE MODEL COMPARISON        # ===================================================================        if show_ensemble_comparison and ENSEMBLE_RESULTS:            print(\"\\n\" + \"=\" * 70)            print(\" ENSEMBLE PERFORMANCE COMPARISON\")            print(\"=\" * 70)                        for ensemble_name, results in ENSEMBLE_RESULTS.items():                ensemble_f1 = results['metrics']['macro_f1']                ensemble_acc = results['metrics']['accuracy']                                # Get base model names from config                base_model_names = results['config']['base_model_names']                                # Find best base model                base_f1_scores = {                    m: TRAINING_RESULTS[m]['metrics']['macro_f1']                     for m in base_model_names                }                best_base_model = max(base_f1_scores, key=base_f1_scores.get)                best_base_f1 = base_f1_scores[best_base_model]                best_base_acc = TRAINING_RESULTS[best_base_model]['metrics']['accuracy']                                # Calculate improvements                f1_improvement = (ensemble_f1 - best_base_f1) / best_base_f1 * 100                acc_improvement = (ensemble_acc - best_base_acc) / best_base_acc * 100                                print(f\"\\n[{ensemble_name.upper()}]\")                print(f\"  Ensemble F1:    {ensemble_f1:.4f}\")                print(f\"  Best Base F1:   {best_base_f1:.4f} ({best_base_model})\")                print(f\"  F1 Improvement: {f1_improvement:+.2f}%\")                print(f\"  Acc Improvement: {acc_improvement:+.2f}%\")                                if f1_improvement > 0:                    print(f\"  \u2713 Ensemble outperforms best base model\")                elif f1_improvement > -1:                    print(f\"  \u2248 Ensemble comparable to best base model\")                else:                    print(f\"  \u26a0 Ensemble underperforms best base model\")                # Summary        print(\"\\n\" + \"=\" * 70)        print(f\" ENSEMBLE TRAINING COMPLETE\")        print(\"=\" * 70)        print(f\"\\n  Ensembles trained: {len(ENSEMBLE_RESULTS)}\")        if ENSEMBLE_RESULTS:            print(f\"  Available: {list(ENSEMBLE_RESULTS.keys())}\")            print(\"\\n  \u2713 Results stored in ENSEMBLE_RESULTS dict\")            print(\"  \u2713 Ready for ensemble analysis in next cell\")        else:            print(\"  No ensembles successfully trained.\")        except Exception as e:        print(f\"\\n\u2717 Ensemble training failed: {e}\")        import traceback        traceback.print_exc()        ENSEMBLE_RESULTS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.2 Ensemble Analysis & Diversity { display-mode: \"form\" }import osimport numpy as npimport pandas as pdfrom pathlib import Pathimport matplotlib.pyplot as pltimport seaborn as sns#@markdown ### Analysis Optionsshow_diversity_metrics = True  #@param {type: \"boolean\"}show_base_contributions = True  #@param {type: \"boolean\"}show_disagreement_analysis = False  #@param {type: \"boolean\"}plot_contribution_charts = True  #@param {type: \"boolean\"}# Ensure environment variablesif 'ENSEMBLE_RESULTS' not in dir():    ENSEMBLE_RESULTS = {}if 'TRAINING_RESULTS' not in dir():    TRAINING_RESULTS = {}if not ENSEMBLE_RESULTS or not ENSEMBLE_RESULTS:    print(\"[Skipped] No ensemble models trained.\")    print(\"Enable TRAIN_VOTING, TRAIN_STACKING, or TRAIN_BLENDING in Section 1\")    print(\"and run Cell 6.1 to train ensembles.\")else:    print(\"=\" * 70)    print(\" ENSEMBLE ANALYSIS & DIVERSITY\")    print(\"=\" * 70)        try:        from src.phase1.stages.datasets.container import TimeSeriesDataContainer                # Load data for predictions        if 'SPLITS_DIR' not in dir():            PROJECT_ROOT = Path('/content/research') if os.path.exists('/content') else Path('.')            SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'                container = TimeSeriesDataContainer.load(SPLITS_DIR, TRAINING_HORIZON)                # ===================================================================        # DIVERSITY ANALYSIS        # ===================================================================        if show_diversity_metrics:            print(\"\\n\" + \"-\" * 70)            print(\" DIVERSITY METRICS\")            print(\"-\" * 70)                        for ensemble_name, results in ENSEMBLE_RESULTS.items():                print(f\"\\n[{ensemble_name.upper()}]\")                                base_model_names = results['config']['base_model_names']                print(f\"  Base models: {base_model_names}\")                                # Get predictions from each base model on validation set                base_predictions = {}                for model_name in base_model_names:                    if model_name in TRAINING_RESULTS:                        model_result = TRAINING_RESULTS[model_name]                        if 'val_predictions' in model_result:                            base_predictions[model_name] = model_result['val_predictions']                                if len(base_predictions) >= 2:                    # Calculate pairwise agreement                    model_names = list(base_predictions.keys())                    n_models = len(model_names)                                        agreements = []                    for i in range(n_models):                        for j in range(i + 1, n_models):                            pred_i = base_predictions[model_names[i]]                            pred_j = base_predictions[model_names[j]]                            agreement = np.mean(pred_i == pred_j)                            agreements.append(agreement)                                        avg_agreement = np.mean(agreements)                    diversity_score = 1 - avg_agreement                                        print(f\"\\n  Pairwise Agreement: {avg_agreement:.3f}\")                    print(f\"  Diversity Score:    {diversity_score:.3f}\")                                        # Interpret diversity                    if diversity_score > 0.3:                        print(f\"  \u2713 Good diversity - models complement each other\")                    elif diversity_score > 0.15:                        print(f\"  \u2248 Moderate diversity - some complementarity\")                    else:                        print(f\"  \u26a0 Low diversity - models may be redundant\")                                        # Q-statistic (measure of diversity for pairs)                    print(f\"\\n  Pairwise Diversity Details:\")                    idx = 0                    for i in range(n_models):                        for j in range(i + 1, n_models):                            pred_i = base_predictions[model_names[i]]                            pred_j = base_predictions[model_names[j]]                            agreement = np.mean(pred_i == pred_j)                            print(f\"    {model_names[i]} <-> {model_names[j]}: {agreement:.3f} agreement\")                            idx += 1                else:                    print(f\"  \u26a0 Predictions not available for diversity analysis\")                # ===================================================================        # BASE MODEL CONTRIBUTIONS        # ===================================================================        if show_base_contributions:            print(\"\\n\" + \"-\" * 70)            print(\" BASE MODEL CONTRIBUTIONS\")            print(\"-\" * 70)                        contributions_data = []                        for ensemble_name, results in ENSEMBLE_RESULTS.items():                print(f\"\\n[{ensemble_name.upper()}]\")                                base_model_names = results['config']['base_model_names']                                if ensemble_name == 'voting':                    # For voting: show weights                    weights = results['config'].get('weights')                    if weights:                        print(f\"  Voting weights (explicit):\")                        for model, weight in zip(base_model_names, weights):                            print(f\"    {model}: {weight:.3f}\")                            contributions_data.append({                                'Ensemble': 'Voting',                                'Model': model,                                'Contribution': weight                            })                    else:                        # Equal weights                        weight = 1.0 / len(base_model_names)                        print(f\"  Voting weights (equal):\")                        for model in base_model_names:                            print(f\"    {model}: {weight:.3f}\")                            contributions_data.append({                                'Ensemble': 'Voting',                                'Model': model,                                'Contribution': weight                            })                                elif ensemble_name in ['stacking', 'blending']:                    # For stacking/blending: show meta-learner importance                    # This would require access to meta-learner internals                    # For now, show equal contributions as placeholder                    print(f\"  Meta-learner: {results['config'].get('meta_learner', 'unknown')}\")                    print(f\"  Base model contributions (estimated from performance):\")                                        # Estimate contribution by individual model F1 scores                    contributions = {}                    for model in base_model_names:                        if model in TRAINING_RESULTS:                            f1 = TRAINING_RESULTS[model]['metrics']['macro_f1']                            contributions[model] = f1                                        # Normalize to sum to 1                    total = sum(contributions.values())                    if total > 0:                        for model in sorted(contributions, key=contributions.get, reverse=True):                            contrib = contributions[model] / total                            print(f\"    {model}: {contrib:.3f} (based on F1)\")                            contributions_data.append({                                'Ensemble': ensemble_name.capitalize(),                                'Model': model,                                'Contribution': contrib                            })                        # Plot contributions            if plot_contribution_charts and contributions_data:                df_contrib = pd.DataFrame(contributions_data)                                fig, axes = plt.subplots(1, len(ENSEMBLE_RESULTS), figsize=(5 * len(ENSEMBLE_RESULTS), 4))                if len(ENSEMBLE_RESULTS) == 1:                    axes = [axes]                                for idx, (ensemble_name, results) in enumerate(ENSEMBLE_RESULTS.items()):                    ensemble_data = df_contrib[df_contrib['Ensemble'] == ensemble_name.capitalize()]                                        axes[idx].barh(ensemble_data['Model'], ensemble_data['Contribution'])                    axes[idx].set_xlabel('Contribution')                    axes[idx].set_title(f'{ensemble_name.capitalize()} Ensemble')                    axes[idx].set_xlim(0, max(ensemble_data['Contribution']) * 1.1)                                plt.tight_layout()                plt.show()                # ===================================================================        # ENSEMBLE COMPARISON TABLE        # ===================================================================        print(\"\\n\" + \"-\" * 70)        print(\" ENSEMBLE COMPARISON TABLE\")        print(\"-\" * 70)                comparison_data = []        for ensemble_name, results in ENSEMBLE_RESULTS.items():            metrics = results['metrics']            base_models = results['config']['base_model_names']                        # Calculate diversity if predictions available            diversity = 0.0            base_predictions = {}            for model_name in base_models:                if model_name in TRAINING_RESULTS:                    if 'val_predictions' in TRAINING_RESULTS[model_name]:                        base_predictions[model_name] = TRAINING_RESULTS[model_name]['val_predictions']                        if len(base_predictions) >= 2:                model_names = list(base_predictions.keys())                n_models = len(model_names)                agreements = []                for i in range(n_models):                    for j in range(i + 1, n_models):                        pred_i = base_predictions[model_names[i]]                        pred_j = base_predictions[model_names[j]]                        agreement = np.mean(pred_i == pred_j)                        agreements.append(agreement)                diversity = 1 - np.mean(agreements)                        # Best base model improvement            base_f1_scores = {                m: TRAINING_RESULTS[m]['metrics']['macro_f1']                 for m in base_models if m in TRAINING_RESULTS            }            if base_f1_scores:                best_base_model = max(base_f1_scores, key=base_f1_scores.get)                best_base_f1 = base_f1_scores[best_base_model]                improvement = (metrics['macro_f1'] - best_base_f1) / best_base_f1 * 100            else:                best_base_model = 'N/A'                improvement = 0.0                        comparison_data.append({                'Ensemble': ensemble_name.capitalize(),                'Accuracy': f\"{metrics['accuracy']:.2%}\",                'F1 Score': f\"{metrics['macro_f1']:.4f}\",                'Base Models': len(base_models),                'Diversity': f\"{diversity:.3f}\",                'Best Base': best_base_model,                'Improvement': f\"{improvement:+.2f}%\"            })                df_comparison = pd.DataFrame(comparison_data)        print(\"\\n\", df_comparison.to_string(index=False))                # ===================================================================        # RECOMMENDATION        # ===================================================================        print(\"\\n\" + \"-\" * 70)        print(\" RECOMMENDATION\")        print(\"-\" * 70)                # Find best ensemble by F1        best_ensemble_name = max(            ENSEMBLE_RESULTS,             key=lambda x: ENSEMBLE_RESULTS[x]['metrics']['macro_f1']        )        best_ensemble = ENSEMBLE_RESULTS[best_ensemble_name]                print(f\"\\n  Best Ensemble: {best_ensemble_name.upper()}\")        print(f\"  Metrics:\")        print(f\"    - Accuracy: {best_ensemble['metrics']['accuracy']:.2%}\")        print(f\"    - F1 Score: {best_ensemble['metrics']['macro_f1']:.4f}\")        print(f\"    - Precision: {best_ensemble['metrics']['macro_precision']:.4f}\")        print(f\"    - Recall: {best_ensemble['metrics']['macro_recall']:.4f}\")                # Reason        base_models = best_ensemble['config']['base_model_names']        print(f\"\\n  Reason: Highest F1 score among {len(ENSEMBLE_RESULTS)} ensembles\")        print(f\"  Base models: {base_models}\")                # Check diversity        if show_diversity_metrics:            base_predictions = {}            for model_name in base_models:                if model_name in TRAINING_RESULTS and 'val_predictions' in TRAINING_RESULTS[model_name]:                    base_predictions[model_name] = TRAINING_RESULTS[model_name]['val_predictions']                        if len(base_predictions) >= 2:                model_names = list(base_predictions.keys())                n_models = len(model_names)                agreements = []                for i in range(n_models):                    for j in range(i + 1, n_models):                        pred_i = base_predictions[model_names[i]]                        pred_j = base_predictions[model_names[j]]                        agreement = np.mean(pred_i == pred_j)                        agreements.append(agreement)                diversity_score = 1 - np.mean(agreements)                                if diversity_score > 0.3:                    print(f\"  \u2713 Good diversity ({diversity_score:.3f}) - models complement each other\")                elif diversity_score > 0.15:                    print(f\"  \u2248 Moderate diversity ({diversity_score:.3f})\")                else:                    print(f\"  \u26a0 Low diversity ({diversity_score:.3f}) - consider different base models\")                # ===================================================================        # DISAGREEMENT ANALYSIS        # ===================================================================        if show_disagreement_analysis:            print(\"\\n\" + \"-\" * 70)            print(\" DISAGREEMENT ANALYSIS\")            print(\"-\" * 70)                        for ensemble_name, results in ENSEMBLE_RESULTS.items():                print(f\"\\n[{ensemble_name.upper()}]\")                                base_model_names = results['config']['base_model_names']                                # Get predictions                base_predictions = {}                for model_name in base_model_names:                    if model_name in TRAINING_RESULTS and 'val_predictions' in TRAINING_RESULTS[model_name]:                        base_predictions[model_name] = TRAINING_RESULTS[model_name]['val_predictions']                                if len(base_predictions) >= 2:                    # Find samples where models disagree                    pred_arrays = [base_predictions[m] for m in base_predictions.keys()]                    pred_matrix = np.array(pred_arrays)                                        # Check disagreement (not all predictions are the same)                    disagreements = np.apply_along_axis(lambda x: len(np.unique(x)) > 1, axis=0, arr=pred_matrix)                    disagreement_rate = np.mean(disagreements)                                        print(f\"  Disagreement rate: {disagreement_rate:.2%}\")                    print(f\"  Samples with disagreement: {np.sum(disagreements):,} / {len(disagreements):,}\")                                        # Show a few examples                    disagreement_indices = np.where(disagreements)[0]                    if len(disagreement_indices) > 0:                        print(f\"\\n  Example disagreements (first 5):\")                        for idx in disagreement_indices[:5]:                            predictions = {m: base_predictions[m][idx] for m in base_predictions.keys()}                            print(f\"    Sample {idx}: {predictions}\")                else:                    print(f\"  \u26a0 Predictions not available\")                print(\"\\n\" + \"=\" * 70)        print(\" ENSEMBLE ANALYSIS COMPLETE\")        print(\"=\" * 70)        except Exception as e:        print(f\"\\n\u2717 Ensemble analysis failed: {e}\")        import traceback        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. RESULTS & EXPORT\n",
    "\n",
    "Summary of all results and export options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.1 Final Summary { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" PIPELINE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n Configuration:\")\n",
    "print(f\"   Symbol: {SYMBOL}\")\n",
    "\n",
    "# Show auto-detected date range (with safety checks)\n",
    "if 'DATA_START' in dir() and DATA_START is not None:\n",
    "    print(f\"   Date Range: {DATA_START.strftime('%Y-%m-%d')} to {DATA_END.strftime('%Y-%m-%d')}\")\n",
    "    if 'DATA_START_YEAR' in dir() and 'DATA_END_YEAR' in dir():\n",
    "        print(f\"   Years: {DATA_START_YEAR} - {DATA_END_YEAR}\")\n",
    "else:\n",
    "    print(f\"   Date Range: Not detected (run Section 3.1)\")\n",
    "\n",
    "print(f\"   Training Horizon: H{TRAINING_HORIZON}\")\n",
    "\n",
    "if 'TRAIN_LEN' in dir():\n",
    "    print(f\"\\n Data:\")\n",
    "    print(f\"   Train: {TRAIN_LEN:,} samples\")\n",
    "    if 'VAL_LEN' in dir():\n",
    "        print(f\"   Val: {VAL_LEN:,} samples\")\n",
    "    if 'TEST_LEN' in dir():\n",
    "        print(f\"   Test: {TEST_LEN:,} samples\")\n",
    "\n",
    "if 'TRAINING_RESULTS' in dir() and TRAINING_RESULTS:\n",
    "    print(f\"\\n Model Results:\")\n",
    "    for model, data in sorted(TRAINING_RESULTS.items(), \n",
    "                              key=lambda x: x[1]['metrics'].get('macro_f1', 0), \n",
    "                              reverse=True):\n",
    "        metrics = data['metrics']\n",
    "        print(f\"   {model}: Acc={metrics.get('accuracy', 0):.2%}, F1={metrics.get('macro_f1', 0):.4f}\")\n",
    "    \n",
    "    best = max(TRAINING_RESULTS, key=lambda x: TRAINING_RESULTS[x]['metrics'].get('macro_f1', 0))\n",
    "    print(f\"\\n Best Model: {best}\")\n",
    "\n",
    "print(f\"\\n Saved Artifacts:\")\n",
    "print(f\"   Data: {SPLITS_DIR}\")\n",
    "print(f\"   Models: {EXPERIMENTS_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" PIPELINE COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Export Model Package { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'RESULTS_DIR' not in dir():\n",
    "    RESULTS_DIR = PROJECT_ROOT / 'experiments'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'TEST_RESULTS' not in dir():\n",
    "    TEST_RESULTS = {}\n",
    "if 'CV_RESULTS' not in dir():\n",
    "    CV_RESULTS = {}\n",
    "if 'ENSEMBLE_RESULTS' not in dir():\n",
    "    ENSEMBLE_RESULTS = {}\n",
    "\n",
    "#@markdown ### Export Configuration\n",
    "\n",
    "export_model = False  #@param {type: \"boolean\"}\n",
    "#@markdown Enable to export model package\n",
    "\n",
    "export_selection = \"Best Model\"  #@param [\"Best Model\", \"All Models\", \"Ensembles Only\", \"Top 3 Models\", \"Custom Selection\"]\n",
    "#@markdown Select which models to export\n",
    "\n",
    "custom_models_to_export = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated model names (only used if Custom Selection)\n",
    "\n",
    "export_format = \"Standard Package\"  #@param [\"Standard Package\", \"Production Bundle\", \"Research Archive\", \"Minimal (Model Only)\"]\n",
    "#@markdown Export package type\n",
    "\n",
    "#@markdown ### Export Options\n",
    "\n",
    "include_onnx = False  #@param {type: \"boolean\"}\n",
    "#@markdown Export to ONNX format for production (XGBoost, LightGBM, CatBoost only)\n",
    "\n",
    "include_predictions = True  #@param {type: \"boolean\"}\n",
    "#@markdown Include validation and test predictions\n",
    "\n",
    "include_visualizations = True  #@param {type: \"boolean\"}\n",
    "#@markdown Include generated plots and charts\n",
    "\n",
    "include_model_card = True  #@param {type: \"boolean\"}\n",
    "#@markdown Generate model cards with performance details\n",
    "\n",
    "create_zip_archive = True  #@param {type: \"boolean\"}\n",
    "#@markdown Create ZIP archive of export package\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_models_to_export():\n",
    "    \"\"\"Determine which models to export based on selection.\"\"\"\n",
    "    all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "\n",
    "    if not all_results:\n",
    "        return []\n",
    "\n",
    "    if export_selection == \"Best Model\":\n",
    "        best_model = max(all_results, key=lambda x: all_results[x]['metrics'].get('macro_f1', 0))\n",
    "        return [best_model]\n",
    "\n",
    "    elif export_selection == \"All Models\":\n",
    "        return list(all_results.keys())\n",
    "\n",
    "    elif export_selection == \"Ensembles Only\":\n",
    "        return [m for m in all_results.keys() if 'ensemble' in m or 'voting' in m or 'stacking' in m or 'blending' in m]\n",
    "\n",
    "    elif export_selection == \"Top 3 Models\":\n",
    "        sorted_models = sorted(all_results.items(), key=lambda x: x[1]['metrics'].get('macro_f1', 0), reverse=True)\n",
    "        return [m[0] for m in sorted_models[:3]]\n",
    "\n",
    "    elif export_selection == \"Custom Selection\":\n",
    "        if not custom_models_to_export:\n",
    "            print(\"\u26a0 Custom selection requires model names in 'custom_models_to_export'\")\n",
    "            return []\n",
    "        models = [m.strip() for m in custom_models_to_export.split(',')]\n",
    "        valid_models = [m for m in models if m in all_results]\n",
    "        if len(valid_models) < len(models):\n",
    "            invalid = set(models) - set(valid_models)\n",
    "            print(f\"\u26a0 Invalid models: {invalid}\")\n",
    "        return valid_models\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_model_card(model_name, model_info, test_info=None, cv_info=None):\n",
    "    \"\"\"Generate model card in Markdown format.\"\"\"\n",
    "    metrics = model_info.get('metrics', {})\n",
    "    config = model_info.get('config', {})\n",
    "\n",
    "    card = f\"\"\"# Model Card: {model_name.upper()}\n",
    "\n",
    "## Model Information\n",
    "- **Type:** {model_info.get('model_type', 'Unknown')}\n",
    "- **Symbol:** {SYMBOL if 'SYMBOL' in dir() else 'N/A'}\n",
    "- **Horizon:** {TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A'} bars\n",
    "- **Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Run ID:** {model_info.get('run_id', 'Unknown')}\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Validation Set\n",
    "- **Accuracy:** {metrics.get('accuracy', 0):.4f}\n",
    "- **Macro F1:** {metrics.get('macro_f1', 0):.4f}\n",
    "- **Precision:** {metrics.get('precision', 0):.4f}\n",
    "- **Recall:** {metrics.get('recall', 0):.4f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add test results if available\n",
    "    if test_info:\n",
    "        test_metrics = test_info.get('metrics', {})\n",
    "        val_f1 = metrics.get('macro_f1', 0)\n",
    "        test_f1 = test_metrics.get('macro_f1', 0)\n",
    "        gap = ((test_f1 - val_f1) / val_f1 * 100) if val_f1 > 0 else 0\n",
    "\n",
    "        card += f\"\"\"### Test Set\n",
    "- **Accuracy:** {test_metrics.get('accuracy', 0):.4f}\n",
    "- **Macro F1:** {test_metrics.get('macro_f1', 0):.4f}\n",
    "- **Precision:** {test_metrics.get('precision', 0):.4f}\n",
    "- **Recall:** {test_metrics.get('recall', 0):.4f}\n",
    "- **Generalization Gap:** {gap:+.2f}%\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add CV results if available\n",
    "    if cv_info:\n",
    "        cv_metrics = cv_info.get('cv_metrics', {})\n",
    "        card += f\"\"\"### Cross-Validation\n",
    "- **Mean F1:** {cv_metrics.get('mean_f1', 0):.4f} \u00b1 {cv_metrics.get('std_f1', 0):.4f}\n",
    "- **Mean Accuracy:** {cv_metrics.get('mean_accuracy', 0):.4f} \u00b1 {cv_metrics.get('std_accuracy', 0):.4f}\n",
    "- **Folds:** {cv_info.get('n_splits', 'N/A')}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add configuration\n",
    "    if config:\n",
    "        card += f\"\"\"## Configuration\n",
    "\n",
    "```json\n",
    "{json.dumps(config, indent=2)}\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add feature information\n",
    "    if 'feature_importance' in model_info:\n",
    "        importance = model_info['feature_importance']\n",
    "        top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        card += f\"\"\"## Top 10 Features\n",
    "\n",
    "\"\"\"\n",
    "        for i, (feature, score) in enumerate(top_features, 1):\n",
    "            card += f\"{i}. **{feature}**: {score:.4f}\\n\"\n",
    "        card += \"\\n\"\n",
    "\n",
    "    # Add training details\n",
    "    train_time = model_info.get('training_time_sec', 0)\n",
    "    card += f\"\"\"## Training Details\n",
    "- **Training Time:** {train_time:.2f}s\n",
    "- **Model Size:** {model_info.get('model_size_mb', 'N/A')} MB\n",
    "- **Framework:** {model_info.get('framework', 'Unknown')}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    return card\n",
    "\n",
    "\n",
    "def export_to_onnx(model, model_name, model_path, feature_names):\n",
    "    \"\"\"Export model to ONNX format (boosting models only).\"\"\"\n",
    "    try:\n",
    "        # Check if model type supports ONNX\n",
    "        onnx_compatible = ['xgboost', 'lightgbm', 'catboost']\n",
    "        if not any(m in model_name.lower() for m in onnx_compatible):\n",
    "            return False, \"Model type not compatible with ONNX\"\n",
    "\n",
    "        # Try to import ONNX libraries\n",
    "        try:\n",
    "            from skl2onnx import convert_sklearn\n",
    "            from skl2onnx.common.data_types import FloatTensorType\n",
    "            import onnx\n",
    "        except ImportError:\n",
    "            return False, \"ONNX libraries not installed (skl2onnx, onnx)\"\n",
    "\n",
    "        # Load the model\n",
    "        loaded_model = joblib.load(model_path)\n",
    "\n",
    "        # Determine number of features\n",
    "        n_features = len(feature_names) if feature_names else 150\n",
    "\n",
    "        # Define input type\n",
    "        initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "\n",
    "        # Convert to ONNX\n",
    "        onnx_model = convert_sklearn(loaded_model, initial_types=initial_type)\n",
    "\n",
    "        # Save ONNX model\n",
    "        onnx_path = model_path.parent / 'model.onnx'\n",
    "        with open(onnx_path, 'wb') as f:\n",
    "            f.write(onnx_model.SerializeToString())\n",
    "\n",
    "        # Get file size\n",
    "        size_mb = onnx_path.stat().st_size / 1e6\n",
    "\n",
    "        return True, f\"ONNX export successful ({size_mb:.2f} MB)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"ONNX export failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def create_manifest(export_dir, models_exported, export_info):\n",
    "    \"\"\"Create manifest.json with export metadata.\"\"\"\n",
    "    all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "\n",
    "    # Find best model\n",
    "    best_model = max(all_results, key=lambda x: all_results[x]['metrics'].get('macro_f1', 0))\n",
    "    best_f1 = all_results[best_model]['metrics'].get('macro_f1', 0)\n",
    "\n",
    "    # Collect model formats\n",
    "    formats = {}\n",
    "    for model_name in models_exported:\n",
    "        model_formats = ['pkl']\n",
    "        onnx_path = export_dir / 'models' / model_name / 'model.onnx'\n",
    "        if onnx_path.exists():\n",
    "            model_formats.append('onnx')\n",
    "        formats[model_name] = model_formats\n",
    "\n",
    "    manifest = {\n",
    "        'export_timestamp': datetime.now().isoformat(),\n",
    "        'symbol': SYMBOL if 'SYMBOL' in dir() else 'N/A',\n",
    "        'horizon': TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A',\n",
    "        'models_exported': models_exported,\n",
    "        'best_model': best_model,\n",
    "        'best_test_f1': best_f1,\n",
    "        'export_format': export_format,\n",
    "        'formats': formats,\n",
    "        'data_stats': export_info.get('data_stats', {}),\n",
    "        'export_options': {\n",
    "            'include_onnx': include_onnx,\n",
    "            'include_predictions': include_predictions,\n",
    "            'include_visualizations': include_visualizations,\n",
    "            'include_model_card': include_model_card\n",
    "        }\n",
    "    }\n",
    "\n",
    "    manifest_path = export_dir / 'manifest.json'\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    return manifest_path\n",
    "\n",
    "\n",
    "def create_readme(export_dir, models_exported):\n",
    "    \"\"\"Create README.md with setup and usage instructions.\"\"\"\n",
    "    readme = f\"\"\"# ML Model Export Package\n",
    "\n",
    "**Export Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Symbol:** {SYMBOL if 'SYMBOL' in dir() else 'N/A'}\n",
    "**Horizon:** {TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A'} bars\n",
    "\n",
    "## Package Contents\n",
    "\n",
    "This export package contains:\n",
    "\n",
    "- **Models:** {len(models_exported)} trained model(s)\n",
    "- **Predictions:** Validation and test set predictions\n",
    "- **Metrics:** Training, validation, and test performance metrics\n",
    "- **Visualizations:** Confusion matrices, feature importance, learning curves\n",
    "- **Model Cards:** Detailed model documentation and performance analysis\n",
    "- **Data Info:** Feature names, label mappings, data statistics\n",
    "\n",
    "## Models Included\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    for model_name in models_exported:\n",
    "        readme += f\"- `{model_name}`\\n\"\n",
    "\n",
    "    readme += \"\"\"\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "```\n",
    "\u251c\u2500\u2500 models/              # Trained models (PKL, ONNX)\n",
    "\u251c\u2500\u2500 predictions/         # Model predictions (CSV)\n",
    "\u251c\u2500\u2500 metrics/             # Performance metrics (JSON)\n",
    "\u251c\u2500\u2500 visualizations/      # Plots and charts (PNG)\n",
    "\u251c\u2500\u2500 model_cards/         # Model documentation (MD)\n",
    "\u251c\u2500\u2500 data/                # Feature info and stats\n",
    "\u251c\u2500\u2500 manifest.json        # Export metadata\n",
    "\u2514\u2500\u2500 README.md            # This file\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### Load a Model\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('models/xgboost/model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### Load ONNX Model (Production)\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Create inference session\n",
    "session = ort.InferenceSession('models/xgboost/model.onnx')\n",
    "\n",
    "# Run inference\n",
    "input_name = session.get_inputs()[0].name\n",
    "predictions = session.run(None, {input_name: X_test.astype('float32')})[0]\n",
    "```\n",
    "\n",
    "### Load Predictions\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load test predictions\n",
    "test_preds = pd.read_csv('predictions/test_predictions.csv')\n",
    "print(test_preds.head())\n",
    "```\n",
    "\n",
    "## Model Cards\n",
    "\n",
    "Each model has a detailed model card in `model_cards/` with:\n",
    "- Performance metrics (validation, test, CV)\n",
    "- Configuration parameters\n",
    "- Feature importance\n",
    "- Training details\n",
    "- Usage examples\n",
    "\n",
    "## Performance Summary\n",
    "\n",
    "See `metrics/test_metrics.json` for detailed performance metrics across all models.\n",
    "\n",
    "## Support\n",
    "\n",
    "For questions or issues:\n",
    "1. Review model cards for specific model details\n",
    "2. Check manifest.json for export metadata\n",
    "3. Consult feature documentation in data/\n",
    "\n",
    "---\n",
    "\n",
    "Generated by ML Model Factory\n",
    "\"\"\"\n",
    "\n",
    "    readme_path = export_dir / 'README.md'\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    return readme_path\n",
    "\n",
    "\n",
    "def export_predictions(model_name, model_info, export_dir):\n",
    "    \"\"\"Export validation and test predictions to CSV.\"\"\"\n",
    "    pred_dir = export_dir / 'predictions' / model_name\n",
    "    pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Export validation predictions if available\n",
    "    if 'val_predictions' in model_info:\n",
    "        val_preds = model_info['val_predictions']\n",
    "        val_df = pd.DataFrame({\n",
    "            'index': range(len(val_preds['actual'])),\n",
    "            'actual': val_preds['actual'],\n",
    "            'predicted': val_preds['predicted']\n",
    "        })\n",
    "        if 'confidence' in val_preds:\n",
    "            val_df['confidence'] = val_preds['confidence']\n",
    "        val_df['correct'] = val_df['actual'] == val_df['predicted']\n",
    "\n",
    "        val_path = pred_dir / 'val_predictions.csv'\n",
    "        val_df.to_csv(val_path, index=False)\n",
    "\n",
    "    # Export test predictions if available\n",
    "    test_info = TEST_RESULTS.get(model_name, {})\n",
    "    if 'predictions' in test_info:\n",
    "        test_preds = test_info['predictions']\n",
    "        test_df = pd.DataFrame({\n",
    "            'index': range(len(test_preds['actual'])),\n",
    "            'actual': test_preds['actual'],\n",
    "            'predicted': test_preds['predicted']\n",
    "        })\n",
    "        if 'confidence' in test_preds:\n",
    "            test_df['confidence'] = test_preds['confidence']\n",
    "        test_df['correct'] = test_df['actual'] == test_df['predicted']\n",
    "\n",
    "        test_path = pred_dir / 'test_predictions.csv'\n",
    "        test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    # Create predictions summary\n",
    "    summary = {\n",
    "        'model_name': model_name,\n",
    "        'val_samples': len(val_preds['actual']) if 'val_predictions' in model_info else 0,\n",
    "        'test_samples': len(test_preds['actual']) if 'predictions' in test_info else 0,\n",
    "        'val_accuracy': (val_df['correct'].sum() / len(val_df)) if 'val_predictions' in model_info else None,\n",
    "        'test_accuracy': (test_df['correct'].sum() / len(test_df)) if 'predictions' in test_info else None\n",
    "    }\n",
    "\n",
    "    summary_path = pred_dir / 'predictions_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPORT LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "if export_model:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL EXPORT PACKAGE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Get models to export\n",
    "    models_to_export = get_models_to_export()\n",
    "\n",
    "    if not models_to_export:\n",
    "        print(\"\\n\u26a0 No models to export. Check your selection criteria.\")\n",
    "    else:\n",
    "        print(f\"\\n\ud83d\udce6 Exporting {len(models_to_export)} model(s): {', '.join(models_to_export)}\")\n",
    "\n",
    "        # Create export directory\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        symbol = SYMBOL if 'SYMBOL' in dir() else 'UNKNOWN'\n",
    "        horizon = TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'XX'\n",
    "        export_name = f\"{timestamp}_{symbol}_H{horizon}\"\n",
    "\n",
    "        export_dir = RESULTS_DIR / 'exports' / export_name\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n\ud83d\udcc1 Export directory: {export_dir}\")\n",
    "\n",
    "        # Track export statistics\n",
    "        export_stats = {\n",
    "            'models_exported': 0,\n",
    "            'onnx_exports': 0,\n",
    "            'predictions_exported': 0,\n",
    "            'visualizations_exported': 0,\n",
    "            'model_cards_generated': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "\n",
    "        export_info = {\n",
    "            'data_stats': {\n",
    "                'train_samples': TRAINING_RESULTS.get(models_to_export[0], {}).get('train_samples', 0),\n",
    "                'val_samples': TRAINING_RESULTS.get(models_to_export[0], {}).get('val_samples', 0),\n",
    "                'test_samples': TEST_RESULTS.get(models_to_export[0], {}).get('test_samples', 0),\n",
    "                'n_features': TRAINING_RESULTS.get(models_to_export[0], {}).get('n_features', 0)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Export each model\n",
    "        for model_name in models_to_export:\n",
    "            print(f\"\\n\ud83d\udcca Processing: {model_name}\")\n",
    "\n",
    "            all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "            model_info = all_results.get(model_name, {})\n",
    "\n",
    "            if not model_info:\n",
    "                print(f\"  \u26a0 No training results found for {model_name}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: No training results\")\n",
    "                continue\n",
    "\n",
    "            run_id = model_info.get('run_id')\n",
    "            if not run_id:\n",
    "                print(f\"  \u26a0 No run_id found for {model_name}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: No run_id\")\n",
    "                continue\n",
    "\n",
    "            # Create model directory\n",
    "            model_dir = export_dir / 'models' / model_name\n",
    "            model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Find and copy model file\n",
    "            source_dir = EXPERIMENTS_DIR / run_id\n",
    "            model_file = source_dir / 'model.pkl'\n",
    "\n",
    "            if not model_file.exists():\n",
    "                print(f\"  \u26a0 Model file not found: {model_file}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: Model file not found\")\n",
    "                continue\n",
    "\n",
    "            # Copy model\n",
    "            dest_model = model_dir / 'model.pkl'\n",
    "            shutil.copy2(model_file, dest_model)\n",
    "            model_size = dest_model.stat().st_size / 1e6\n",
    "            print(f\"  \u2713 Model copied ({model_size:.2f} MB)\")\n",
    "            export_stats['models_exported'] += 1\n",
    "\n",
    "            # Export to ONNX if requested\n",
    "            if include_onnx:\n",
    "                feature_names = model_info.get('feature_names', [])\n",
    "                success, message = export_to_onnx(model_info, model_name, dest_model, feature_names)\n",
    "                if success:\n",
    "                    print(f\"  \u2713 ONNX: {message}\")\n",
    "                    export_stats['onnx_exports'] += 1\n",
    "                else:\n",
    "                    print(f\"  \u26a0 ONNX: {message}\")\n",
    "\n",
    "            # Save configuration\n",
    "            config = model_info.get('config', {})\n",
    "            if config:\n",
    "                config_path = model_dir / 'config.json'\n",
    "                with open(config_path, 'w') as f:\n",
    "                    json.dump(config, f, indent=2)\n",
    "                print(f\"  \u2713 Configuration saved\")\n",
    "\n",
    "            # Export predictions\n",
    "            if include_predictions:\n",
    "                try:\n",
    "                    export_predictions(model_name, model_info, export_dir)\n",
    "                    print(f\"  \u2713 Predictions exported\")\n",
    "                    export_stats['predictions_exported'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  \u26a0 Predictions export failed: {e}\")\n",
    "\n",
    "            # Generate model card\n",
    "            if include_model_card:\n",
    "                try:\n",
    "                    card_dir = export_dir / 'model_cards'\n",
    "                    card_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                    test_info = TEST_RESULTS.get(model_name, {})\n",
    "                    cv_info = CV_RESULTS.get(model_name, {})\n",
    "\n",
    "                    card_content = generate_model_card(model_name, model_info, test_info, cv_info)\n",
    "                    card_path = card_dir / f\"{model_name}_card.md\"\n",
    "\n",
    "                    with open(card_path, 'w') as f:\n",
    "                        f.write(card_content)\n",
    "\n",
    "                    print(f\"  \u2713 Model card generated\")\n",
    "                    export_stats['model_cards_generated'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  \u26a0 Model card generation failed: {e}\")\n",
    "\n",
    "        # Copy visualizations\n",
    "        if include_visualizations:\n",
    "            print(f\"\\n\ud83c\udfa8 Copying visualizations...\")\n",
    "            viz_dir = export_dir / 'visualizations'\n",
    "            viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Copy from experiments directory\n",
    "            for model_name in models_to_export:\n",
    "                model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(model_name, {})\n",
    "                run_id = model_info.get('run_id')\n",
    "                if run_id:\n",
    "                    source_viz = EXPERIMENTS_DIR / run_id / 'visualizations'\n",
    "                    if source_viz.exists():\n",
    "                        dest_viz = viz_dir / model_name\n",
    "                        shutil.copytree(source_viz, dest_viz, dirs_exist_ok=True)\n",
    "                        viz_count = len(list(dest_viz.rglob('*.png')))\n",
    "                        export_stats['visualizations_exported'] += viz_count\n",
    "\n",
    "            if export_stats['visualizations_exported'] > 0:\n",
    "                print(f\"  \u2713 {export_stats['visualizations_exported']} visualizations copied\")\n",
    "\n",
    "        # Export metrics\n",
    "        print(f\"\\n\ud83d\udcc8 Exporting metrics...\")\n",
    "        metrics_dir = export_dir / 'metrics'\n",
    "        metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Training metrics\n",
    "        training_metrics = {m: TRAINING_RESULTS[m]['metrics'] for m in models_to_export if m in TRAINING_RESULTS}\n",
    "        with open(metrics_dir / 'training_metrics.json', 'w') as f:\n",
    "            json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "        # Test metrics\n",
    "        test_metrics = {m: TEST_RESULTS[m]['metrics'] for m in models_to_export if m in TEST_RESULTS}\n",
    "        if test_metrics:\n",
    "            with open(metrics_dir / 'test_metrics.json', 'w') as f:\n",
    "                json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "        # CV results\n",
    "        cv_metrics = {m: CV_RESULTS[m] for m in models_to_export if m in CV_RESULTS}\n",
    "        if cv_metrics:\n",
    "            with open(metrics_dir / 'cv_results.json', 'w') as f:\n",
    "                json.dump(cv_metrics, f, indent=2)\n",
    "\n",
    "        print(f\"  \u2713 Metrics exported\")\n",
    "\n",
    "        # Export data info\n",
    "        print(f\"\\n\ud83d\udcca Exporting data information...\")\n",
    "        data_dir = export_dir / 'data'\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Feature names\n",
    "        if models_to_export:\n",
    "            first_model = models_to_export[0]\n",
    "            model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(first_model, {})\n",
    "            feature_names = model_info.get('feature_names', [])\n",
    "\n",
    "            if feature_names:\n",
    "                with open(data_dir / 'feature_names.txt', 'w') as f:\n",
    "                    f.write('\\n'.join(feature_names))\n",
    "\n",
    "        # Label mapping\n",
    "        label_mapping = {-1: 'SHORT', 0: 'NEUTRAL', 1: 'LONG'}\n",
    "        with open(data_dir / 'label_mapping.json', 'w') as f:\n",
    "            json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "        # Data stats\n",
    "        with open(data_dir / 'data_stats.json', 'w') as f:\n",
    "            json.dump(export_info['data_stats'], f, indent=2)\n",
    "\n",
    "        print(f\"  \u2713 Data info exported\")\n",
    "\n",
    "        # Create manifest\n",
    "        print(f\"\\n\ud83d\udccb Creating manifest...\")\n",
    "        manifest_path = create_manifest(export_dir, models_to_export, export_info)\n",
    "        print(f\"  \u2713 Manifest created: {manifest_path.name}\")\n",
    "\n",
    "        # Create README\n",
    "        print(f\"\\n\ud83d\udcdd Creating README...\")\n",
    "        readme_path = create_readme(export_dir, models_to_export)\n",
    "        print(f\"  \u2713 README created: {readme_path.name}\")\n",
    "\n",
    "        # Calculate total size\n",
    "        total_size = sum(f.stat().st_size for f in export_dir.rglob('*') if f.is_file())\n",
    "        total_size_mb = total_size / 1e6\n",
    "\n",
    "        # Create ZIP archive\n",
    "        zip_path = None\n",
    "        if create_zip_archive:\n",
    "            print(f\"\\n\ud83d\udce6 Creating ZIP archive...\")\n",
    "            zip_base = export_dir.parent / export_name\n",
    "            zip_path = Path(shutil.make_archive(str(zip_base), 'zip', export_dir))\n",
    "            zip_size_mb = zip_path.stat().st_size / 1e6\n",
    "            print(f\"  \u2713 Archive created: {zip_path.name} ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"EXPORT SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n\ud83d\udcc1 Export Path: {export_dir}\")\n",
    "        print(f\"\\n\ud83d\udcca Models Exported: {export_stats['models_exported']}\")\n",
    "        for model_name in models_to_export:\n",
    "            model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(model_name, {})\n",
    "            formats = ['PKL']\n",
    "            if (export_dir / 'models' / model_name / 'model.onnx').exists():\n",
    "                formats.append('ONNX')\n",
    "            print(f\"  \u2713 {model_name} ({', '.join(formats)})\")\n",
    "\n",
    "        print(f\"\\n\ud83d\udce6 Package Contents:\")\n",
    "        print(f\"  \u2713 Models: {export_stats['models_exported']}\")\n",
    "        if export_stats['onnx_exports'] > 0:\n",
    "            print(f\"  \u2713 ONNX exports: {export_stats['onnx_exports']}\")\n",
    "        if include_predictions:\n",
    "            print(f\"  \u2713 Predictions: Val + Test\")\n",
    "        print(f\"  \u2713 Metrics: Training, Test, CV\")\n",
    "        if export_stats['visualizations_exported'] > 0:\n",
    "            print(f\"  \u2713 Visualizations: {export_stats['visualizations_exported']} plots\")\n",
    "        if export_stats['model_cards_generated'] > 0:\n",
    "            print(f\"  \u2713 Model Cards: {export_stats['model_cards_generated']}\")\n",
    "        print(f\"  \u2713 Data Info: Features, labels, stats\")\n",
    "        print(f\"  \u2713 README: Setup and usage guide\")\n",
    "\n",
    "        print(f\"\\n\ud83d\udcbe Total Size: {total_size_mb:.1f} MB\", end='')\n",
    "        if zip_path:\n",
    "            zip_size_mb = zip_path.stat().st_size / 1e6\n",
    "            print(f\" (compressed: {zip_size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "        if export_stats['errors']:\n",
    "            print(f\"\\n\u26a0 Errors ({len(export_stats['errors'])}):\")\n",
    "            for error in export_stats['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "\n",
    "        print(f\"\\n\u2705 Next Steps:\")\n",
    "        print(f\"1. Extract ZIP to deployment environment\")\n",
    "        print(f\"2. Review model cards for performance details\")\n",
    "        if export_stats['onnx_exports'] > 0:\n",
    "            print(f\"3. Use ONNX models for production inference\")\n",
    "        print(f\"4. Check README for usage examples\")\n",
    "\n",
    "        # Colab download helper\n",
    "        if IS_COLAB and create_zip_archive and zip_path:\n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(\"DOWNLOAD TO LOCAL\")\n",
    "            print(\"=\" * 80)\n",
    "            download_export = False  #@param {type: \"boolean\"}\n",
    "\n",
    "            if download_export:\n",
    "                try:\n",
    "                    from google.colab import files\n",
    "                    files.download(str(zip_path))\n",
    "                    print(f\"\\n\u2713 Download started: {zip_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n\u26a0 Download failed: {e}\")\n",
    "                    print(f\"Manual download from: {zip_path}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\"Model export skipped. Enable 'export_model' checkbox above to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Quick Reference\n",
    "\n",
    "## Command Line Usage\n",
    "\n",
    "```bash\n",
    "# Train single model\n",
    "python scripts/train_model.py --model xgboost --horizon 20\n",
    "\n",
    "# Train neural model\n",
    "python scripts/train_model.py --model lstm --horizon 20 --seq-len 60\n",
    "\n",
    "# Run cross-validation\n",
    "python scripts/run_cv.py --models xgboost,lightgbm --horizons 20 --n-splits 5\n",
    "\n",
    "# Train ensemble\n",
    "python scripts/train_model.py --model voting --horizon 20\n",
    "\n",
    "# List all available models\n",
    "python scripts/train_model.py --list-models\n",
    "```\n",
    "\n",
    "## Model Families\n",
    "\n",
    "| Family | Models | Best For |\n",
    "|--------|--------|----------|\n",
    "| Boosting | XGBoost, LightGBM, CatBoost | Fast, accurate, tabular data |\n",
    "| Classical | Random Forest, Logistic, SVM | Baselines, interpretability |\n",
    "| Neural | LSTM, GRU, TCN | Sequential patterns, temporal dependencies |\n",
    "| Ensemble | Voting, Stacking, Blending | Combined predictions, robustness |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}