{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Unified Pipeline\n",
    "\n",
    "**Complete ML training pipeline for OHLCV time series with 13 models.**\n",
    "\n",
    "## \u2728 Features (Production Ready)\n",
    "\n",
    "### \ud83e\udd16 Model Support (13 Models)\n",
    "- **Boosting (3):** XGBoost, LightGBM, CatBoost\n",
    "- **Neural (4):** LSTM, GRU, TCN, Transformer\n",
    "- **Classical (3):** Random Forest, Logistic Regression, SVM\n",
    "- **Ensemble (3):** Voting, Stacking, Blending\n",
    "\n",
    "### \ud83d\udd2c Advanced Capabilities\n",
    "- **Transformer Support:** Self-attention with 8-head architecture + attention visualization\n",
    "- **Hyperparameter Tuning:** Optuna integration with 20+ trials per model\n",
    "- **Cross-Validation:** Purged K-fold for time series (prevents lookahead bias)\n",
    "- **Ensemble Intelligence:** Diversity analysis, contribution metrics, production recommendations\n",
    "- **Professional Export:** ONNX conversion, model cards, ZIP packages\n",
    "\n",
    "### \ud83d\udcca Rich Visualizations\n",
    "- Confusion matrices with class-wise accuracy\n",
    "- Feature importance (boosting models)\n",
    "- Learning curves (training/validation loss)\n",
    "- Prediction distribution analysis\n",
    "- Per-class precision/recall/F1 metrics\n",
    "- **Transformer attention heatmaps** (NEW)\n",
    "\n",
    "### \ud83d\udcc8 Evaluation & Analysis\n",
    "- Test set evaluation with generalization gap analysis\n",
    "- Out-of-fold predictions for stacking\n",
    "- Cross-validation with time series purge/embargo\n",
    "- Ensemble diversity metrics (disagreement, correlation, Q-statistic)\n",
    "- Production readiness scoring\n",
    "\n",
    "## Pipeline Phases\n",
    "1. **Configuration** - All settings in one place (13 model toggles)\n",
    "2. **Environment Setup** - Auto-detects Colab vs Local\n",
    "3. **Phase 1: Data Pipeline** - Clean \u2192 Features \u2192 Labels \u2192 Splits \u2192 Scale\n",
    "4. **Phase 2: Model Training** - Train any of 13 model types\n",
    "   - 4.1 Train Models\n",
    "   - 4.2 Training Summary\n",
    "   - 4.3 Visualizations (5 types)\n",
    "   - 4.4 Transformer Attention (NEW)\n",
    "   - 4.5 Test Set Performance\n",
    "   - 4.6 Trading Performance Simulation\n",
    "5. **Phase 3: Cross-Validation** - Robust evaluation with tuning (optional)\n",
    "6. **Phase 4: Ensemble** - Combine models intelligently (optional)\n",
    "7. **Results & Export** - Professional packages with ONNX\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MASTER CONFIGURATION\n",
    "\n",
    "**Configure ALL settings here. No need to modify any other cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 Master Configuration Panel { display-mode: \"form\" }\n",
    "#@markdown ## Data Configuration\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ### Contract Selection\n",
    "SYMBOL = \"SI\"  #@param [\"SI\", \"MES\", \"MGC\", \"ES\", \"GC\", \"NQ\", \"CL\", \"HG\", \"ZB\", \"ZN\"]\n",
    "#@markdown Select ONE contract. Each contract is trained in complete isolation.\n",
    "#@markdown - **SI** = Silver, **MES** = Micro E-mini S&P, **MGC** = Micro Gold\n",
    "#@markdown - **ES** = E-mini S&P, **GC** = Gold, **NQ** = E-mini Nasdaq\n",
    "#@markdown - **CL** = Crude Oil, **HG** = Copper, **ZB/ZN** = Bonds\n",
    "\n",
    "#@markdown ### Date Range Selection\n",
    "DATE_RANGE = \"2019-2024\"  #@param [\"2019-2024\", \"2020-2024\", \"2021-2024\", \"2022-2024\", \"2023-2024\", \"Full Dataset\"]\n",
    "#@markdown Select the date range for your data\n",
    "\n",
    "#@markdown ### Data Source\n",
    "DRIVE_DATA_PATH = \"research/data/raw\"  #@param {type: \"string\"}\n",
    "#@markdown Google Drive path relative to My Drive\n",
    "\n",
    "#@markdown ### Custom Data File (optional)\n",
    "CUSTOM_DATA_FILE = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Leave empty for auto-detection, or specify exact filename (e.g., `si_historical_2019_2024.parquet`)\n",
    "\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Path Configuration\n",
    "\n",
    "#@markdown ### GitHub Repository URL (Colab only)\n",
    "GITHUB_REPO_URL = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Leave empty to skip git clone (use local files or Drive mount).\n",
    "#@markdown Example: `https://github.com/yourusername/research.git`\n",
    "\n",
    "#@markdown ### Colab Project Path\n",
    "COLAB_PROJECT_PATH = \"/content/research\"  #@param {type: \"string\"}\n",
    "#@markdown Where to clone/mount the project in Colab\n",
    "\n",
    "#@markdown ### Local Project Root (local only)\n",
    "LOCAL_PROJECT_ROOT = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Leave empty to use current working directory (Path.cwd())\n",
    "#@markdown Example: `/home/user/projects/research`\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Pipeline Configuration\n",
    "\n",
    "#@markdown ### Label Horizons (bars)\n",
    "HORIZONS = \"5,10,15,20\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated prediction horizons\n",
    "\n",
    "#@markdown ### Train/Val/Test Split Ratios\n",
    "TRAIN_RATIO = 0.70  #@param {type: \"number\"}\n",
    "VAL_RATIO = 0.15  #@param {type: \"number\"}\n",
    "TEST_RATIO = 0.15  #@param {type: \"number\"}\n",
    "\n",
    "#@markdown ### Leakage Prevention\n",
    "PURGE_BARS = 60  #@param {type: \"integer\"}\n",
    "#@markdown Bars to purge around train/val boundary (3x max horizon)\n",
    "EMBARGO_BARS = 1440  #@param {type: \"integer\"}\n",
    "#@markdown Embargo period after validation (~5 days at 5-min)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Model Training Configuration\n",
    "\n",
    "#@markdown ### Training Horizon\n",
    "TRAINING_HORIZON = 20  #@param [5, 10, 15, 20]\n",
    "#@markdown Which horizon to train models on\n",
    "\n",
    "#@markdown ### Model Selection\n",
    "#@markdown #### Boosting Models\n",
    "TRAIN_XGBOOST = True  #@param {type: \"boolean\"}\n",
    "TRAIN_LIGHTGBM = True  #@param {type: \"boolean\"}\n",
    "TRAIN_CATBOOST = True  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Classical Models\n",
    "TRAIN_RANDOM_FOREST = False  #@param {type: \"boolean\"}\n",
    "TRAIN_LOGISTIC = False  #@param {type: \"boolean\"}\n",
    "TRAIN_SVM = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Neural Network Models\n",
    "TRAIN_LSTM = False  #@param {type: \"boolean\"}\n",
    "TRAIN_GRU = False  #@param {type: \"boolean\"}\n",
    "TRAIN_TCN = False  #@param {type: \"boolean\"}\n",
    "TRAIN_TRANSFORMER = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Ensemble Models\n",
    "TRAIN_VOTING = False  #@param {type: \"boolean\"}\n",
    "TRAIN_STACKING = False  #@param {type: \"boolean\"}\n",
    "TRAIN_BLENDING = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ### Neural Network Settings\n",
    "SEQUENCE_LENGTH = 60  #@param {type: \"slider\", min: 30, max: 120, step: 10}\n",
    "BATCH_SIZE = 256  #@param [64, 128, 256, 512, 1024]\n",
    "MAX_EPOCHS = 50  #@param {type: \"integer\"}\n",
    "EARLY_STOPPING_PATIENCE = 10  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Transformer Settings (when enabled)\n",
    "TRANSFORMER_SEQUENCE_LENGTH = 128  #@param {type: \"integer\"}\n",
    "TRANSFORMER_N_HEADS = 8  #@param [4, 8, 16]\n",
    "TRANSFORMER_N_LAYERS = 3  #@param [2, 3, 4, 6]\n",
    "TRANSFORMER_D_MODEL = 256  #@param [128, 256, 512]\n",
    "\n",
    "#@markdown ### Boosting Settings\n",
    "N_ESTIMATORS = 500  #@param {type: \"integer\"}\n",
    "BOOSTING_EARLY_STOPPING = 50  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Voting Ensemble Configuration (when enabled)\n",
    "VOTING_BASE_MODELS = \"xgboost,lightgbm,catboost\"  #@param {type: \"string\"}\n",
    "VOTING_WEIGHTS = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Leave weights empty for equal weighting\n",
    "\n",
    "#@markdown ### Stacking Ensemble Configuration (when enabled)\n",
    "STACKING_BASE_MODELS = \"xgboost,lightgbm,lstm\"  #@param {type: \"string\"}\n",
    "STACKING_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"xgboost\", \"random_forest\"]\n",
    "STACKING_N_FOLDS = 5  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Blending Ensemble Configuration (when enabled)\n",
    "BLENDING_BASE_MODELS = \"xgboost,lightgbm,random_forest\"  #@param {type: \"string\"}\n",
    "BLENDING_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"xgboost\", \"random_forest\"]\n",
    "BLENDING_HOLDOUT_RATIO = 0.2  #@param {type: \"number\"}\n",
    "\n",
    "#@markdown ### Class Balancing\n",
    "USE_CLASS_WEIGHTS = True  #@param {type: \"boolean\"}\n",
    "#@markdown Automatically balance classes during training (recommended for imbalanced data)\n",
    "USE_SAMPLE_WEIGHTS = True  #@param {type: \"boolean\"}\n",
    "#@markdown Use quality-based sample weights from pipeline (if available)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Optional Phases\n",
    "\n",
    "#@markdown ### Cross-Validation\n",
    "RUN_CROSS_VALIDATION = False  #@param {type: \"boolean\"}\n",
    "CV_N_SPLITS = 5  #@param {type: \"integer\"}\n",
    "CV_TUNE_HYPERPARAMS = False  #@param {type: \"boolean\"}\n",
    "CV_N_TRIALS = 20  #@param {type: \"integer\"}\n",
    "CV_USE_PRESCALED = True  #@param {type: \"boolean\"}\n",
    "#@markdown Use pre-scaled data (faster) or scale per-fold (stricter, slower)\n",
    "\n",
    "#@markdown ### Ensemble Training\n",
    "TRAIN_ENSEMBLE = False  #@param {type: \"boolean\"}\n",
    "ENSEMBLE_TYPE = \"voting\"  #@param [\"voting\", \"stacking\", \"blending\"]\n",
    "ENSEMBLE_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"random_forest\", \"xgboost\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Execution Options\n",
    "\n",
    "#@markdown ### What to Run\n",
    "RUN_DATA_PIPELINE = True  #@param {type: \"boolean\"}\n",
    "#@markdown Run Phase 1 data pipeline\n",
    "RUN_MODEL_TRAINING = True  #@param {type: \"boolean\"}\n",
    "#@markdown Run Phase 2 model training\n",
    "\n",
    "#@markdown ### Memory Management\n",
    "SAFE_MODE = False  #@param {type: \"boolean\"}\n",
    "#@markdown Enable for low-memory environments (reduces batch size, limits iterations)\n",
    "\n",
    "#@markdown ### Reproducibility\n",
    "RANDOM_SEED = 42  #@param {type: \"integer\"}\n",
    "#@markdown Random seed for reproducibility (set to 0 for random initialization)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD CONFIGURATION (DO NOT MODIFY BELOW)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Parse horizons\n",
    "HORIZON_LIST = [int(h.strip()) for h in HORIZONS.split(',')]\n",
    "\n",
    "# Parse date range\n",
    "if DATE_RANGE == \"Full Dataset\":\n",
    "    YEAR_START = None\n",
    "    YEAR_END = None\n",
    "else:\n",
    "    years = DATE_RANGE.split('-')\n",
    "    YEAR_START = int(years[0])\n",
    "    YEAR_END = int(years[1])\n",
    "\n",
    "# Build model list\n",
    "MODELS_TO_TRAIN = []\n",
    "if TRAIN_XGBOOST: MODELS_TO_TRAIN.append('xgboost')\n",
    "if TRAIN_LIGHTGBM: MODELS_TO_TRAIN.append('lightgbm')\n",
    "if TRAIN_CATBOOST: MODELS_TO_TRAIN.append('catboost')\n",
    "if TRAIN_RANDOM_FOREST: MODELS_TO_TRAIN.append('random_forest')\n",
    "if TRAIN_LOGISTIC: MODELS_TO_TRAIN.append('logistic')\n",
    "if TRAIN_SVM: MODELS_TO_TRAIN.append('svm')\n",
    "if TRAIN_LSTM: MODELS_TO_TRAIN.append('lstm')\n",
    "if TRAIN_GRU: MODELS_TO_TRAIN.append('gru')\n",
    "if TRAIN_TCN: MODELS_TO_TRAIN.append('tcn')\n",
    "if TRAIN_TRANSFORMER: MODELS_TO_TRAIN.append('transformer')\n",
    "if TRAIN_VOTING: MODELS_TO_TRAIN.append('voting')\n",
    "if TRAIN_STACKING: MODELS_TO_TRAIN.append('stacking')\n",
    "if TRAIN_BLENDING: MODELS_TO_TRAIN.append('blending')\n",
    "\n",
    "# Date range will be auto-detected from data file\n",
    "DATA_START = None  # Auto-detected\n",
    "DATA_END = None    # Auto-detected\n",
    "\n",
    "# Safe mode adjustments\n",
    "if SAFE_MODE:\n",
    "    BATCH_SIZE = min(BATCH_SIZE, 64)\n",
    "    N_ESTIMATORS = min(N_ESTIMATORS, 300)\n",
    "    SEQUENCE_LENGTH = min(SEQUENCE_LENGTH, 30)\n",
    "    TRANSFORMER_SEQUENCE_LENGTH = min(TRANSFORMER_SEQUENCE_LENGTH, 64)\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=\" * 70)\n",
    "print(\" ML PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  Contract:        {SYMBOL}\")\n",
    "print(f\"  Date Range:      {DATE_RANGE}\")\n",
    "if CUSTOM_DATA_FILE:\n",
    "    print(f\"  Custom File:     {CUSTOM_DATA_FILE}\")\n",
    "print(f\"  Horizons:        {HORIZON_LIST}\")\n",
    "print(f\"  Split Ratios:    {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")\n",
    "print(f\"  Training Horizon: H{TRAINING_HORIZON}\")\n",
    "print(f\"  Models:          {MODELS_TO_TRAIN if MODELS_TO_TRAIN else 'None selected'}\")\n",
    "if MODELS_TO_TRAIN:\n",
    "    boosting_models = [m for m in MODELS_TO_TRAIN if m in ['xgboost', 'lightgbm', 'catboost']]\n",
    "    classical_models = [m for m in MODELS_TO_TRAIN if m in ['random_forest', 'logistic', 'svm']]\n",
    "    neural_models = [m for m in MODELS_TO_TRAIN if m in ['lstm', 'gru', 'tcn', 'transformer']]\n",
    "    ensemble_models = [m for m in MODELS_TO_TRAIN if m in ['voting', 'stacking', 'blending']]\n",
    "    if boosting_models:\n",
    "        print(f\"    Boosting:      {boosting_models}\")\n",
    "    if classical_models:\n",
    "        print(f\"    Classical:     {classical_models}\")\n",
    "    if neural_models:\n",
    "        print(f\"    Neural:        {neural_models}\")\n",
    "    if ensemble_models:\n",
    "        print(f\"    Ensemble:      {ensemble_models}\")\n",
    "print(f\"\\n  Run Pipeline:    {RUN_DATA_PIPELINE}\")\n",
    "print(f\"  Run Training:    {RUN_MODEL_TRAINING}\")\n",
    "print(f\"  Cross-Validation: {RUN_CROSS_VALIDATION}\")\n",
    "print(f\"  Class Weights:   {USE_CLASS_WEIGHTS}\")\n",
    "print(f\"  Sample Weights:  {USE_SAMPLE_WEIGHTS}\")\n",
    "print(f\"  Ensemble:        {TRAIN_ENSEMBLE}\")\n",
    "print(f\"  Safe Mode:       {SAFE_MODE}\")\n",
    "print(f\"  Random Seed:     {RANDOM_SEED}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration complete! Run the next cells sequentially.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ENVIRONMENT SETUP\n",
    "\n",
    "Auto-detects Colab vs Local environment and sets up paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Environment Detection & Setup { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# ENVIRONMENT DETECTION\n",
    "# ============================================================\n",
    "IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Global flag for Drive usage\n",
    "USE_DRIVE = True  # Set to False to skip Drive mounting\n",
    "\n",
    "# ============================================================\n",
    "# PROJECT ROOT RESOLUTION\n",
    "# ============================================================\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH)\n",
    "    elif LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\n[Environment] Google Colab detected\")\n",
    "\n",
    "    # Mount Google Drive with enhanced validation\n",
    "    if USE_DRIVE:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "        # Validate Drive data path exists\n",
    "        drive_data_path = Path('/content/drive/MyDrive') / DRIVE_DATA_PATH\n",
    "        if not drive_data_path.exists():\n",
    "            print(f\"\\n  [WARNING] Drive path not found: {DRIVE_DATA_PATH}\")\n",
    "            print(\"  Available directories in /content/drive/MyDrive:\")\n",
    "            try:\n",
    "                for p in sorted(Path('/content/drive/MyDrive').iterdir())[:10]:\n",
    "                    if p.is_dir():\n",
    "                        print(f\"    - {p.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Could not list directories: {e}\")\n",
    "        else:\n",
    "            print(f\"\\n  [OK] Drive data path verified: {drive_data_path}\")\n",
    "    else:\n",
    "        print(\"\\n[Setup] Google Drive mounting disabled (USE_DRIVE=False)\")\n",
    "\n",
    "    # Clone/update repository (conditional on GITHUB_REPO_URL)\n",
    "    REPO_PATH = Path(COLAB_PROJECT_PATH)\n",
    "    if GITHUB_REPO_URL:\n",
    "        if not REPO_PATH.exists():\n",
    "            print(\"\\n[Setup] Cloning repository...\")\n",
    "            !git clone {GITHUB_REPO_URL} {COLAB_PROJECT_PATH}\n",
    "        else:\n",
    "            print(\"\\n[Setup] Updating repository...\")\n",
    "            !cd {COLAB_PROJECT_PATH} && git pull --quiet\n",
    "    else:\n",
    "        if not REPO_PATH.exists():\n",
    "            print(\"\\n[Setup] No GITHUB_REPO_URL configured.\")\n",
    "            print(\"Please either:\")\n",
    "            print(\"  1. Set GITHUB_REPO_URL in Section 1\")\n",
    "            print(\"  2. Upload your code manually\")\n",
    "            print(\"  3. Mount from Google Drive\")\n",
    "            REPO_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            print(f\"\\n[Setup] Using existing project at {REPO_PATH}\")\n",
    "\n",
    "    # Set paths\n",
    "    PROJECT_ROOT = REPO_PATH\n",
    "    DRIVE_ROOT = Path('/content/drive/MyDrive') if USE_DRIVE else None\n",
    "    RAW_DATA_DIR = DRIVE_ROOT / DRIVE_DATA_PATH if DRIVE_ROOT else PROJECT_ROOT / 'data/raw'\n",
    "    RESULTS_DIR = DRIVE_ROOT / 'research/experiments' if DRIVE_ROOT else PROJECT_ROOT / 'experiments'\n",
    "\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "\n",
    "else:\n",
    "    print(\"\\n[Environment] Local environment detected\")\n",
    "\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "    DRIVE_ROOT = None\n",
    "    RAW_DATA_DIR = PROJECT_ROOT / 'data/raw'\n",
    "    RESULTS_DIR = PROJECT_ROOT / 'experiments'\n",
    "    USE_DRIVE = False\n",
    "\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Add to Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Create output directories\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "EXPERIMENTS_DIR = RESULTS_DIR / 'runs'\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create checkpoints directory for Colab timeout protection\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / 'checkpoints'\n",
    "if IS_COLAB:\n",
    "    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n  Project Root:  {PROJECT_ROOT}\")\n",
    "print(f\"  Raw Data:      {RAW_DATA_DIR}\")\n",
    "print(f\"  Splits:        {SPLITS_DIR}\")\n",
    "print(f\"  Experiments:   {EXPERIMENTS_DIR}\")\n",
    "if IS_COLAB:\n",
    "    print(f\"  Checkpoints:   {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.2 Install Dependencies { display-mode: \"form\" }\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for Colab environment.\"\"\"\n",
    "    \n",
    "    # Core dependencies that Colab may not have or need specific versions\n",
    "    packages = [\n",
    "        # PyTorch with CUDA support (Colab has PyTorch but we ensure CUDA version)\n",
    "        \"torch>=2.0.0\",\n",
    "        \"torchvision\",\n",
    "        \n",
    "        # ML frameworks (Colab has sklearn, but we need specific boosting libs)\n",
    "        \"xgboost>=2.0.0\",\n",
    "        \"lightgbm>=4.0.0\",\n",
    "        \"catboost>=1.2.0\",\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        \"optuna>=3.4.0\",\n",
    "        \n",
    "        # Technical analysis & features\n",
    "        \"ta>=0.11.0\",\n",
    "        \"pywavelets>=1.5.0\",\n",
    "        \n",
    "        # Data processing (ensure compatible versions)\n",
    "        \"pyarrow>=14.0.0\",\n",
    "        \"numba>=0.58.0\",\n",
    "        \n",
    "        # Utilities\n",
    "        \"psutil>=5.9.0\",\n",
    "        \"joblib>=1.3.0\",\n",
    "        \"pyyaml>=6.0.0\",\n",
    "        \n",
    "        # ONNX export support\n",
    "        \"onnx>=1.15.0\",\n",
    "        \"onnxruntime>=1.16.0\",\n",
    "        \"skl2onnx>=1.16.0\",\n",
    "    ]\n",
    "    \n",
    "    # Join packages for single pip command\n",
    "    pkg_str = \" \".join(f'\"{p}\"' for p in packages)\n",
    "    \n",
    "    print(\"[Dependencies] Installing packages...\")\n",
    "    subprocess.run(\n",
    "        f\"{sys.executable} -m pip install -q {pkg_str}\",\n",
    "        shell=True,\n",
    "        check=True\n",
    "    )\n",
    "    print(\"[Dependencies] Core packages installed!\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    install_packages()\n",
    "    \n",
    "    # Install local package for src.* imports\n",
    "    print(\"[Dependencies] Installing local package...\")\n",
    "    import os\n",
    "    os.chdir(str(PROJECT_ROOT))\n",
    "    subprocess.run(\n",
    "        f\"{sys.executable} -m pip install -q -e .\",\n",
    "        shell=True,\n",
    "        check=True\n",
    "    )\n",
    "    print(\"[Dependencies] Local package installed!\")\n",
    "else:\n",
    "    print(\"[Dependencies] Local environment - assuming packages installed.\")\n",
    "    print(\"  If needed, run: pip install -e .\")\n",
    "\n",
    "# Verify critical imports\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" DEPENDENCY VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm  # Works in both Colab and local\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced progress bar for Colab (uses notebook widgets when available)\n",
    "if IS_COLAB:\n",
    "    try:\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        # tqdm_notebook provides better widget rendering in Colab\n",
    "        print(\"  [Progress] Using enhanced Colab progress bars\")\n",
    "    except ImportError:\n",
    "        tqdm_notebook = tqdm\n",
    "        print(\"  [Progress] Using standard progress bars\")\n",
    "else:\n",
    "    tqdm_notebook = tqdm\n",
    "\n",
    "# Core library versions\n",
    "print(f\"\\n  pandas:      {pd.__version__}\")\n",
    "print(f\"  numpy:       {np.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"  torch:       {torch.__version__}\")\n",
    "\n",
    "import sklearn\n",
    "print(f\"  sklearn:     {sklearn.__version__}\")\n",
    "\n",
    "try:\n",
    "    import xgboost\n",
    "    print(f\"  xgboost:     {xgboost.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"  xgboost:     NOT INSTALLED\")\n",
    "\n",
    "try:\n",
    "    import lightgbm\n",
    "    print(f\"  lightgbm:    {lightgbm.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"  lightgbm:    NOT INSTALLED\")\n",
    "\n",
    "try:\n",
    "    import catboost\n",
    "    print(f\"  catboost:    {catboost.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"  catboost:    NOT INSTALLED\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    print(f\"  optuna:      {optuna.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"  optuna:      NOT INSTALLED\")\n",
    "\n",
    "try:\n",
    "    import onnx\n",
    "    print(f\"  onnx:        {onnx.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"  onnx:        NOT INSTALLED (optional)\")\n",
    "\n",
    "try:\n",
    "    import onnxruntime\n",
    "    print(f\"  onnxruntime: {onnxruntime.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"  onnxruntime: NOT INSTALLED (optional)\")\n",
    "\n",
    "# Verify local package\n",
    "try:\n",
    "    import src\n",
    "    print(f\"\\n  Local src package: OK\")\n",
    "except ImportError:\n",
    "    print(f\"\\n  Local src package: NOT FOUND - run 'pip install -e .'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.3 GPU Detection & Optimization { display-mode: \"form\" }\n",
    "\n",
    "import torch\n",
    "\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_NAME = None\n",
    "GPU_MEMORY = 0\n",
    "GPU_COMPUTE_CAPABILITY = None\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" HARDWARE DETECTION & OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    GPU_NAME = props.name\n",
    "    GPU_MEMORY = props.total_memory / (1024**3)\n",
    "    GPU_COMPUTE_CAPABILITY = (props.major, props.minor)\n",
    "\n",
    "    print(f\"\\n  GPU: {GPU_NAME}\")\n",
    "    print(f\"  Memory: {GPU_MEMORY:.1f} GB\")\n",
    "    print(f\"  Compute: {props.major}.{props.minor}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # GPU MEMORY OPTIMIZATION (Colab-specific)\n",
    "    # ============================================================\n",
    "    if IS_COLAB:\n",
    "        # Clear any existing GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Set memory fraction to avoid OOM errors\n",
    "        # Leave 20% headroom for system/display\n",
    "        torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "\n",
    "        # Enable TF32 for faster computation on Ampere+ GPUs (compute >= 8.0)\n",
    "        if props.major >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            print(\"\\n  [Optimization] TF32 enabled (Ampere+ GPU)\")\n",
    "\n",
    "        # Enable cuDNN benchmark for consistent input sizes\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"  [Optimization] cuDNN benchmark enabled\")\n",
    "        print(\"  [Optimization] Memory fraction set to 80%\")\n",
    "\n",
    "    # Adjust batch size based on GPU memory\n",
    "    if GPU_MEMORY >= 40:\n",
    "        RECOMMENDED_BATCH = 1024\n",
    "    elif GPU_MEMORY >= 15:\n",
    "        RECOMMENDED_BATCH = 512\n",
    "    elif GPU_MEMORY >= 8:\n",
    "        RECOMMENDED_BATCH = 256\n",
    "    else:\n",
    "        RECOMMENDED_BATCH = 128\n",
    "\n",
    "    print(f\"\\n  Recommended batch size: {RECOMMENDED_BATCH}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n  GPU: Not available (using CPU)\")\n",
    "    print(\"  Tip: Runtime -> Change runtime type -> GPU\")\n",
    "    RECOMMENDED_BATCH = 128\n",
    "\n",
    "# ============================================================\n",
    "# MIXED PRECISION CONFIGURATION\n",
    "# ============================================================\n",
    "USE_AMP = False  # Automatic Mixed Precision flag\n",
    "AMP_DTYPE = torch.float32\n",
    "\n",
    "if GPU_AVAILABLE and IS_COLAB:\n",
    "    # Check if GPU supports BF16 (Ampere and newer, compute >= 8.0)\n",
    "    if GPU_COMPUTE_CAPABILITY[0] >= 8:\n",
    "        USE_AMP = True\n",
    "        AMP_DTYPE = torch.bfloat16\n",
    "        print(\"\\n  [Mixed Precision] BFloat16 available (Ampere+ GPU)\")\n",
    "    # Check for FP16 support (Volta and newer, compute >= 7.0)\n",
    "    elif GPU_COMPUTE_CAPABILITY[0] >= 7:\n",
    "        USE_AMP = True\n",
    "        AMP_DTYPE = torch.float16\n",
    "        print(\"\\n  [Mixed Precision] Float16 available (Volta+ GPU)\")\n",
    "    else:\n",
    "        print(\"\\n  [Mixed Precision] Not available (older GPU)\")\n",
    "\n",
    "# Check for neural models without GPU\n",
    "NEURAL_MODELS = {'lstm', 'gru', 'tcn', 'transformer', 'patchtst', 'itransformer', 'tft', 'nbeats', 'inceptiontime', 'resnet1d'}\n",
    "selected_neural = set(MODELS_TO_TRAIN) & NEURAL_MODELS\n",
    "if selected_neural and not GPU_AVAILABLE:\n",
    "    print(f\"\\n  [WARNING] Neural models selected but no GPU: {selected_neural}\")\n",
    "    print(\"  Training will be significantly slower on CPU.\")\n",
    "    print(\"  Consider: Runtime -> Change runtime type -> T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.4 Memory & Checkpoint Utilities { display-mode: \"form\" }\n",
    "\n",
    "import psutil\n",
    "import gc\n",
    "import torch\n",
    "import datetime\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure GPU_AVAILABLE is defined (in case cells run out of order)\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'CHECKPOINT_DIR' not in dir():\n",
    "    CHECKPOINT_DIR = Path('./checkpoints')\n",
    "if 'USE_DRIVE' not in dir():\n",
    "    USE_DRIVE = False\n",
    "\n",
    "# ============================================================\n",
    "# MEMORY UTILITIES\n",
    "# ============================================================\n",
    "def print_memory_status(label: str = \"Current\"):\n",
    "    \"\"\"Print current RAM and GPU memory usage.\"\"\"\n",
    "    print(f\"\\n--- Memory: {label} ---\")\n",
    "\n",
    "    # RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM: {ram.used/1e9:.1f}GB / {ram.total/1e9:.1f}GB ({ram.percent}%)\")\n",
    "\n",
    "    # GPU\n",
    "    if GPU_AVAILABLE:\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved / {total:.1f}GB total\")\n",
    "\n",
    "def clear_memory(verbose: bool = True):\n",
    "    \"\"\"Clear RAM and GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    if verbose:\n",
    "        print(\"Memory cleared.\")\n",
    "\n",
    "# ============================================================\n",
    "# CHECKPOINT UTILITIES (Colab Timeout Protection)\n",
    "# ============================================================\n",
    "def save_checkpoint(data, name: str, to_drive: bool = True):\n",
    "    \"\"\"\n",
    "    Save checkpoint to prevent losing work on Colab timeout.\n",
    "\n",
    "    Args:\n",
    "        data: Any picklable object (dict, DataFrame, model, etc.)\n",
    "        name: Base name for the checkpoint file\n",
    "        to_drive: If True and Drive is mounted, also save to Drive\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{name}_{timestamp}.pkl\"\n",
    "\n",
    "    # Save to local checkpoints directory\n",
    "    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    local_path = CHECKPOINT_DIR / filename\n",
    "    joblib.dump(data, local_path)\n",
    "    print(f\"[Checkpoint] Saved locally: {local_path}\")\n",
    "\n",
    "    # Also save to Drive for persistence across sessions\n",
    "    if IS_COLAB and USE_DRIVE and to_drive:\n",
    "        drive_checkpoint_dir = Path('/content/drive/MyDrive/ml_pipeline_checkpoints')\n",
    "        drive_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        drive_path = drive_checkpoint_dir / filename\n",
    "        joblib.dump(data, drive_path)\n",
    "        print(f\"[Checkpoint] Saved to Drive: {drive_path}\")\n",
    "\n",
    "    return local_path\n",
    "\n",
    "def load_latest_checkpoint(name: str, from_drive: bool = True):\n",
    "    \"\"\"\n",
    "    Load the most recent checkpoint matching the given name.\n",
    "\n",
    "    Args:\n",
    "        name: Base name to search for\n",
    "        from_drive: If True, prefer Drive checkpoints over local\n",
    "\n",
    "    Returns:\n",
    "        Loaded checkpoint data, or None if not found\n",
    "    \"\"\"\n",
    "    checkpoints = []\n",
    "\n",
    "    # Search local checkpoints\n",
    "    if CHECKPOINT_DIR.exists():\n",
    "        checkpoints.extend(list(CHECKPOINT_DIR.glob(f\"{name}_*.pkl\")))\n",
    "\n",
    "    # Search Drive checkpoints\n",
    "    if IS_COLAB and USE_DRIVE and from_drive:\n",
    "        drive_dir = Path('/content/drive/MyDrive/ml_pipeline_checkpoints')\n",
    "        if drive_dir.exists():\n",
    "            checkpoints.extend(list(drive_dir.glob(f\"{name}_*.pkl\")))\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(f\"[Checkpoint] No checkpoints found for '{name}'\")\n",
    "        return None\n",
    "\n",
    "    # Sort by modification time (newest first)\n",
    "    checkpoints.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    latest = checkpoints[0]\n",
    "\n",
    "    print(f\"[Checkpoint] Loading: {latest}\")\n",
    "    return joblib.load(latest)\n",
    "\n",
    "def list_checkpoints():\n",
    "    \"\"\"List all available checkpoints.\"\"\"\n",
    "    print(\"\\n--- Available Checkpoints ---\")\n",
    "\n",
    "    all_checkpoints = []\n",
    "\n",
    "    # Local checkpoints\n",
    "    if CHECKPOINT_DIR.exists():\n",
    "        for cp in sorted(CHECKPOINT_DIR.glob(\"*.pkl\")):\n",
    "            all_checkpoints.append((\"Local\", cp))\n",
    "\n",
    "    # Drive checkpoints\n",
    "    if IS_COLAB and USE_DRIVE:\n",
    "        drive_dir = Path('/content/drive/MyDrive/ml_pipeline_checkpoints')\n",
    "        if drive_dir.exists():\n",
    "            for cp in sorted(drive_dir.glob(\"*.pkl\")):\n",
    "                all_checkpoints.append((\"Drive\", cp))\n",
    "\n",
    "    if not all_checkpoints:\n",
    "        print(\"  No checkpoints found.\")\n",
    "    else:\n",
    "        for location, cp in all_checkpoints:\n",
    "            size_mb = cp.stat().st_size / 1e6\n",
    "            print(f\"  [{location}] {cp.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# ============================================================\n",
    "# MEMORY-EFFICIENT DATA LOADING\n",
    "# ============================================================\n",
    "def load_data_efficiently(path, chunk_size: int = 50000):\n",
    "    \"\"\"\n",
    "    Load data with memory optimization for Colab.\n",
    "\n",
    "    Args:\n",
    "        path: Path to parquet file\n",
    "        chunk_size: Not used for parquet, kept for API compatibility\n",
    "\n",
    "    Returns:\n",
    "        Loaded DataFrame\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Clear memory before loading large datasets\n",
    "    if IS_COLAB:\n",
    "        gc.collect()\n",
    "        if GPU_AVAILABLE:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Load the parquet file\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    # Optimize memory usage\n",
    "    if IS_COLAB:\n",
    "        # Downcast numeric columns where possible\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype('float32')\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            if df[col].min() >= 0 and df[col].max() < 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "            elif df[col].min() >= -32768 and df[col].max() < 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            elif df[col].min() >= -2147483648 and df[col].max() < 2147483647:\n",
    "                df[col] = df[col].astype('int32')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Memory and checkpoint utilities loaded.\")\n",
    "print_memory_status(\"Initial\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\n[Colab] Checkpoint utilities available:\")\n",
    "    print(\"  - save_checkpoint(data, 'name') - Save work periodically\")\n",
    "    print(\"  - load_latest_checkpoint('name') - Resume after timeout\")\n",
    "    print(\"  - list_checkpoints() - View available checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. PHASE 1: DATA PIPELINE\n",
    "\n",
    "Processes raw OHLCV data into training-ready datasets.\n",
    "\n",
    "**Pipeline stages:**\n",
    "1. Load raw 1-minute data\n",
    "2. Clean and resample to 5-minute bars\n",
    "3. Generate 150+ technical features\n",
    "4. Apply triple-barrier labeling\n",
    "5. Create train/val/test splits with purge/embargo\n",
    "6. Scale features (train-only fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Verify Raw Data & Detect Date Range { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined (in case cells run out of order)\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'RAW_DATA_DIR' not in dir():\n",
    "    if IS_COLAB:\n",
    "        RAW_DATA_DIR = Path('/content/drive/MyDrive') / DRIVE_DATA_PATH\n",
    "    else:\n",
    "        RAW_DATA_DIR = Path('./data/raw')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" RAW DATA VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nLooking for {SYMBOL} data in: {RAW_DATA_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# FLEXIBLE FILE DETECTION\n",
    "# ============================================================\n",
    "RAW_DATA_FILE = None\n",
    "\n",
    "# If custom file specified, use it directly\n",
    "if CUSTOM_DATA_FILE:\n",
    "    custom_path = RAW_DATA_DIR / CUSTOM_DATA_FILE\n",
    "    if custom_path.exists():\n",
    "        RAW_DATA_FILE = custom_path\n",
    "        print(f\"\\n  Using custom file: {CUSTOM_DATA_FILE}\")\n",
    "    else:\n",
    "        print(f\"\\n  [WARNING] Custom file not found: {CUSTOM_DATA_FILE}\")\n",
    "\n",
    "# Auto-detect file with flexible patterns\n",
    "if RAW_DATA_FILE is None and RAW_DATA_DIR.exists():\n",
    "    symbol_lower = SYMBOL.lower()\n",
    "    symbol_upper = SYMBOL.upper()\n",
    "    \n",
    "    # Build list of all matching files\n",
    "    matching_files = []\n",
    "    \n",
    "    for f in RAW_DATA_DIR.iterdir():\n",
    "        if f.suffix not in ['.parquet', '.csv']:\n",
    "            continue\n",
    "        \n",
    "        fname_lower = f.name.lower()\n",
    "        \n",
    "        # Check if file contains the symbol (case-insensitive)\n",
    "        if symbol_lower in fname_lower:\n",
    "            # Priority scoring: prefer files with date range matching config\n",
    "            priority = 0\n",
    "            \n",
    "            # Boost priority if filename contains the configured date range\n",
    "            if YEAR_START and YEAR_END:\n",
    "                date_pattern = f\"{YEAR_START}_{YEAR_END}|{YEAR_START}-{YEAR_END}\"\n",
    "                if re.search(date_pattern, fname_lower):\n",
    "                    priority += 10\n",
    "            \n",
    "            # Boost priority for common naming patterns\n",
    "            if '_1m' in fname_lower or '_1min' in fname_lower:\n",
    "                priority += 5\n",
    "            if 'historical' in fname_lower:\n",
    "                priority += 3\n",
    "            if f.suffix == '.parquet':\n",
    "                priority += 2  # Prefer parquet over CSV\n",
    "            \n",
    "            matching_files.append((priority, f))\n",
    "    \n",
    "    # Sort by priority (highest first) and pick best match\n",
    "    if matching_files:\n",
    "        matching_files.sort(key=lambda x: x[0], reverse=True)\n",
    "        RAW_DATA_FILE = matching_files[0][1]\n",
    "        \n",
    "        if len(matching_files) > 1:\n",
    "            print(f\"\\n  Found {len(matching_files)} matching files:\")\n",
    "            for pri, f in matching_files[:5]:\n",
    "                marker = \"\u2192\" if f == RAW_DATA_FILE else \" \"\n",
    "                print(f\"    {marker} {f.name} (priority: {pri})\")\n",
    "\n",
    "# ============================================================\n",
    "# VALIDATE AND LOAD DATA\n",
    "# ============================================================\n",
    "if RAW_DATA_FILE:\n",
    "    size_mb = RAW_DATA_FILE.stat().st_size / 1e6\n",
    "    print(f\"\\n  Selected: {RAW_DATA_FILE.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Load and validate\n",
    "    print(\"  Loading data...\")\n",
    "    if RAW_DATA_FILE.suffix == '.parquet':\n",
    "        df_raw = pd.read_parquet(RAW_DATA_FILE)\n",
    "    else:\n",
    "        df_raw = pd.read_csv(RAW_DATA_FILE)\n",
    "    \n",
    "    print(f\"  Rows: {len(df_raw):,}\")\n",
    "    print(f\"  Columns: {list(df_raw.columns)}\")\n",
    "    \n",
    "    # Validate OHLCV columns (case-insensitive)\n",
    "    required = {'open', 'high', 'low', 'close', 'volume'}\n",
    "    found = {c.lower() for c in df_raw.columns}\n",
    "    if required.issubset(found):\n",
    "        print(\"  OHLCV columns: \u2713 OK\")\n",
    "    else:\n",
    "        missing = required - found\n",
    "        print(f\"  [ERROR] Missing columns: {missing}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # AUTO-DETECT DATE RANGE FROM DATA\n",
    "    # ============================================================\n",
    "    date_col = None\n",
    "    for c in df_raw.columns:\n",
    "        if 'date' in c.lower() or 'time' in c.lower():\n",
    "            date_col = c\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        df_raw[date_col] = pd.to_datetime(df_raw[date_col])\n",
    "        \n",
    "        # Store globally for pipeline use\n",
    "        DATA_START = df_raw[date_col].min()\n",
    "        DATA_END = df_raw[date_col].max()\n",
    "        DATA_START_YEAR = DATA_START.year\n",
    "        DATA_END_YEAR = DATA_END.year\n",
    "        \n",
    "        print(f\"\\n  [DATE RANGE DETECTED]\")\n",
    "        print(f\"  Start: {DATA_START.strftime('%Y-%m-%d %H:%M')} ({DATA_START_YEAR})\")\n",
    "        print(f\"  End:   {DATA_END.strftime('%Y-%m-%d %H:%M')} ({DATA_END_YEAR})\")\n",
    "        print(f\"  Span:  {(DATA_END - DATA_START).days:,} days ({DATA_END_YEAR - DATA_START_YEAR + 1} years)\")\n",
    "        \n",
    "        # Validate against configured date range\n",
    "        if YEAR_START and YEAR_END:\n",
    "            if DATA_START_YEAR <= YEAR_START and DATA_END_YEAR >= YEAR_END:\n",
    "                print(f\"  Config Match: \u2713 Data covers {DATE_RANGE}\")\n",
    "            else:\n",
    "                print(f\"  [WARNING] Data range ({DATA_START_YEAR}-{DATA_END_YEAR}) differs from config ({DATE_RANGE})\")\n",
    "    else:\n",
    "        print(\"  [WARNING] No datetime column found - using index\")\n",
    "        DATA_START = None\n",
    "        DATA_END = None\n",
    "        DATA_START_YEAR = YEAR_START or 2019\n",
    "        DATA_END_YEAR = YEAR_END or 2024\n",
    "    \n",
    "    del df_raw\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\n  \u2713 Data verified and ready for processing!\")\n",
    "else:\n",
    "    print(f\"\\n  [ERROR] No data file found for {SYMBOL}!\")\n",
    "    print(f\"  Expected location: {RAW_DATA_DIR}\")\n",
    "    print(f\"\\n  Tried patterns matching '{SYMBOL}' (case-insensitive)\")\n",
    "    print(f\"\\n  Available files in directory:\")\n",
    "    if RAW_DATA_DIR.exists():\n",
    "        for f in sorted(RAW_DATA_DIR.iterdir()):\n",
    "            if f.suffix in ['.csv', '.parquet']:\n",
    "                print(f\"    - {f.name}\")\n",
    "    else:\n",
    "        print(f\"    Directory does not exist!\")\n",
    "    \n",
    "    print(f\"\\n  [FIX] Set CUSTOM_DATA_FILE in Section 1 to your exact filename\")\n",
    "    \n",
    "    RAW_DATA_FILE = None\n",
    "    DATA_START = None\n",
    "    DATA_END = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 Run Data Pipeline { display-mode: \"form\" }\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import gc\n",
    "\n",
    "import time\n",
    "\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "\n",
    "if 'IS_COLAB' not in dir():\n",
    "\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "\n",
    "if 'RUN_DATA_PIPELINE' not in dir():\n",
    "\n",
    "    RUN_DATA_PIPELINE = True\n",
    "\n",
    "if 'RAW_DATA_FILE' not in dir():\n",
    "\n",
    "    RAW_DATA_FILE = None\n",
    "\n",
    "\n",
    "\n",
    "if not RUN_DATA_PIPELINE:\n",
    "\n",
    "    print(\"[Skipped] Data pipeline disabled in configuration.\")\n",
    "\n",
    "    print(\"Set RUN_DATA_PIPELINE = True in Section 1 to enable.\")\n",
    "\n",
    "elif RAW_DATA_FILE is None:\n",
    "\n",
    "    print(\"[Error] No raw data file found. Cannot run pipeline.\")\n",
    "\n",
    "    print(\"Run Section 3.1 first to detect data files.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\" PHASE 1: DATA PIPELINE\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(f\"\\n  Symbol: {SYMBOL}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # ============================================================\n",
    "\n",
    "    # COPY DATA FROM DRIVE TO PROJECT (Colab only)\n",
    "\n",
    "    # ============================================================\n",
    "\n",
    "    if IS_COLAB and RAW_DATA_FILE is not None:\n",
    "\n",
    "        # The pipeline expects data in PROJECT_ROOT/data/raw/\n",
    "\n",
    "        project_raw_dir = PROJECT_ROOT / 'data/raw'\n",
    "\n",
    "        project_raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        \n",
    "\n",
    "        # Create standardized filename for pipeline: SYMBOL_1m.parquet\n",
    "\n",
    "        target_filename = f\"{SYMBOL}_1m{RAW_DATA_FILE.suffix}\"\n",
    "\n",
    "        target_path = project_raw_dir / target_filename\n",
    "\n",
    "        \n",
    "\n",
    "        # Only copy if source is not already in project directory\n",
    "\n",
    "        source_in_project = str(RAW_DATA_FILE).startswith(str(PROJECT_ROOT))\n",
    "\n",
    "        if not source_in_project:\n",
    "\n",
    "            if not target_path.exists() or target_path.stat().st_size != RAW_DATA_FILE.stat().st_size:\n",
    "\n",
    "                print(f\"\\n  [Setup] Copying data from Drive to project...\")\n",
    "\n",
    "                print(f\"    From: {RAW_DATA_FILE}\")\n",
    "\n",
    "                print(f\"    To:   {target_path}\")\n",
    "\n",
    "                shutil.copy2(RAW_DATA_FILE, target_path)\n",
    "\n",
    "                print(f\"    Done! ({target_path.stat().st_size / 1e6:.1f} MB)\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"\\n  [Setup] Data already in project: {target_path.name}\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"\\n  [Setup] Data already in project directory\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Use auto-detected date range\n",
    "\n",
    "    if 'DATA_START' in dir() and DATA_START is not None:\n",
    "\n",
    "        print(f\"  Date Range: {DATA_START.strftime('%Y-%m-%d')} to {DATA_END.strftime('%Y-%m-%d')} (auto-detected)\")\n",
    "\n",
    "        start_date_str = DATA_START.strftime('%Y-%m-%d')\n",
    "\n",
    "        end_date_str = DATA_END.strftime('%Y-%m-%d')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"  Date Range: Full dataset (no filter)\")\n",
    "\n",
    "        start_date_str = None\n",
    "\n",
    "        end_date_str = None\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"  Horizons: {HORIZON_LIST}\")\n",
    "\n",
    "    print(f\"  Purge: {PURGE_BARS} bars, Embargo: {EMBARGO_BARS} bars\")\n",
    "\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "\n",
    "        from src.phase1.pipeline_config import PipelineConfig\n",
    "\n",
    "        from src.pipeline.runner import PipelineRunner\n",
    "\n",
    "        \n",
    "\n",
    "        # Configure pipeline with auto-detected dates\n",
    "\n",
    "        # NOTE: auto_scale_purge_embargo=False uses our explicit PURGE_BARS/EMBARGO_BARS\n",
    "\n",
    "        config = PipelineConfig(\n",
    "\n",
    "            symbols=[SYMBOL],\n",
    "\n",
    "            project_root=PROJECT_ROOT,\n",
    "\n",
    "            label_horizons=HORIZON_LIST,\n",
    "\n",
    "            train_ratio=TRAIN_RATIO,\n",
    "\n",
    "            val_ratio=VAL_RATIO,\n",
    "\n",
    "            test_ratio=TEST_RATIO,\n",
    "\n",
    "            purge_bars=PURGE_BARS,\n",
    "\n",
    "            embargo_bars=EMBARGO_BARS,\n",
    "\n",
    "            start_date=start_date_str,\n",
    "\n",
    "            end_date=end_date_str,\n",
    "\n",
    "            allow_batch_symbols=False,  # Single-contract architecture\n",
    "\n",
    "            auto_scale_purge_embargo=False,  # Use explicit purge/embargo values\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # Run pipeline\n",
    "\n",
    "        runner = PipelineRunner(config)\n",
    "\n",
    "        success = runner.run()\n",
    "\n",
    "        \n",
    "        # Copy scaled data to global location for training cells\n",
    "        if success:\n",
    "            run_scaled_dir = config.splits_dir / \"scaled\"\n",
    "            global_scaled_dir = PROJECT_ROOT / \"data\" / \"splits\" / \"scaled\"\n",
    "            \n",
    "            if run_scaled_dir.exists():\n",
    "                print(f\"\\n  Copying scaled data to global location...\")\n",
    "                global_scaled_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                import shutil\n",
    "                for f in run_scaled_dir.glob(\"*.parquet\"):\n",
    "                    shutil.copy2(f, global_scaled_dir / f.name)\n",
    "                    print(f\"    Copied: {f.name}\")\n",
    "                \n",
    "                # Also copy scaler and metadata\n",
    "                for f in run_scaled_dir.glob(\"*.pkl\"):\n",
    "                    shutil.copy2(f, global_scaled_dir / f.name)\n",
    "                for f in run_scaled_dir.glob(\"*.json\"):\n",
    "                    shutil.copy2(f, global_scaled_dir / f.name)\n",
    "                \n",
    "                print(f\"  \u2713 Scaled data available at: {global_scaled_dir}\")\n",
    "                \n",
    "                # Update SPLITS_DIR for subsequent cells\n",
    "                SPLITS_DIR = global_scaled_dir\n",
    "\n",
    "        \n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        \n",
    "\n",
    "        if success:\n",
    "\n",
    "            print(f\"\\n  Pipeline completed in {elapsed/60:.1f} minutes\")\n",
    "\n",
    "            \n",
    "\n",
    "            # Verify output\n",
    "\n",
    "            if (SPLITS_DIR / 'train_scaled.parquet').exists():\n",
    "\n",
    "                for split in ['train', 'val', 'test']:\n",
    "\n",
    "                    df = pd.read_parquet(SPLITS_DIR / f'{split}_scaled.parquet')\n",
    "\n",
    "                    print(f\"  {split}: {len(df):,} samples\")\n",
    "\n",
    "                    del df\n",
    "\n",
    "                gc.collect()\n",
    "\n",
    "                print(\"\\n  Data ready for training!\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(\"\\n  [ERROR] Pipeline failed. Check logs above.\")\n",
    "\n",
    "        \n",
    "\n",
    "        del runner, config\n",
    "\n",
    "        if 'clear_memory' in dir():\n",
    "\n",
    "            clear_memory()\n",
    "\n",
    "        else:\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"\\n  [ERROR] Pipeline failed: {e}\")\n",
    "\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.3 Verify Processed Data { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined (in case cells run out of order)\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" PROCESSED DATA VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for pre-processed data (local) or pipeline output (Colab)\n",
    "if not IS_COLAB:\n",
    "    # Local: check pre-processed data\n",
    "    local_splits = PROJECT_ROOT / 'data/splits/final_correct/scaled'\n",
    "    if (local_splits / 'train_scaled.parquet').exists():\n",
    "        SPLITS_DIR = local_splits\n",
    "        print(f\"\\nUsing pre-processed data: {SPLITS_DIR}\")\n",
    "\n",
    "if (SPLITS_DIR / 'train_scaled.parquet').exists():\n",
    "    # Load metadata without keeping DataFrames\n",
    "    train_df = pd.read_parquet(SPLITS_DIR / 'train_scaled.parquet')\n",
    "    \n",
    "    FEATURE_COLS = [c for c in train_df.columns \n",
    "                   if not c.startswith(('label_', 'sample_weight', 'quality_', 'datetime', 'symbol'))]\n",
    "    LABEL_COLS = [c for c in train_df.columns if c.startswith('label_')]\n",
    "    TRAIN_LEN = len(train_df)\n",
    "    \n",
    "    # Label distribution with safety check\n",
    "    label_dists = {}\n",
    "    for col in LABEL_COLS:\n",
    "        label_dists[col] = train_df[col].value_counts().sort_index().to_dict()\n",
    "    \n",
    "    del train_df\n",
    "    \n",
    "    # Get val/test sizes\n",
    "    val_df = pd.read_parquet(SPLITS_DIR / 'val_scaled.parquet')\n",
    "    VAL_LEN = len(val_df)\n",
    "    del val_df\n",
    "    \n",
    "    test_df = pd.read_parquet(SPLITS_DIR / 'test_scaled.parquet')\n",
    "    TEST_LEN = len(test_df)\n",
    "    del test_df\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"  Train: {TRAIN_LEN:,} samples\")\n",
    "    print(f\"  Val:   {VAL_LEN:,} samples\")\n",
    "    print(f\"  Test:  {TEST_LEN:,} samples\")\n",
    "    print(f\"  Total: {TRAIN_LEN + VAL_LEN + TEST_LEN:,} samples\")\n",
    "    print(f\"\\n  Features: {len(FEATURE_COLS)}\")\n",
    "    print(f\"  Labels: {LABEL_COLS}\")\n",
    "    \n",
    "    print(f\"\\nLabel Distribution (train):\")\n",
    "    for col, dist in label_dists.items():\n",
    "        total = sum(dist.values())\n",
    "        if total == 0:\n",
    "            print(f\"  {col}: No valid samples!\")\n",
    "            continue\n",
    "        long_pct = dist.get(1, 0) / total * 100\n",
    "        neutral_pct = dist.get(0, 0) / total * 100\n",
    "        short_pct = dist.get(-1, 0) / total * 100\n",
    "        print(f\"  {col}: Long={long_pct:.1f}% | Neutral={neutral_pct:.1f}% | Short={short_pct:.1f}%\")\n",
    "    \n",
    "    # Validate TRAINING_HORIZON is in available labels\n",
    "    if 'TRAINING_HORIZON' in dir() and 'HORIZON_LIST' in dir():\n",
    "        if TRAINING_HORIZON not in HORIZON_LIST:\n",
    "            print(f\"\\n  [WARNING] TRAINING_HORIZON={TRAINING_HORIZON} not in HORIZON_LIST={HORIZON_LIST}\")\n",
    "            print(f\"  Model training may fail. Update TRAINING_HORIZON in Section 1.\")\n",
    "    \n",
    "    DATA_READY = True\n",
    "    print(\"\\n  Data verified and ready for training!\")\n",
    "else:\n",
    "    print(\"\\n[ERROR] Processed data not found!\")\n",
    "    print(f\"  Expected: {SPLITS_DIR}/train_scaled.parquet\")\n",
    "    print(\"  Run Section 3.2 to process raw data.\")\n",
    "    DATA_READY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. PHASE 2: MODEL TRAINING\n",
    "\n",
    "Train selected models on the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Train Models { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'RUN_MODEL_TRAINING' not in dir():\n",
    "    RUN_MODEL_TRAINING = True\n",
    "if 'DATA_READY' not in dir():\n",
    "    DATA_READY = (SPLITS_DIR / 'train_scaled.parquet').exists()\n",
    "if 'MODELS_TO_TRAIN' not in dir():\n",
    "    MODELS_TO_TRAIN = ['xgboost', 'lightgbm', 'catboost']\n",
    "if 'HORIZON_LIST' not in dir():\n",
    "    HORIZON_LIST = [5, 10, 15, 20]\n",
    "if 'USE_CLASS_WEIGHTS' not in dir():\n",
    "    USE_CLASS_WEIGHTS = True\n",
    "if 'USE_SAMPLE_WEIGHTS' not in dir():\n",
    "    USE_SAMPLE_WEIGHTS = True\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "# Define clear_memory if not available\n",
    "if 'clear_memory' not in dir():\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        if GPU_AVAILABLE:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Validate training horizon before starting\n",
    "horizon_valid = True\n",
    "if 'TRAINING_HORIZON' in dir() and TRAINING_HORIZON not in HORIZON_LIST:\n",
    "    print(f\"[ERROR] TRAINING_HORIZON={TRAINING_HORIZON} not in processed horizons {HORIZON_LIST}\")\n",
    "    print(f\"  Update TRAINING_HORIZON in Section 1 to one of: {HORIZON_LIST}\")\n",
    "    horizon_valid = False\n",
    "\n",
    "if not RUN_MODEL_TRAINING:\n",
    "    print(\"[Skipped] Model training disabled in configuration.\")\n",
    "elif not DATA_READY:\n",
    "    print(\"[Error] Data not ready. Run Section 3 first.\")\n",
    "elif not MODELS_TO_TRAIN:\n",
    "    print(\"[Error] No models selected. Enable models in Section 1.\")\n",
    "elif not horizon_valid:\n",
    "    print(\"[Error] Invalid training horizon. See error above.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 2: MODEL TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n  Models: {MODELS_TO_TRAIN}\")\n",
    "    print(f\"  Horizon: H{TRAINING_HORIZON}\")\n",
    "    \n",
    "    # Initialize results dict before training loop\n",
    "    TRAINING_RESULTS = {}\n",
    "    \n",
    "    try:\n",
    "        from src.models import ModelRegistry, Trainer, TrainerConfig\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        # Load data container\n",
    "        print(\"\\nLoading data...\")\n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        print(f\"  Train: {container.splits['train'].n_samples:,}\")\n",
    "        print(f\"  Val: {container.splits['val'].n_samples:,}\")\n",
    "        \n",
    "        \n",
    "        # Create wrapper to control sample weight usage\n",
    "        class ContainerWrapper:\n",
    "            '''Wrapper to optionally disable sample weights.'''\n",
    "            def __init__(self, container, use_weights=True):\n",
    "                self._container = container\n",
    "                self._use_weights = use_weights\n",
    "            \n",
    "            def __getattr__(self, name):\n",
    "                return getattr(self._container, name)\n",
    "            \n",
    "            def get_sklearn_arrays(self, split, return_df=False):\n",
    "                X, y, w = self._container.get_sklearn_arrays(split, return_df)\n",
    "                if not self._use_weights:\n",
    "                    w = None if not return_df else pd.Series(np.ones(len(y)), index=y.index)\n",
    "                return X, y, w\n",
    "            \n",
    "            def get_pytorch_sequences(self, *args, **kwargs):\n",
    "                # Neural models use sequences; sample weights handled differently\n",
    "                return self._container.get_pytorch_sequences(*args, **kwargs)\n",
    "        \n",
    "        # Wrap container if sample weights are disabled\n",
    "        if not USE_SAMPLE_WEIGHTS:\n",
    "            container = ContainerWrapper(container, use_weights=False)\n",
    "            print(\"  Note: Sample weights disabled for training\")\n",
    "        \n",
    "        # Class balance configuration\n",
    "        print(f\"\\n  Class Weights:   {'Enabled' if USE_CLASS_WEIGHTS else 'Disabled'}\")\n",
    "        print(f\"  Sample Weights:  {'Enabled' if USE_SAMPLE_WEIGHTS else 'Disabled'}\")\n",
    "        \n",
    "        # Get training data for class distribution\n",
    "        _, y_train, w_train = container.get_sklearn_arrays('train')\n",
    "        from collections import Counter\n",
    "        class_counts = Counter(y_train)\n",
    "        total_samples = len(y_train)\n",
    "        \n",
    "        print(\"\\n  Class Distribution:\")\n",
    "        class_names = {-1: 'Short', 0: 'Neutral', 1: 'Long'}\n",
    "        for cls in sorted(class_counts.keys()):\n",
    "            count = class_counts[cls]\n",
    "            pct = 100 * count / total_samples\n",
    "            print(f\"    {class_names.get(cls, cls):8s}: {count:7,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Calculate class weights if enabled\n",
    "        class_weights = None\n",
    "        if USE_CLASS_WEIGHTS:\n",
    "            n_classes = len(class_counts)\n",
    "            class_weights = {cls: total_samples / (n_classes * count) for cls, count in class_counts.items()}\n",
    "            print(\"\\n  Computed Class Weights:\")\n",
    "            for cls in sorted(class_weights.keys()):\n",
    "                print(f\"    {class_names.get(cls, cls):8s}: {class_weights[cls]:.4f}\")\n",
    "        \n",
    "        # Sample weights info\n",
    "        if USE_SAMPLE_WEIGHTS and w_train is not None:\n",
    "            print(f\"\\n  Sample Weights: min={w_train.min():.2f}, max={w_train.max():.2f}, mean={w_train.mean():.2f}\")\n",
    "        elif USE_SAMPLE_WEIGHTS:\n",
    "            print(\"\\n  Sample Weights: Not available in data, using uniform weights\")\n",
    "        \n",
    "        # Determine class weight setting for sklearn models\n",
    "        sklearn_class_weight = 'balanced' if USE_CLASS_WEIGHTS else None\n",
    "        \n",
    "        # Control sample weight usage\n",
    "        use_sample_weights = USE_SAMPLE_WEIGHTS\n",
    "        \n",
    "        # Train each model with per-model error handling\n",
    "        for i, model_name in enumerate(MODELS_TO_TRAIN, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\" [{i}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            clear_memory()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Configure model\n",
    "                if model_name in ['lstm', 'gru', 'tcn']:\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        sequence_length=SEQUENCE_LENGTH,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        max_epochs=MAX_EPOCHS,\n",
    "                        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n",
    "                    )\n",
    "                elif model_name == 'catboost':\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        model_config={\n",
    "                            \"iterations\": N_ESTIMATORS,\n",
    "                            \"early_stopping_rounds\": BOOSTING_EARLY_STOPPING,\n",
    "                            \"use_gpu\": False,\n",
    "                            \"task_type\": \"CPU\",\n",
    "                            \"verbose\": False,\n",
    "                        },\n",
    "                    )\n",
    "                elif model_name in ['random_forest', 'logistic', 'svm']:\n",
    "                    # Classical sklearn models with class_weight support\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        model_config={\n",
    "                            'class_weight': sklearn_class_weight,\n",
    "                        },\n",
    "                    )\n",
    "                else:\n",
    "                    # Boosting models (xgboost, lightgbm) and others\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        model_config={\n",
    "                            'n_estimators': N_ESTIMATORS,\n",
    "                            'early_stopping_rounds': BOOSTING_EARLY_STOPPING,\n",
    "                        } if model_name in ['xgboost', 'lightgbm'] else {},\n",
    "                    )\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(config)\n",
    "                results = trainer.run(container)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Store results\n",
    "                metrics = results.get('evaluation_metrics', {})\n",
    "                TRAINING_RESULTS[model_name] = {\n",
    "                    'metrics': metrics,\n",
    "                    'time': elapsed,\n",
    "                    'run_id': results.get('run_id', 'unknown'),\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n  Accuracy: {metrics.get('accuracy', 0):.2%}\")\n",
    "                print(f\"  Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n",
    "                print(f\"  Time: {elapsed:.1f}s\")\n",
    "                \n",
    "                del trainer, config\n",
    "                \n",
    "            except Exception as model_error:\n",
    "                # Per-model error handling - continue to next model\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"\\n  [ERROR] {model_name} training failed: {model_error}\")\n",
    "                TRAINING_RESULTS[model_name] = {\n",
    "                    'metrics': {},\n",
    "                    'time': elapsed,\n",
    "                    'run_id': 'failed',\n",
    "                    'error': str(model_error),\n",
    "                }\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            clear_memory()\n",
    "        \n",
    "        # Save results\n",
    "        results_file = EXPERIMENTS_DIR / 'training_results.json'\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(TRAINING_RESULTS, f, indent=2)\n",
    "        \n",
    "        # Summary\n",
    "        successful = [m for m, r in TRAINING_RESULTS.items() if r.get('run_id') != 'failed']\n",
    "        failed = [m for m, r in TRAINING_RESULTS.items() if r.get('run_id') == 'failed']\n",
    "        print(f\"\\n  Completed: {len(successful)}/{len(MODELS_TO_TRAIN)} models\")\n",
    "        if failed:\n",
    "            print(f\"  Failed: {failed}\")\n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "        \n",
    "        del container\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Training setup failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Compare Models { display-mode: \"form\" }\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure TRAINING_RESULTS is defined\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Check if training results are populated\n",
    "if not TRAINING_RESULTS:\n",
    "    print(\"[WARNING] No trained models found in TRAINING_RESULTS.\")\n",
    "    print(\"Please run Section 4.1 (Model Training) first.\")\n",
    "elif TRAINING_RESULTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" MODEL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Build comparison table\n",
    "    rows = []\n",
    "    for model, data in TRAINING_RESULTS.items():\n",
    "        metrics = data.get('metrics', {})\n",
    "        rows.append({\n",
    "            'Model': model,\n",
    "            'Accuracy': metrics.get('accuracy', 0),\n",
    "            'Macro F1': metrics.get('macro_f1', 0),\n",
    "            'Weighted F1': metrics.get('weighted_f1', 0),\n",
    "            'Time (s)': data.get('time', 0),\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(rows)\n",
    "    comparison_df = comparison_df.sort_values('Macro F1', ascending=False)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Best model\n",
    "    best_model = comparison_df.iloc[0]['Model']\n",
    "    best_f1 = comparison_df.iloc[0]['Macro F1']\n",
    "    print(f\"\\n  Best Model: {best_model} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # Visualization\n",
    "    if len(TRAINING_RESULTS) > 1:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        sorted_df = comparison_df.sort_values('Accuracy', ascending=True)\n",
    "        axes[0].barh(sorted_df['Model'], sorted_df['Accuracy'], color='steelblue')\n",
    "        axes[0].set_xlabel('Accuracy')\n",
    "        axes[0].set_title('Model Accuracy')\n",
    "        axes[0].set_xlim(0, 1)\n",
    "        \n",
    "        # Training time\n",
    "        sorted_df = comparison_df.sort_values('Time (s)', ascending=True)\n",
    "        axes[1].barh(sorted_df['Model'], sorted_df['Time (s)'], color='coral')\n",
    "        axes[1].set_xlabel('Training Time (seconds)')\n",
    "        axes[1].set_title('Training Time')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 4.3 Visualize Training Results { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "\n",
    "# Visualization toggles\n",
    "show_confusion_matrix = True  #@param {type: \"boolean\"}\n",
    "show_feature_importance = True  #@param {type: \"boolean\"}\n",
    "show_learning_curves = True  #@param {type: \"boolean\"}\n",
    "show_prediction_dist = True  #@param {type: \"boolean\"}\n",
    "show_per_class_metrics = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Check if we have training results\n",
    "if not TRAINING_RESULTS:\n",
    "    print(\"No training results available.\")\n",
    "    print(\"Run Section 4.1 to train models first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" TRAINING VISUALIZATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Filter successful models only\n",
    "    successful_models = {\n",
    "        name: data for name, data in TRAINING_RESULTS.items()\n",
    "        if data.get('run_id') != 'failed' and data.get('metrics')\n",
    "    }\n",
    "    \n",
    "    if not successful_models:\n",
    "        print(\"\\nNo successful models to visualize.\")\n",
    "        print(\"All models failed during training.\")\n",
    "    else:\n",
    "        print(f\"\\nVisualizing {len(successful_models)} models: {list(successful_models.keys())}\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 1. CONFUSION MATRICES\n",
    "        # ============================================================\n",
    "        if show_confusion_matrix:\n",
    "            print(\"\\n[1/5] Generating confusion matrices...\")\n",
    "            \n",
    "            n_models = len(successful_models)\n",
    "            n_cols = min(3, n_models)\n",
    "            n_rows = (n_models + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "            if n_models == 1:\n",
    "                axes = np.array([axes])\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for idx, (model_name, data) in enumerate(successful_models.items()):\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_true = np.array(pred_data.get('y_true', []))\n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_true) > 0 and len(y_pred) > 0:\n",
    "                        cm = confusion_matrix(y_true, y_pred, labels=[-1, 0, 1])\n",
    "                        disp = ConfusionMatrixDisplay(\n",
    "                            confusion_matrix=cm,\n",
    "                            display_labels=['Short', 'Neutral', 'Long']\n",
    "                        )\n",
    "                        disp.plot(ax=axes[idx], cmap='Blues', values_format='d')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}', fontweight='bold')\n",
    "                    else:\n",
    "                        axes[idx].text(0.5, 0.5, 'No predictions available',\n",
    "                                     ha='center', va='center')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}')\n",
    "                else:\n",
    "                    axes[idx].text(0.5, 0.5, 'Predictions not found',\n",
    "                                 ha='center', va='center')\n",
    "                    axes[idx].set_title(f'{model_name.upper()}')\n",
    "            \n",
    "            # Hide extra subplots\n",
    "            for idx in range(n_models, len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # ============================================================\n",
    "        # 2. FEATURE IMPORTANCE (Top 20)\n",
    "        # ============================================================\n",
    "        if show_feature_importance:\n",
    "            print(\"\\n[2/5] Generating feature importance plots...\")\n",
    "            \n",
    "            # Models that support feature importance\n",
    "            boosting_models = ['xgboost', 'lightgbm', 'catboost']\n",
    "            classical_models = ['random_forest']\n",
    "            \n",
    "            fi_models = {\n",
    "                name: data for name, data in successful_models.items()\n",
    "                if name in boosting_models + classical_models\n",
    "            }\n",
    "            \n",
    "            if fi_models:\n",
    "                n_models = len(fi_models)\n",
    "                n_cols = min(2, n_models)\n",
    "                n_rows = (n_models + n_cols - 1) // n_cols\n",
    "                \n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(10*n_cols, 6*n_rows))\n",
    "                if n_models == 1:\n",
    "                    axes = np.array([axes])\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for idx, (model_name, data) in enumerate(fi_models.items()):\n",
    "                    run_id = data.get('run_id', 'unknown')\n",
    "                    fi_file = EXPERIMENTS_DIR / run_id / 'feature_importance.json'\n",
    "                    \n",
    "                    if fi_file.exists():\n",
    "                        with open(fi_file, 'r') as f:\n",
    "                            fi_data = json.load(f)\n",
    "                        \n",
    "                        # Convert to DataFrame and sort\n",
    "                        fi_df = pd.DataFrame(list(fi_data.items()),\n",
    "                                            columns=['feature', 'importance'])\n",
    "                        fi_df = fi_df.sort_values('importance', ascending=False).head(20)\n",
    "                        \n",
    "                        # Plot\n",
    "                        axes[idx].barh(range(len(fi_df)), fi_df['importance'], color='steelblue')\n",
    "                        axes[idx].set_yticks(range(len(fi_df)))\n",
    "                        axes[idx].set_yticklabels(fi_df['feature'], fontsize=8)\n",
    "                        axes[idx].invert_yaxis()\n",
    "                        axes[idx].set_xlabel('Importance', fontweight='bold')\n",
    "                        axes[idx].set_title(f'{model_name.upper()} - Top 20 Features',\n",
    "                                          fontweight='bold')\n",
    "                        axes[idx].grid(axis='x', alpha=0.3)\n",
    "                    else:\n",
    "                        axes[idx].text(0.5, 0.5, 'Feature importance not available',\n",
    "                                     ha='center', va='center')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}')\n",
    "                \n",
    "                # Hide extra subplots\n",
    "                for idx in range(n_models, len(axes)):\n",
    "                    axes[idx].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No models with feature importance (boosting/classical models only)\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 3. LEARNING CURVES (Neural Models)\n",
    "        # ============================================================\n",
    "        if show_learning_curves:\n",
    "            print(\"\\n[3/5] Generating learning curves...\")\n",
    "            \n",
    "            # Neural models that have training history\n",
    "            neural_models = ['lstm', 'gru', 'tcn', 'transformer']\n",
    "            \n",
    "            lc_models = {\n",
    "                name: data for name, data in successful_models.items()\n",
    "                if name in neural_models\n",
    "            }\n",
    "            \n",
    "            if lc_models:\n",
    "                n_models = len(lc_models)\n",
    "                \n",
    "                fig, axes = plt.subplots(n_models, 2, figsize=(12, 4*n_models))\n",
    "                if n_models == 1:\n",
    "                    axes = axes.reshape(1, -1)\n",
    "                \n",
    "                for idx, (model_name, data) in enumerate(lc_models.items()):\n",
    "                    run_id = data.get('run_id', 'unknown')\n",
    "                    history_file = EXPERIMENTS_DIR / run_id / 'training_history.json'\n",
    "                    \n",
    "                    if history_file.exists():\n",
    "                        with open(history_file, 'r') as f:\n",
    "                            history = json.load(f)\n",
    "                        \n",
    "                        epochs = range(1, len(history.get('train_loss', [])) + 1)\n",
    "                        \n",
    "                        # Loss plot\n",
    "                        axes[idx, 0].plot(epochs, history.get('train_loss', []),\n",
    "                                        label='Train', linewidth=2, color='steelblue')\n",
    "                        axes[idx, 0].plot(epochs, history.get('val_loss', []),\n",
    "                                        label='Val', linewidth=2, color='coral')\n",
    "                        axes[idx, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "                        axes[idx, 0].set_ylabel('Loss', fontweight='bold')\n",
    "                        axes[idx, 0].set_title(f'{model_name.upper()} - Loss',\n",
    "                                              fontweight='bold')\n",
    "                        axes[idx, 0].legend()\n",
    "                        axes[idx, 0].grid(alpha=0.3)\n",
    "                        \n",
    "                        # Accuracy plot\n",
    "                        axes[idx, 1].plot(epochs, history.get('train_acc', []),\n",
    "                                        label='Train', linewidth=2, color='steelblue')\n",
    "                        axes[idx, 1].plot(epochs, history.get('val_acc', []),\n",
    "                                        label='Val', linewidth=2, color='coral')\n",
    "                        axes[idx, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "                        axes[idx, 1].set_ylabel('Accuracy', fontweight='bold')\n",
    "                        axes[idx, 1].set_title(f'{model_name.upper()} - Accuracy',\n",
    "                                              fontweight='bold')\n",
    "                        axes[idx, 1].legend()\n",
    "                        axes[idx, 1].grid(alpha=0.3)\n",
    "                    else:\n",
    "                        for col in [0, 1]:\n",
    "                            axes[idx, col].text(0.5, 0.5, 'History not available',\n",
    "                                              ha='center', va='center')\n",
    "                            axes[idx, col].set_title(f'{model_name.upper()}')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No neural models with training history\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 4. PREDICTION DISTRIBUTION\n",
    "        # ============================================================\n",
    "        if show_prediction_dist:\n",
    "            print(\"\\n[4/5] Generating prediction distribution...\")\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Prepare data for stacked bar chart\n",
    "            model_names = []\n",
    "            long_counts = []\n",
    "            neutral_counts = []\n",
    "            short_counts = []\n",
    "            \n",
    "            for model_name, data in successful_models.items():\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_pred) > 0:\n",
    "                        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "                        count_dict = dict(zip(unique, counts))\n",
    "                        total = len(y_pred)\n",
    "                        \n",
    "                        model_names.append(model_name)\n",
    "                        short_counts.append(count_dict.get(-1, 0) / total * 100)\n",
    "                        neutral_counts.append(count_dict.get(0, 0) / total * 100)\n",
    "                        long_counts.append(count_dict.get(1, 0) / total * 100)\n",
    "            \n",
    "            if model_names:\n",
    "                x = np.arange(len(model_names))\n",
    "                width = 0.6\n",
    "                \n",
    "                p1 = ax.bar(x, short_counts, width, label='Short (-1)', color='#d62728')\n",
    "                p2 = ax.bar(x, neutral_counts, width, bottom=short_counts,\n",
    "                           label='Neutral (0)', color='#7f7f7f')\n",
    "                p3 = ax.bar(x, long_counts, width,\n",
    "                           bottom=np.array(short_counts) + np.array(neutral_counts),\n",
    "                           label='Long (1)', color='#2ca02c')\n",
    "                \n",
    "                ax.set_ylabel('Percentage (%)', fontweight='bold')\n",
    "                ax.set_title('Prediction Distribution by Model', fontweight='bold', fontsize=14)\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                ax.legend(loc='upper right')\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                # Add percentage labels\n",
    "                for i, (s, n, l) in enumerate(zip(short_counts, neutral_counts, long_counts)):\n",
    "                    if s > 5:\n",
    "                        ax.text(i, s/2, f'{s:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                    if n > 5:\n",
    "                        ax.text(i, s + n/2, f'{n:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                    if l > 5:\n",
    "                        ax.text(i, s + n + l/2, f'{l:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No predictions available for distribution plot\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 5. PER-CLASS METRICS\n",
    "        # ============================================================\n",
    "        if show_per_class_metrics:\n",
    "            print(\"\\n[5/5] Generating per-class metrics...\")\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            model_names = []\n",
    "            short_precision = []\n",
    "            neutral_precision = []\n",
    "            long_precision = []\n",
    "            short_recall = []\n",
    "            neutral_recall = []\n",
    "            long_recall = []\n",
    "            short_f1 = []\n",
    "            neutral_f1 = []\n",
    "            long_f1 = []\n",
    "            \n",
    "            for model_name, data in successful_models.items():\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_true = np.array(pred_data.get('y_true', []))\n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_true) > 0 and len(y_pred) > 0:\n",
    "                        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                            y_true, y_pred, labels=[-1, 0, 1], average=None, zero_division=0\n",
    "                        )\n",
    "                        \n",
    "                        model_names.append(model_name)\n",
    "                        short_precision.append(precision[0])\n",
    "                        neutral_precision.append(precision[1])\n",
    "                        long_precision.append(precision[2])\n",
    "                        short_recall.append(recall[0])\n",
    "                        neutral_recall.append(recall[1])\n",
    "                        long_recall.append(recall[2])\n",
    "                        short_f1.append(f1[0])\n",
    "                        neutral_f1.append(f1[1])\n",
    "                        long_f1.append(f1[2])\n",
    "            \n",
    "            if model_names:\n",
    "                x = np.arange(len(model_names))\n",
    "                width = 0.25\n",
    "                \n",
    "                # Precision\n",
    "                axes[0].bar(x - width, short_precision, width, label='Short', color='#d62728')\n",
    "                axes[0].bar(x, neutral_precision, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[0].bar(x + width, long_precision, width, label='Long', color='#2ca02c')\n",
    "                axes[0].set_ylabel('Precision', fontweight='bold')\n",
    "                axes[0].set_title('Precision by Class', fontweight='bold')\n",
    "                axes[0].set_xticks(x)\n",
    "                axes[0].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(axis='y', alpha=0.3)\n",
    "                axes[0].set_ylim(0, 1)\n",
    "                \n",
    "                # Recall\n",
    "                axes[1].bar(x - width, short_recall, width, label='Short', color='#d62728')\n",
    "                axes[1].bar(x, neutral_recall, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[1].bar(x + width, long_recall, width, label='Long', color='#2ca02c')\n",
    "                axes[1].set_ylabel('Recall', fontweight='bold')\n",
    "                axes[1].set_title('Recall by Class', fontweight='bold')\n",
    "                axes[1].set_xticks(x)\n",
    "                axes[1].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[1].legend()\n",
    "                axes[1].grid(axis='y', alpha=0.3)\n",
    "                axes[1].set_ylim(0, 1)\n",
    "                \n",
    "                # F1 Score\n",
    "                axes[2].bar(x - width, short_f1, width, label='Short', color='#d62728')\n",
    "                axes[2].bar(x, neutral_f1, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[2].bar(x + width, long_f1, width, label='Long', color='#2ca02c')\n",
    "                axes[2].set_ylabel('F1 Score', fontweight='bold')\n",
    "                axes[2].set_title('F1 Score by Class', fontweight='bold')\n",
    "                axes[2].set_xticks(x)\n",
    "                axes[2].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[2].legend()\n",
    "                axes[2].grid(axis='y', alpha=0.3)\n",
    "                axes[2].set_ylim(0, 1)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No predictions available for per-class metrics\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" VISUALIZATIONS COMPLETE\")\n",
    "        print(\"=\" * 70)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.4 Transformer Attention Visualization { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Visualization settings\n",
    "visualize_attention = True  #@param {type: \"boolean\"}\n",
    "sample_index = 0  #@param {type: \"integer\"}\n",
    "#@markdown Sample index from validation set to visualize\n",
    "layer_to_visualize = -1  #@param {type: \"integer\"}\n",
    "#@markdown Layer index (-1 for last layer, 0 for first)\n",
    "head_to_visualize = 0  #@param {type: \"integer\"}\n",
    "#@markdown Head index to visualize in detail (0-7)\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if IS_COLAB:\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "else:\n",
    "    PROJECT_ROOT = Path.home() / 'Research'\n",
    "\n",
    "EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments' / 'runs'\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data' / 'splits' / 'scaled'\n",
    "\n",
    "if not visualize_attention:\n",
    "    print(\"\u2713 Attention visualization disabled.\")\n",
    "    print(\"  Enable 'visualize_attention' to see transformer attention patterns.\")\n",
    "elif 'TRAINING_RESULTS' not in dir() or not TRAINING_RESULTS:\n",
    "    print(\"\u26a0 No training results found.\")\n",
    "    print(\"  Run Section 4.1 (Model Training) first.\")\n",
    "elif 'transformer' not in TRAINING_RESULTS:\n",
    "    print(\"\u26a0 Transformer model not trained.\")\n",
    "    print(\"  Enable TRAIN_TRANSFORMER in Section 1 and run Section 4.1.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"=\"*80)\n",
    "        print(\"TRANSFORMER ATTENTION VISUALIZATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Get transformer run ID\n",
    "        run_id = TRAINING_RESULTS['transformer']['run_id']\n",
    "        print(f\"\\n[Loading Model]\")\n",
    "        print(f\"  Run ID: {run_id}\")\n",
    "        \n",
    "        # Load container\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        \n",
    "        print(f\"  Horizon: {TRAINING_HORIZON}\")\n",
    "        print(f\"  Validation samples: {len(container.val_X)}\")\n",
    "        \n",
    "        # Load trained transformer\n",
    "        model_path = EXPERIMENTS_DIR / run_id / 'checkpoints'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            print(f\"\\n\u26a0 Model checkpoint not found at {model_path}\")\n",
    "            print(\"  The model may not have been saved during training.\")\n",
    "        else:\n",
    "            # Import transformer model\n",
    "            from src.models import ModelRegistry\n",
    "            from src.models.config import TrainerConfig\n",
    "            import torch\n",
    "            \n",
    "            # Create model instance with same config\n",
    "            config = TrainerConfig(\n",
    "                model_type='transformer',\n",
    "                horizon=TRAINING_HORIZON,\n",
    "                seq_len=TRANSFORMER_SEQUENCE_LENGTH,\n",
    "                d_model=TRANSFORMER_D_MODEL,\n",
    "                n_heads=TRANSFORMER_N_HEADS,\n",
    "                n_layers=TRANSFORMER_N_LAYERS,\n",
    "                dropout=0.1\n",
    "            )\n",
    "            \n",
    "            model = ModelRegistry.create('transformer', config=config.to_dict())\n",
    "            \n",
    "            # Load trained weights\n",
    "            checkpoint_file = list(model_path.glob('*.pt'))\n",
    "            if checkpoint_file:\n",
    "                model.load(model_path)\n",
    "                print(f\"  \u2713 Model loaded from {checkpoint_file[0].name}\")\n",
    "            else:\n",
    "                print(f\"\\n\u26a0 No .pt checkpoint files found in {model_path}\")\n",
    "                raise FileNotFoundError(\"Model checkpoint not found\")\n",
    "            \n",
    "            # Get validation sample\n",
    "            print(f\"\\n[Extracting Sample]\")\n",
    "            print(f\"  Sample index: {sample_index}\")\n",
    "            \n",
    "            if sample_index >= len(container.val_X):\n",
    "                print(f\"  \u26a0 Sample index {sample_index} out of range (max: {len(container.val_X)-1})\")\n",
    "                print(f\"  Using index 0 instead.\")\n",
    "                sample_index = 0\n",
    "            \n",
    "            # Prepare sample\n",
    "            X_val = container.val_X.iloc[[sample_index]]\n",
    "            y_val = container.val_y.iloc[sample_index]\n",
    "            \n",
    "            # Convert to torch tensor\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            # Reshape for transformer: (batch, seq_len, features)\n",
    "            seq_len = config.seq_len\n",
    "            n_features = X_val.shape[1] // seq_len\n",
    "            \n",
    "            X_tensor = torch.FloatTensor(X_val.values).reshape(1, seq_len, n_features).to(device)\n",
    "            \n",
    "            print(f\"  Input shape: {X_tensor.shape}\")\n",
    "            print(f\"  True label: {y_val}\")\n",
    "            \n",
    "            # Get model prediction and attention weights\n",
    "            model.model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Forward pass to get attention\n",
    "                if hasattr(model.model, 'get_attention_weights'):\n",
    "                    attention_weights, prediction = model.model.get_attention_weights(X_tensor, layer_idx=layer_to_visualize)\n",
    "                else:\n",
    "                    # Fallback: hook into transformer layers\n",
    "                    print(\"  \u26a0 Model doesn't have get_attention_weights method\")\n",
    "                    print(\"  Attention visualization requires model modifications\")\n",
    "                    raise NotImplementedError(\"Attention extraction not implemented\")\n",
    "            \n",
    "            # Convert to numpy\n",
    "            attention_weights = attention_weights.cpu().numpy()  # Shape: (batch, n_heads, seq_len, seq_len)\n",
    "            attention_weights = attention_weights[0]  # Remove batch dim: (n_heads, seq_len, seq_len)\n",
    "            prediction = prediction.cpu().numpy()[0]\n",
    "            \n",
    "            print(f\"\\n[Model Output]\")\n",
    "            print(f\"  Prediction: {prediction.argmax()}\")\n",
    "            print(f\"  Confidence: {prediction.max():.2%}\")\n",
    "            print(f\"  Attention shape: {attention_weights.shape}\")\n",
    "            \n",
    "            # Visualize attention heatmaps for all heads\n",
    "            print(f\"\\n[Visualizing Attention Patterns]\")\n",
    "            \n",
    "            n_heads = attention_weights.shape[0]\n",
    "            n_rows = 2\n",
    "            n_cols = (n_heads + n_rows - 1) // n_rows  # Ceiling division\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 8))\n",
    "            if n_heads == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif n_rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for head_idx in range(n_heads):\n",
    "                ax = axes[head_idx]\n",
    "                \n",
    "                # Plot heatmap\n",
    "                sns.heatmap(\n",
    "                    attention_weights[head_idx],\n",
    "                    cmap='viridis',\n",
    "                    ax=ax,\n",
    "                    cbar=True,\n",
    "                    square=True,\n",
    "                    vmin=0,\n",
    "                    vmax=attention_weights[head_idx].max(),\n",
    "                    cbar_kws={'label': 'Attention Weight'}\n",
    "                )\n",
    "                \n",
    "                ax.set_title(f'Head {head_idx+1}', fontsize=12, fontweight='bold')\n",
    "                ax.set_xlabel('Key Position (Source)')\n",
    "                ax.set_ylabel('Query Position (Target)')\n",
    "                \n",
    "                # Add grid for readability\n",
    "                ax.grid(False)\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for idx in range(n_heads, len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "            \n",
    "            layer_name = f\"Layer {layer_to_visualize}\" if layer_to_visualize >= 0 else \"Final Layer\"\n",
    "            plt.suptitle(\n",
    "                f'Transformer Attention Weights - {layer_name}\\nSample {sample_index} | True: {y_val} | Pred: {prediction.argmax()}',\n",
    "                fontsize=14,\n",
    "                fontweight='bold',\n",
    "                y=1.02\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Detailed analysis for selected head\n",
    "            print(f\"\\n[Attention Analysis - Head {head_to_visualize + 1}]\")\n",
    "            \n",
    "            if head_to_visualize >= n_heads:\n",
    "                print(f\"  \u26a0 Head {head_to_visualize} not available (max: {n_heads-1})\")\n",
    "                head_to_visualize = 0\n",
    "            \n",
    "            head_attention = attention_weights[head_to_visualize]\n",
    "            \n",
    "            # Average attention per position (what positions are attended to)\n",
    "            avg_attention_received = head_attention.mean(axis=0)  # Average over queries\n",
    "            avg_attention_given = head_attention.mean(axis=1)     # Average over keys\n",
    "            \n",
    "            print(f\"\\n  Most attended positions (received):\")\n",
    "            top_positions = avg_attention_received.argsort()[-5:][::-1]\n",
    "            for pos in top_positions:\n",
    "                print(f\"    Position {pos:3d}: {avg_attention_received[pos]:.4f}\")\n",
    "            \n",
    "            print(f\"\\n  Most attentive positions (given):\")\n",
    "            top_giving = avg_attention_given.argsort()[-5:][::-1]\n",
    "            for pos in top_giving:\n",
    "                print(f\"    Position {pos:3d}: {avg_attention_given[pos]:.4f}\")\n",
    "            \n",
    "            # Attention entropy (uniformity)\n",
    "            attention_entropy = [entropy(head_attention[i]) for i in range(len(head_attention))]\n",
    "            avg_entropy = np.mean(attention_entropy)\n",
    "            \n",
    "            print(f\"\\n  Attention entropy: {avg_entropy:.4f}\")\n",
    "            print(f\"    (Higher = more uniform, Lower = more focused)\")\n",
    "            \n",
    "            # Interpretability insights\n",
    "            print(f\"\\n[Interpretability Insights]\")\n",
    "            \n",
    "            # Check recency bias\n",
    "            recent_positions = seq_len // 10  # Last 10% of sequence\n",
    "            recent_attention = avg_attention_received[-recent_positions:].sum()\n",
    "            \n",
    "            if recent_attention > 0.3:  # >30% on recent bars\n",
    "                print(f\"  \u2192 Strong recency bias ({recent_attention:.1%} on last {recent_positions} positions)\")\n",
    "                print(f\"     Model relies heavily on most recent observations\")\n",
    "            \n",
    "            # Check long-range dependencies\n",
    "            early_positions = seq_len // 10  # First 10% of sequence\n",
    "            early_attention = avg_attention_received[:early_positions].sum()\n",
    "            \n",
    "            if early_attention > 0.15:  # >15% on early bars\n",
    "                print(f\"  \u2192 Long-range context ({early_attention:.1%} on first {early_positions} positions)\")\n",
    "                print(f\"     Model uses historical information beyond recent bars\")\n",
    "            \n",
    "            # Check attention focus vs spread\n",
    "            if avg_entropy < 2.0:\n",
    "                print(f\"  \u2192 Focused attention (entropy={avg_entropy:.2f})\")\n",
    "                print(f\"     Model concentrates on specific positions\")\n",
    "            elif avg_entropy > 4.0:\n",
    "                print(f\"  \u2192 Distributed attention (entropy={avg_entropy:.2f})\")\n",
    "                print(f\"     Model spreads attention broadly across sequence\")\n",
    "            \n",
    "            # Diagonal attention (position attends to itself)\n",
    "            self_attention = np.diag(head_attention).mean()\n",
    "            if self_attention > 0.2:\n",
    "                print(f\"  \u2192 Self-attention ({self_attention:.1%} average)\")\n",
    "                print(f\"     Positions attend to themselves (local context)\")\n",
    "            \n",
    "            print(f\"\\n\u2713 Attention visualization complete\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n\u26a0 Error: {e}\")\n",
    "        print(\"  The transformer model checkpoint was not found.\")\n",
    "        print(\"  Make sure the model completed training in Section 4.1.\")\n",
    "        \n",
    "    except NotImplementedError as e:\n",
    "        print(f\"\\n\u26a0 {e}\")\n",
    "        print(\"  The transformer model needs modifications to extract attention weights.\")\n",
    "        print(\"  Add a 'get_attention_weights' method to the transformer model class.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u26a0 Error during attention visualization:\")\n",
    "        print(f\"  {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 4.5 Test Set Performance { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Configuration options\n",
    "run_test_evaluation = True  #@param {type: \"boolean\"}\n",
    "show_sample_predictions = True  #@param {type: \"boolean\"}\n",
    "n_samples_to_show = 20  #@param {type: \"integer\"}\n",
    "show_generalization_gap = True  #@param {type: \"boolean\"}\n",
    "save_test_predictions = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Default sequence length for neural models\n",
    "DEFAULT_SEQ_LEN = 30  #@param {type: \"integer\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Check if we can run test evaluation\n",
    "if not run_test_evaluation:\n",
    "    print(\"[Skipped] Test evaluation disabled. Enable checkbox above to run.\")\n",
    "elif not TRAINING_RESULTS:\n",
    "    print(\"[Error] No training results available.\")\n",
    "    print(\"Run Section 4.1 to train models first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" TEST SET PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Filter successful models only\n",
    "    successful_models = {\n",
    "        name: data for name, data in TRAINING_RESULTS.items()\n",
    "        if data.get('run_id') != 'failed' and data.get('metrics')\n",
    "    }\n",
    "    \n",
    "    if not successful_models:\n",
    "        print(\"\\n[Error] No successful models to evaluate.\")\n",
    "        print(\"All models failed during training.\")\n",
    "    else:\n",
    "        try:\n",
    "            # ============================================================\n",
    "            # LOAD TEST DATA (both tabular and sequence formats)\n",
    "            # ============================================================\n",
    "            print(f\"\\n[1/5] Loading test data...\")\n",
    "            \n",
    "            from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "            from src.models import ModelRegistry\n",
    "            from src.models.data_preparation import prepare_test_data\n",
    "            \n",
    "            container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "                path=SPLITS_DIR,\n",
    "                horizon=TRAINING_HORIZON\n",
    "            )\n",
    "            \n",
    "            # Load tabular data (2D) for tabular models\n",
    "            X_test_tabular, y_test_tabular, _ = prepare_test_data(\n",
    "                container, requires_sequences=False\n",
    "            )\n",
    "            print(f\"  Tabular test samples: {len(X_test_tabular):,}\")\n",
    "            print(f\"  Tabular features: {X_test_tabular.shape[1]}\")\n",
    "            \n",
    "            # Load sequence data (3D) for neural models\n",
    "            HAS_SEQUENCE_DATA = False\n",
    "            X_test_seq = None\n",
    "            y_test_seq = None\n",
    "            try:\n",
    "                X_test_seq, y_test_seq, _ = prepare_test_data(\n",
    "                    container, requires_sequences=True, sequence_length=DEFAULT_SEQ_LEN\n",
    "                )\n",
    "                HAS_SEQUENCE_DATA = True\n",
    "                print(f\"  Sequence test samples: {len(X_test_seq):,}\")\n",
    "                print(f\"  Sequence shape: {X_test_seq.shape} (samples, seq_len, features)\")\n",
    "            except Exception as seq_error:\n",
    "                print(f\"  [WARNING] Could not load sequence data: {seq_error}\")\n",
    "                print(f\"  Neural models will be skipped during evaluation.\")\n",
    "            \n",
    "            # ============================================================\n",
    "            # RUN PREDICTIONS ON TEST SET\n",
    "            # ============================================================\n",
    "            print(f\"\\n[2/5] Running predictions on test set...\")\n",
    "            \n",
    "            TEST_RESULTS = {}\n",
    "            \n",
    "            for model_name, train_data in successful_models.items():\n",
    "                print(f\"\\n  Evaluating: {model_name.upper()}\")\n",
    "                \n",
    "                try:\n",
    "                    # Determine if model requires sequences using ModelRegistry\n",
    "                    requires_sequences = False\n",
    "                    if ModelRegistry.is_registered(model_name):\n",
    "                        model_info = ModelRegistry.get_model_info(model_name)\n",
    "                        requires_sequences = model_info.get(\"requires_sequences\", False)\n",
    "                    \n",
    "                    # Select appropriate test data\n",
    "                    if requires_sequences:\n",
    "                        if not HAS_SEQUENCE_DATA:\n",
    "                            print(f\"    [SKIPPED] No sequence data available for {model_name}\")\n",
    "                            continue\n",
    "                        X_test = X_test_seq\n",
    "                        y_test = y_test_seq\n",
    "                        print(f\"    Using sequence data: {X_test.shape}\")\n",
    "                    else:\n",
    "                        X_test = X_test_tabular\n",
    "                        y_test = y_test_tabular\n",
    "                        print(f\"    Using tabular data: {X_test.shape}\")\n",
    "                    \n",
    "                    run_id = train_data.get('run_id', 'unknown')\n",
    "                    model_dir = EXPERIMENTS_DIR / run_id\n",
    "                    \n",
    "                    # Load model from checkpoints\n",
    "                    checkpoint_dir = model_dir / 'checkpoints'\n",
    "                    \n",
    "                    # Try different model file formats\n",
    "                    model_loaded = False\n",
    "                    model = None\n",
    "                    \n",
    "                    # Method 1: Try pickle format\n",
    "                    pickle_path = checkpoint_dir / 'model.pkl'\n",
    "                    if pickle_path.exists():\n",
    "                        with open(pickle_path, 'rb') as f:\n",
    "                            model = pickle.load(f)\n",
    "                        model_loaded = True\n",
    "                        print(f\"    Loaded from: {pickle_path.name}\")\n",
    "                    \n",
    "                    # Method 2: Try joblib format\n",
    "                    if not model_loaded:\n",
    "                        joblib_path = checkpoint_dir / 'model.joblib'\n",
    "                        if joblib_path.exists():\n",
    "                            model = joblib.load(joblib_path)\n",
    "                            model_loaded = True\n",
    "                            print(f\"    Loaded from: {joblib_path.name}\")\n",
    "                    \n",
    "                    # Method 3: Try PyTorch format (for neural models)\n",
    "                    if not model_loaded and requires_sequences:\n",
    "                        torch_path = checkpoint_dir / 'model.pt'\n",
    "                        if torch_path.exists():\n",
    "                            import torch\n",
    "                            \n",
    "                            # Recreate model architecture with proper input size\n",
    "                            input_size = X_test.shape[2] if X_test.ndim == 3 else X_test.shape[1]\n",
    "                            model = ModelRegistry.create(model_name, config={\n",
    "                                'input_size': input_size,\n",
    "                                'hidden_size': 128,\n",
    "                                'num_layers': 2,\n",
    "                            })\n",
    "                            \n",
    "                            # Load weights\n",
    "                            state_dict = torch.load(torch_path, map_location='cpu')\n",
    "                            model.model.load_state_dict(state_dict)\n",
    "                            model.model.eval()\n",
    "                            model_loaded = True\n",
    "                            print(f\"    Loaded from: {torch_path.name}\")\n",
    "                    \n",
    "                    if not model_loaded:\n",
    "                        print(f\"    [WARNING] Model file not found in {checkpoint_dir}\")\n",
    "                        print(f\"    Skipping {model_name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    if hasattr(model, 'predict'):\n",
    "                        pred_result = model.predict(X_test)\n",
    "                        \n",
    "                        # Handle PredictionOutput from models\n",
    "                        if hasattr(pred_result, 'class_predictions'):\n",
    "                            y_pred = pred_result.class_predictions\n",
    "                        elif hasattr(pred_result, 'predictions'):\n",
    "                            y_pred = pred_result.predictions\n",
    "                        else:\n",
    "                            y_pred = pred_result\n",
    "                        \n",
    "                        # Ensure y_pred is a numpy array\n",
    "                        if hasattr(y_pred, 'numpy'):\n",
    "                            y_pred = y_pred.numpy()\n",
    "                        y_pred = np.asarray(y_pred)\n",
    "                    else:\n",
    "                        print(f\"    [WARNING] Model has no predict method\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate test metrics\n",
    "                    test_acc = accuracy_score(y_test, y_pred)\n",
    "                    test_macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    test_weighted_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                    test_precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    test_recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    # Per-class metrics\n",
    "                    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    per_class_precision = precision_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    per_class_recall = recall_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    \n",
    "                    # Confusion matrix\n",
    "                    cm = confusion_matrix(y_test, y_pred, labels=[-1, 0, 1])\n",
    "                    \n",
    "                    # Store results\n",
    "                    val_metrics = train_data.get('metrics', {})\n",
    "                    \n",
    "                    TEST_RESULTS[model_name] = {\n",
    "                        'test_metrics': {\n",
    "                            'accuracy': test_acc,\n",
    "                            'macro_f1': test_macro_f1,\n",
    "                            'weighted_f1': test_weighted_f1,\n",
    "                            'precision': test_precision,\n",
    "                            'recall': test_recall,\n",
    "                            'per_class_f1': per_class_f1.tolist(),\n",
    "                            'per_class_precision': per_class_precision.tolist(),\n",
    "                            'per_class_recall': per_class_recall.tolist(),\n",
    "                            'confusion_matrix': cm.tolist(),\n",
    "                        },\n",
    "                        'val_metrics': val_metrics,\n",
    "                        'predictions': y_pred.tolist() if hasattr(y_pred, 'tolist') else list(y_pred),\n",
    "                        'run_id': run_id,\n",
    "                        'requires_sequences': requires_sequences,\n",
    "                        'test_samples': len(y_test),\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"    Test Accuracy: {test_acc:.2%}\")\n",
    "                    print(f\"    Test Macro F1: {test_macro_f1:.4f}\")\n",
    "                    \n",
    "                    # Save predictions if requested\n",
    "                    if save_test_predictions:\n",
    "                        test_pred_file = model_dir / 'test_predictions.json'\n",
    "                        with open(test_pred_file, 'w') as f:\n",
    "                            json.dump({\n",
    "                                'y_true': y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test),\n",
    "                                'y_pred': y_pred.tolist() if hasattr(y_pred, 'tolist') else list(y_pred),\n",
    "                                'test_metrics': TEST_RESULTS[model_name]['test_metrics'],\n",
    "                                'requires_sequences': requires_sequences,\n",
    "                            }, f, indent=2)\n",
    "                    \n",
    "                    del model\n",
    "                    \n",
    "                except Exception as model_error:\n",
    "                    print(f\"    [ERROR] Failed to evaluate {model_name}: {model_error}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            # ============================================================\n",
    "            # DISPLAY COMPARISON TABLE\n",
    "            # ============================================================\n",
    "            if TEST_RESULTS:\n",
    "                print(f\"\\n[3/5] Model Performance Comparison\")\n",
    "                print(\"=\" * 70)\n",
    "                \n",
    "                # Build comparison DataFrame\n",
    "                comparison_data = []\n",
    "                for model_name, results in TEST_RESULTS.items():\n",
    "                    val_metrics = results['val_metrics']\n",
    "                    test_metrics = results['test_metrics']\n",
    "                    \n",
    "                    val_acc = val_metrics.get('accuracy', 0)\n",
    "                    test_acc = test_metrics.get('accuracy', 0)\n",
    "                    val_f1 = val_metrics.get('macro_f1', 0)\n",
    "                    test_f1 = test_metrics.get('macro_f1', 0)\n",
    "                    \n",
    "                    # Calculate generalization gap\n",
    "                    acc_gap = (test_acc - val_acc) * 100\n",
    "                    f1_gap = (test_f1 - val_f1) * 100\n",
    "                    \n",
    "                    # Model type indicator\n",
    "                    model_type = \"Seq\" if results.get('requires_sequences', False) else \"Tab\"\n",
    "                    \n",
    "                    comparison_data.append({\n",
    "                        'Model': model_name,\n",
    "                        'Type': model_type,\n",
    "                        'Val Acc': f\"{val_acc:.2%}\",\n",
    "                        'Test Acc': f\"{test_acc:.2%}\",\n",
    "                        'Val F1': f\"{val_f1:.4f}\",\n",
    "                        'Test F1': f\"{test_f1:.4f}\",\n",
    "                        'Acc Gap (%)': f\"{acc_gap:+.2f}\",\n",
    "                        'F1 Gap (%)': f\"{f1_gap:+.2f}\",\n",
    "                    })\n",
    "                \n",
    "                comparison_df = pd.DataFrame(comparison_data)\n",
    "                \n",
    "                # Sort by test F1 score\n",
    "                comparison_df = comparison_df.sort_values(\n",
    "                    by='Test F1',\n",
    "                    ascending=False,\n",
    "                    key=lambda x: x.str.replace('%', '').astype(float) if x.dtype == 'object' else x\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\")\n",
    "                print(comparison_df.to_string(index=False))\n",
    "                \n",
    "                # Best performing model\n",
    "                best_model_name = comparison_df.iloc[0]['Model']\n",
    "                best_test_f1 = comparison_df.iloc[0]['Test F1']\n",
    "                print(f\"\\n  Best Model on Test Set: {best_model_name} (F1: {best_test_f1})\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # GENERALIZATION ANALYSIS\n",
    "                # ============================================================\n",
    "                if show_generalization_gap:\n",
    "                    print(f\"\\n[4/5] Generalization Analysis\")\n",
    "                    print(\"=\" * 70)\n",
    "                    \n",
    "                    for model_name, results in TEST_RESULTS.items():\n",
    "                        val_metrics = results['val_metrics']\n",
    "                        test_metrics = results['test_metrics']\n",
    "                        \n",
    "                        val_f1 = val_metrics.get('macro_f1', 0)\n",
    "                        test_f1 = test_metrics.get('macro_f1', 0)\n",
    "                        \n",
    "                        gap_pct = ((test_f1 - val_f1) / val_f1 * 100) if val_f1 > 0 else 0\n",
    "                        \n",
    "                        # Model type indicator\n",
    "                        model_type = \"[Seq]\" if results.get('requires_sequences', False) else \"[Tab]\"\n",
    "                        \n",
    "                        # Color-code based on gap\n",
    "                        if abs(gap_pct) < 2:\n",
    "                            status = \"EXCELLENT\"\n",
    "                        elif abs(gap_pct) < 5:\n",
    "                            status = \"GOOD\"\n",
    "                        else:\n",
    "                            status = \"POOR\"\n",
    "                        \n",
    "                        print(f\"\\n  {model_name.upper()} {model_type}:\")\n",
    "                        print(f\"    Val F1:  {val_f1:.4f}\")\n",
    "                        print(f\"    Test F1: {test_f1:.4f}\")\n",
    "                        print(f\"    Gap:     {gap_pct:+.2f}% [{status}]\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # SAMPLE PREDICTIONS\n",
    "                # ============================================================\n",
    "                if show_sample_predictions and n_samples_to_show > 0:\n",
    "                    print(f\"\\n[5/5] Sample Predictions (first {n_samples_to_show})\")\n",
    "                    print(\"=\" * 70)\n",
    "                    \n",
    "                    # Show predictions for each model (using its appropriate test data)\n",
    "                    for model_name, results in TEST_RESULTS.items():\n",
    "                        predictions = results['predictions']\n",
    "                        sample_pred = predictions[:n_samples_to_show]\n",
    "                        \n",
    "                        # Get actual labels from appropriate test set\n",
    "                        if results.get('requires_sequences', False):\n",
    "                            sample_actual = y_test_seq[:n_samples_to_show] if HAS_SEQUENCE_DATA else []\n",
    "                        else:\n",
    "                            sample_actual = y_test_tabular[:n_samples_to_show]\n",
    "                        \n",
    "                        if len(sample_actual) > 0:\n",
    "                            # Calculate accuracy for this sample\n",
    "                            matches = sum(1 for a, p in zip(sample_actual, sample_pred) if a == p)\n",
    "                            sample_acc = matches / len(sample_actual) * 100\n",
    "                            \n",
    "                            model_type = \"[Seq]\" if results.get('requires_sequences', False) else \"[Tab]\"\n",
    "                            print(f\"\\n  {model_name} {model_type}:\")\n",
    "                            print(f\"    Actual:    {list(sample_actual[:10])}{'...' if len(sample_actual) > 10 else ''}\")\n",
    "                            print(f\"    Predicted: {sample_pred[:10]}{'...' if len(sample_pred) > 10 else ''}\")\n",
    "                            print(f\"    Match:     {sample_acc:.1f}%\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 70)\n",
    "                print(\" TEST EVALUATION COMPLETE\")\n",
    "                print(\"=\" * 70)\n",
    "                \n",
    "                # Summary statistics\n",
    "                n_tabular = sum(1 for r in TEST_RESULTS.values() if not r.get('requires_sequences', False))\n",
    "                n_sequence = sum(1 for r in TEST_RESULTS.values() if r.get('requires_sequences', False))\n",
    "                print(f\"\\n  Evaluated: {len(TEST_RESULTS)} models ({n_tabular} tabular, {n_sequence} sequence)\")\n",
    "                print(f\"  Tabular test samples: {len(X_test_tabular):,}\")\n",
    "                if HAS_SEQUENCE_DATA:\n",
    "                    print(f\"  Sequence test samples: {len(X_test_seq):,}\")\n",
    "                \n",
    "                if save_test_predictions:\n",
    "                    print(f\"  Predictions saved to: {EXPERIMENTS_DIR}/[run_id]/test_predictions.json\")\n",
    "            else:\n",
    "                print(\"\\n[WARNING] No test results generated.\")\n",
    "                print(\"All models failed to load or predict.\")\n",
    "            \n",
    "            # Clean up\n",
    "            del container, X_test_tabular, y_test_tabular\n",
    "            if HAS_SEQUENCE_DATA:\n",
    "                del X_test_seq, y_test_seq\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] Test evaluation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.6 Trading Performance Simulation { display-mode: \"form\" }\n",
    "\n",
    "# ============================================================================\n",
    "# TRADING PERFORMANCE SIMULATION\n",
    "# ============================================================================\n",
    "# This cell evaluates models from a trading perspective with realistic costs.\n",
    "# ML metrics (accuracy, F1) can be misleading - a model with good F1 may lose\n",
    "# money after accounting for trading fees and slippage.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#@markdown ### Trading Cost Configuration\n",
    "COMMISSION_PER_TRADE = 2.50  #@param {type: \"number\"}\n",
    "#@markdown Commission per round-trip trade (buy + sell)\n",
    "\n",
    "SLIPPAGE_TICKS = 1  #@param {type: \"integer\"}\n",
    "#@markdown Slippage in ticks per side (1 tick = 0.25 for MES)\n",
    "\n",
    "TICK_VALUE = 1.25  #@param {type: \"number\"}\n",
    "#@markdown Value per tick (MES = $1.25, ES = $12.50)\n",
    "\n",
    "POINT_VALUE = 5.0  #@param {type: \"number\"}\n",
    "#@markdown Contract point value (MES = $5, ES = $50)\n",
    "\n",
    "#@markdown ### Signal Configuration\n",
    "POSITION_SIZE = 1  #@param {type: \"integer\"}\n",
    "#@markdown Number of contracts per trade\n",
    "\n",
    "#@markdown ### Evaluation Options\n",
    "run_trading_simulation = True  #@param {type: \"boolean\"}\n",
    "show_equity_curves = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "\n",
    "def calculate_trading_metrics(y_true, y_pred, prices, \n",
    "                              commission=COMMISSION_PER_TRADE,\n",
    "                              slippage_ticks=SLIPPAGE_TICKS,\n",
    "                              tick_value=TICK_VALUE,\n",
    "                              point_value=POINT_VALUE,\n",
    "                              position_size=POSITION_SIZE):\n",
    "    \"\"\"\n",
    "    Calculate trading-realistic metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual labels (0=Short, 1=Neutral, 2=Long typically)\n",
    "        y_pred: Predicted labels\n",
    "        prices: Close prices aligned with predictions\n",
    "        commission: Commission per round-trip trade\n",
    "        slippage_ticks: Slippage in ticks per side\n",
    "        tick_value: Value per tick\n",
    "        point_value: Contract point value\n",
    "        position_size: Number of contracts per trade\n",
    "    \n",
    "    Returns:\n",
    "        dict: Trading metrics including PnL, Sharpe, drawdown\n",
    "    \"\"\"\n",
    "    # Cost per trade (round-trip)\n",
    "    total_cost_per_trade = commission + (slippage_ticks * tick_value * 2)\n",
    "    \n",
    "    # Map predictions to positions: 0->-1 (short), 1->0 (neutral), 2->1 (long)\n",
    "    # Adjust mapping based on actual label encoding\n",
    "    pred_array = np.array(y_pred).flatten()\n",
    "    unique_labels = np.unique(pred_array)\n",
    "    \n",
    "    # Common label schemes: {0,1,2} or {-1,0,1}\n",
    "    if set(unique_labels).issubset({0, 1, 2}):\n",
    "        # Map 0->-1, 1->0, 2->1\n",
    "        positions = pred_array - 1\n",
    "    elif set(unique_labels).issubset({-1, 0, 1}):\n",
    "        positions = pred_array\n",
    "    else:\n",
    "        # Fallback: use as-is\n",
    "        positions = pred_array\n",
    "    \n",
    "    positions = positions.astype(float)\n",
    "    \n",
    "    # Ensure prices and positions are aligned\n",
    "    min_len = min(len(positions), len(prices) - 1)\n",
    "    positions = positions[:min_len]\n",
    "    \n",
    "    # Calculate returns\n",
    "    price_returns = np.diff(prices[:min_len + 1]) / prices[:min_len]\n",
    "    \n",
    "    # Signal at bar t predicts return from t to t+1\n",
    "    trade_signals = positions\n",
    "    trade_returns = price_returns\n",
    "    \n",
    "    # Calculate strategy returns (in dollar terms)\n",
    "    gross_returns = trade_signals * trade_returns * point_value * position_size\n",
    "    \n",
    "    # Identify trade entry/exit points (position changes)\n",
    "    position_changes = np.diff(np.concatenate([[0], positions]))\n",
    "    \n",
    "    # Calculate trade costs\n",
    "    trade_costs = np.abs(position_changes[:len(gross_returns)]) * total_cost_per_trade / 2\n",
    "    \n",
    "    # Net returns after costs\n",
    "    net_returns = gross_returns - trade_costs\n",
    "    \n",
    "    # Cumulative PnL\n",
    "    cumulative_pnl = np.cumsum(net_returns)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_pnl = cumulative_pnl[-1] if len(cumulative_pnl) > 0 else 0\n",
    "    total_gross = np.sum(gross_returns)\n",
    "    total_costs = np.sum(trade_costs)\n",
    "    \n",
    "    # Count round-trip trades\n",
    "    n_trades = int(np.sum(np.abs(position_changes) > 0) // 2)\n",
    "    \n",
    "    # Sharpe Ratio (annualized, assuming 5-min bars)\n",
    "    bars_per_year = 252 * 78  # 252 days * 78 bars per day (5-min, 6.5 hour session)\n",
    "    if len(net_returns) > 0 and np.std(net_returns) > 0:\n",
    "        sharpe = np.mean(net_returns) / np.std(net_returns) * np.sqrt(bars_per_year)\n",
    "    else:\n",
    "        sharpe = 0.0\n",
    "    \n",
    "    # Maximum Drawdown\n",
    "    peak = np.maximum.accumulate(np.concatenate([[0], cumulative_pnl]))\n",
    "    drawdown = peak - np.concatenate([[0], cumulative_pnl])\n",
    "    max_drawdown = np.max(drawdown) if len(drawdown) > 0 else 0\n",
    "    \n",
    "    # Win Rate (by bar when in a position)\n",
    "    active_bars = trade_signals != 0\n",
    "    if np.sum(active_bars) > 0:\n",
    "        winning_bars = net_returns[active_bars] > 0\n",
    "        win_rate = np.mean(winning_bars)\n",
    "    else:\n",
    "        win_rate = 0.0\n",
    "    \n",
    "    # Profit Factor\n",
    "    gross_profit = np.sum(net_returns[net_returns > 0])\n",
    "    gross_loss = np.abs(np.sum(net_returns[net_returns < 0]))\n",
    "    profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')\n",
    "    \n",
    "    # Average trade PnL\n",
    "    avg_trade = total_pnl / n_trades if n_trades > 0 else 0\n",
    "    \n",
    "    # Calmar ratio (annualized return / max drawdown)\n",
    "    if max_drawdown > 0:\n",
    "        annual_return = total_pnl * (bars_per_year / len(net_returns)) if len(net_returns) > 0 else 0\n",
    "        calmar = annual_return / max_drawdown\n",
    "    else:\n",
    "        calmar = float('inf') if total_pnl > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_pnl': total_pnl,\n",
    "        'total_gross': total_gross,\n",
    "        'total_costs': total_costs,\n",
    "        'n_trades': n_trades,\n",
    "        'avg_trade': avg_trade,\n",
    "        'win_rate': win_rate,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'profit_factor': profit_factor,\n",
    "        'calmar_ratio': calmar,\n",
    "        'cumulative_pnl': cumulative_pnl\n",
    "    }\n",
    "\n",
    "# Check prerequisites\n",
    "if not run_trading_simulation:\n",
    "    print(\"[Skipped] Trading simulation disabled. Enable checkbox above to run.\")\n",
    "elif 'TEST_RESULTS' not in dir() or not TEST_RESULTS:\n",
    "    print(\"[Error] No test results available.\")\n",
    "    print(\"Run Section 4.5 (Test Set Performance) first.\")\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\" TRADING PERFORMANCE SIMULATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nCost assumptions: ${COMMISSION_PER_TRADE:.2f} commission + {SLIPPAGE_TICKS} tick(s) slippage\")\n",
    "    print(f\"Tick value: ${TICK_VALUE:.2f}, Point value: ${POINT_VALUE:.2f}\")\n",
    "    print(f\"Position size: {POSITION_SIZE} contract(s)\")\n",
    "    \n",
    "    # Re-load container to get price data (was deleted in previous cell)\n",
    "    try:\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 20\n",
    "        )\n",
    "        \n",
    "        test_df = container.get_dataframe('test')\n",
    "        test_prices = test_df['close'].values\n",
    "        print(f\"Test samples: {len(test_prices):,}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Error] Could not load price data: {e}\")\n",
    "        test_prices = None\n",
    "    \n",
    "    TRADING_RESULTS = {}\n",
    "    \n",
    "    if test_prices is not None and len(TEST_RESULTS) > 0:\n",
    "        # Calculate trading metrics for each model\n",
    "        for model_name, result in TEST_RESULTS.items():\n",
    "            if 'predictions' in result:\n",
    "                y_pred = result['predictions']\n",
    "                n_samples = len(y_pred)\n",
    "                \n",
    "                # Ensure we have enough prices\n",
    "                if n_samples + 1 <= len(test_prices):\n",
    "                    metrics = calculate_trading_metrics(\n",
    "                        y_true=None,  # Not used in current implementation\n",
    "                        y_pred=y_pred,\n",
    "                        prices=test_prices[:n_samples + 1]\n",
    "                    )\n",
    "                    TRADING_RESULTS[model_name] = metrics\n",
    "        \n",
    "        if TRADING_RESULTS:\n",
    "            # Display results table\n",
    "            print(f\"{'Model':<20} {'PnL':>12} {'Trades':>8} {'Win%':>8} {'Sharpe':>8} {'MaxDD':>12} {'PF':>8}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Sort by PnL\n",
    "            sorted_results = sorted(TRADING_RESULTS.items(), key=lambda x: x[1]['total_pnl'], reverse=True)\n",
    "            \n",
    "            for model_name, m in sorted_results:\n",
    "                pf_str = f\"{m['profit_factor']:.2f}\" if m['profit_factor'] != float('inf') else \"inf\"\n",
    "                print(f\"{model_name:<20} ${m['total_pnl']:>11,.2f} {m['n_trades']:>8} {m['win_rate']*100:>7.1f}% {m['sharpe_ratio']:>8.2f} ${m['max_drawdown']:>11,.2f} {pf_str:>8}\")\n",
    "            \n",
    "            # Best model by trading performance\n",
    "            best_trading = sorted_results[0] if sorted_results else None\n",
    "            \n",
    "            print()\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            if best_trading:\n",
    "                print(f\"\\nBest Trading Model: {best_trading[0]}\")\n",
    "                print(f\"  Total PnL: ${best_trading[1]['total_pnl']:,.2f}\")\n",
    "                print(f\"  Sharpe Ratio: {best_trading[1]['sharpe_ratio']:.2f}\")\n",
    "                print(f\"  Max Drawdown: ${best_trading[1]['max_drawdown']:,.2f}\")\n",
    "                \n",
    "                # Compare with best ML model (by accuracy from TEST_RESULTS)\n",
    "                ml_accuracies = {\n",
    "                    name: r.get('test_metrics', {}).get('accuracy', 0) \n",
    "                    for name, r in TEST_RESULTS.items()\n",
    "                }\n",
    "                if ml_accuracies:\n",
    "                    best_ml_name = max(ml_accuracies.keys(), key=lambda x: ml_accuracies[x])\n",
    "                    best_ml_acc = ml_accuracies[best_ml_name]\n",
    "                    \n",
    "                    if best_trading[0] != best_ml_name:\n",
    "                        print(f\"\\n[!] Note: Best ML model by accuracy ({best_ml_name}, {best_ml_acc:.2%})\")\n",
    "                        print(f\"    differs from best trading model ({best_trading[0]})\")\n",
    "                        print(\"    This demonstrates why trading-realistic evaluation matters!\")\n",
    "            \n",
    "            # Plot equity curves if enabled\n",
    "            if show_equity_curves and len(sorted_results) > 0:\n",
    "                try:\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    \n",
    "                    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                    \n",
    "                    # Plot top 5 models\n",
    "                    for model_name, m in sorted_results[:5]:\n",
    "                        ax.plot(m['cumulative_pnl'], label=f\"{model_name} (${m['total_pnl']:,.0f})\")\n",
    "                    \n",
    "                    ax.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "                    ax.set_xlabel('Bar')\n",
    "                    ax.set_ylabel('Cumulative PnL ($)')\n",
    "                    ax.set_title('Trading Performance: Equity Curves (Top 5 Models)')\n",
    "                    ax.legend(loc='best')\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Could not plot equity curves: {e}\")\n",
    "        else:\n",
    "            print(\"[Warning] No trading results generated. Check TEST_RESULTS structure.\")\n",
    "        \n",
    "        # Clean up\n",
    "        del container\n",
    "    else:\n",
    "        if test_prices is None:\n",
    "            print(\"[Error] Price data not available for trading simulation.\")\n",
    "        if len(TEST_RESULTS) == 0:\n",
    "            print(\"[Error] No test results available.\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. PHASE 3: CROSS-VALIDATION (Optional)\n",
    "\n",
    "Run purged K-fold cross-validation for robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Run Cross-Validation { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'RUN_CROSS_VALIDATION' not in dir():\n",
    "    RUN_CROSS_VALIDATION = False\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "if 'CV_TUNE_HYPERPARAMS' not in dir():\n",
    "    CV_TUNE_HYPERPARAMS = False\n",
    "if 'CV_N_TRIALS' not in dir():\n",
    "    CV_N_TRIALS = 20\n",
    "\n",
    "# Define clear_memory if not available\n",
    "if 'clear_memory' not in dir():\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        if GPU_AVAILABLE:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize global results dictionaries\n",
    "if 'CV_RESULTS' not in dir():\n",
    "    CV_RESULTS = {}\n",
    "if 'TUNING_RESULTS' not in dir():\n",
    "    TUNING_RESULTS = {}\n",
    "\n",
    "if not RUN_CROSS_VALIDATION:\n",
    "    print(\"[Skipped] Cross-validation disabled in configuration.\")\n",
    "    print(\"Set RUN_CROSS_VALIDATION = True in Section 1 to enable.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 3: CROSS-VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        from src.cross_validation import PurgedKFold, PurgedKFoldConfig\n",
    "        from src.cross_validation.cv_runner import TimeSeriesOptunaTuner\n",
    "        from src.cross_validation.param_spaces import get_param_space\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        from src.models import ModelRegistry\n",
    "        from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "        \n",
    "        # Load data\n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        \n",
    "        X, y, _ = container.get_sklearn_arrays('train')\n",
    "        print(f\"\\nData: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "        \n",
    "        # Configure CV\n",
    "        cv_config = PurgedKFoldConfig(\n",
    "            n_splits=CV_N_SPLITS,\n",
    "            purge_bars=PURGE_BARS,\n",
    "            embargo_bars=EMBARGO_BARS,\n",
    "        )\n",
    "        cv = PurgedKFold(cv_config)\n",
    "        \n",
    "        print(f\"CV: {CV_N_SPLITS} folds, purge={PURGE_BARS}, embargo={EMBARGO_BARS}\")\n",
    "        \n",
    "        # Get list of successfully trained models\n",
    "        successful_models = [m for m in TRAINING_RESULTS.keys() if TRAINING_RESULTS[m].get('status') == 'success']\n",
    "        \n",
    "        if not successful_models:\n",
    "            print(\"\\n[Warning] No trained models found. Train models first in Section 4.\")\n",
    "        else:\n",
    "            print(f\"\\nRunning CV for {len(successful_models)} models: {', '.join(successful_models)}\")\n",
    "            \n",
    "            # Run CV for ALL trained models\n",
    "            cv_summary_data = []\n",
    "            \n",
    "            for model_name in tqdm(successful_models, desc=\"Cross-Validation\"):\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Model: {model_name}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                \n",
    "                # Hyperparameter tuning (if enabled)\n",
    "                tuned_params = {}\n",
    "                if CV_TUNE_HYPERPARAMS:\n",
    "                    param_space = get_param_space(model_name)\n",
    "                    \n",
    "                    if param_space:\n",
    "                        print(f\"  Tuning hyperparameters ({CV_N_TRIALS} trials)...\")\n",
    "                        try:\n",
    "                            tuner = TimeSeriesOptunaTuner(\n",
    "                                model_name=model_name,\n",
    "                                cv=cv,\n",
    "                                n_trials=CV_N_TRIALS,\n",
    "                                direction=\"maximize\",\n",
    "                                metric=\"f1\"\n",
    "                            )\n",
    "                            \n",
    "                            tuning_result = tuner.tune(\n",
    "                                X=pd.DataFrame(X),\n",
    "                                y=pd.Series(y),\n",
    "                                sample_weights=None,\n",
    "                                param_space=param_space\n",
    "                            )\n",
    "                            \n",
    "                            if not tuning_result.get('skipped', False):\n",
    "                                tuned_params = tuning_result.get('best_params', {})\n",
    "                                best_value = tuning_result.get('best_value', 0.0)\n",
    "                                \n",
    "                                TUNING_RESULTS[model_name] = {\n",
    "                                    'best_params': tuned_params,\n",
    "                                    'best_value': best_value,\n",
    "                                    'n_trials': tuning_result.get('n_trials', 0)\n",
    "                                }\n",
    "                                \n",
    "                                print(f\"    Best F1: {best_value:.4f}\")\n",
    "                                print(f\"    Best params: {tuned_params}\")\n",
    "                            else:\n",
    "                                print(f\"    [Skipped] No tuning support or Optuna not installed\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"    [Warning] Tuning failed: {e}\")\n",
    "                    else:\n",
    "                        print(f\"  [Skipped] No param space defined for {model_name}\")\n",
    "                \n",
    "                # Get model config (use tuned params if available)\n",
    "                try:\n",
    "                    default_config = ModelRegistry.get_model_info(model_name).get('default_config', {})\n",
    "                except:\n",
    "                    default_config = {\n",
    "                        'n_estimators': N_ESTIMATORS,\n",
    "                        'early_stopping_rounds': BOOSTING_EARLY_STOPPING,\n",
    "                    }\n",
    "                \n",
    "                model_config = {**default_config, **tuned_params}\n",
    "                \n",
    "                # Run cross-validation\n",
    "                print(f\"  Running {CV_N_SPLITS}-fold CV...\")\n",
    "                fold_scores = []\n",
    "                fold_details = []\n",
    "                \n",
    "                for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "                    X_train, X_val = X[train_idx], X[val_idx]\n",
    "                    y_train, y_val = y[train_idx], y[val_idx]\n",
    "                    \n",
    "                    # Train model\n",
    "                    model = ModelRegistry.create(model_name, config=model_config)\n",
    "                    model.fit(X_train, y_train, X_val, y_val)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    predictions = model.predict(X_val)\n",
    "                    y_pred = predictions.class_predictions\n",
    "                    \n",
    "                    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "                    acc = accuracy_score(y_val, y_pred)\n",
    "                    prec = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "                    rec = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    fold_scores.append(f1)\n",
    "                    fold_details.append({\n",
    "                        'fold': fold_idx,\n",
    "                        'f1': f1,\n",
    "                        'accuracy': acc,\n",
    "                        'precision': prec,\n",
    "                        'recall': rec,\n",
    "                        'train_size': len(train_idx),\n",
    "                        'val_size': len(val_idx)\n",
    "                    })\n",
    "                    \n",
    "                    del model\n",
    "                    clear_memory()\n",
    "                \n",
    "                # Calculate CV statistics\n",
    "                mean_f1 = np.mean(fold_scores)\n",
    "                std_f1 = np.std(fold_scores)\n",
    "                best_f1 = np.max(fold_scores)\n",
    "                \n",
    "                # Stability grading\n",
    "                if std_f1 < 0.01:\n",
    "                    stability = \"Excellent\"\n",
    "                elif std_f1 < 0.02:\n",
    "                    stability = \"Good\"\n",
    "                elif std_f1 < 0.04:\n",
    "                    stability = \"Fair\"\n",
    "                else:\n",
    "                    stability = \"Poor\"\n",
    "                \n",
    "                # Store results\n",
    "                CV_RESULTS[model_name] = {\n",
    "                    'mean_f1': mean_f1,\n",
    "                    'std_f1': std_f1,\n",
    "                    'best_f1': best_f1,\n",
    "                    'fold_scores': fold_scores,\n",
    "                    'fold_details': fold_details,\n",
    "                    'stability': stability,\n",
    "                    'tuned_params': tuned_params\n",
    "                }\n",
    "                \n",
    "                cv_summary_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'CV Mean F1': mean_f1,\n",
    "                    'CV Std': std_f1,\n",
    "                    'Best F1': best_f1,\n",
    "                    'Stability': stability\n",
    "                })\n",
    "                \n",
    "                print(f\"  Mean F1: {mean_f1:.4f} (+/- {std_f1:.4f})\")\n",
    "                print(f\"  Best F1: {best_f1:.4f}\")\n",
    "                print(f\"  Stability: {stability}\")\n",
    "            \n",
    "            # Display CV summary table\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\" CROSS-VALIDATION SUMMARY\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            cv_summary_df = pd.DataFrame(cv_summary_data)\n",
    "            cv_summary_df = cv_summary_df.sort_values('CV Mean F1', ascending=False)\n",
    "            \n",
    "            # Format for display\n",
    "            print(cv_summary_df.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Cross-validation complete for {len(successful_models)} models\")\n",
    "            print(f\"{'='*70}\")\n",
    "        \n",
    "        del container, X, y\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Cross-validation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.2 Hyperparameter Tuning Results { display-mode: \"form\" }\n",
    "\n",
    "#@markdown Display hyperparameter tuning results and recommendations.\n",
    "\n",
    "show_retrain_recommendation = True  #@param {type: \"boolean\"}\n",
    "show_optimization_plots = False  #@param {type: \"boolean\"}\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'TUNING_RESULTS' not in dir():\n",
    "    TUNING_RESULTS = {}\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'CV_TUNE_HYPERPARAMS' not in dir():\n",
    "    CV_TUNE_HYPERPARAMS = False\n",
    "\n",
    "# Check if tuning was run\n",
    "if not CV_TUNE_HYPERPARAMS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n[Skipped] Hyperparameter tuning not enabled.\")\n",
    "    print(\"\\nTo enable tuning:\")\n",
    "    print(\"  1. Set CV_TUNE_HYPERPARAMS = True in Section 1\")\n",
    "    print(\"  2. Run Cross-Validation (Section 5.1)\")\n",
    "    print(f\"  3. Configure CV_N_TRIALS (currently: {globals().get('CV_N_TRIALS', 20)})\")\n",
    "elif 'TUNING_RESULTS' not in dir() or not TUNING_RESULTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n[No Data] No tuning results available.\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  - Cross-validation hasn't been run yet\")\n",
    "    print(\"  - No models have param spaces defined\")\n",
    "    print(\"  - Optuna is not installed\")\n",
    "    print(\"\\nRun Section 5.1 (Cross-Validation) first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not TUNING_RESULTS:\n",
    "        print(\"\\n[No Results] Tuning enabled but no models were tuned.\")\n",
    "        print(\"\\nModels with tuning support:\")\n",
    "        from src.cross_validation.param_spaces import PARAM_SPACES\n",
    "        supported_models = list(PARAM_SPACES.keys())\n",
    "        print(f\"  {', '.join(supported_models)}\")\n",
    "    else:\n",
    "        print(f\"\\nTuned {len(TUNING_RESULTS)} model(s)\")\n",
    "        print(f\"Trials per model: {globals().get('CV_N_TRIALS', 20)}\\n\")\n",
    "        \n",
    "        # Display results for each model\n",
    "        for model_name, results in TUNING_RESULTS.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\" {model_name.upper()}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            best_params = results.get('best_params', {})\n",
    "            best_value = results.get('best_value', 0.0)\n",
    "            n_trials = results.get('n_trials', 0)\n",
    "            \n",
    "            print(f\"\\nOptimization Summary:\")\n",
    "            print(f\"  Trials completed: {n_trials}\")\n",
    "            print(f\"  Best F1 score:    {best_value:.4f}\")\n",
    "            \n",
    "            if best_params:\n",
    "                print(f\"\\n  Best Parameters:\")\n",
    "                \n",
    "                # Get default params for comparison\n",
    "                try:\n",
    "                    from src.models import ModelRegistry\n",
    "                    model_info = ModelRegistry.get_model_info(model_name)\n",
    "                    default_config = model_info.get('default_config', {})\n",
    "                except:\n",
    "                    default_config = {}\n",
    "                \n",
    "                # Create parameter comparison table\n",
    "                param_data = []\n",
    "                for param_name, tuned_value in best_params.items():\n",
    "                    default_value = default_config.get(param_name, None)\n",
    "                    \n",
    "                    # Calculate change\n",
    "                    if default_value is not None:\n",
    "                        if isinstance(tuned_value, (int, float)) and isinstance(default_value, (int, float)):\n",
    "                            change_pct = ((tuned_value - default_value) / default_value * 100) if default_value != 0 else 0\n",
    "                            change_str = f\"{change_pct:+.1f}%\"\n",
    "                        else:\n",
    "                            change_str = \"changed\"\n",
    "                    else:\n",
    "                        change_str = \"new\"\n",
    "                    \n",
    "                    param_data.append({\n",
    "                        'Parameter': param_name,\n",
    "                        'Default': str(default_value) if default_value is not None else 'N/A',\n",
    "                        'Tuned': str(tuned_value),\n",
    "                        'Change': change_str\n",
    "                    })\n",
    "                \n",
    "                if param_data:\n",
    "                    param_df = pd.DataFrame(param_data)\n",
    "                    print(\"\\n\" + param_df.to_string(index=False))\n",
    "                else:\n",
    "                    for param_name, value in best_params.items():\n",
    "                        print(f\"    {param_name}: {value}\")\n",
    "            \n",
    "            # Calculate improvement over default\n",
    "            if model_name in TRAINING_RESULTS:\n",
    "                default_f1 = TRAINING_RESULTS[model_name].get('metrics', {}).get('macro_f1', 0.0)\n",
    "                improvement = ((best_value - default_f1) / default_f1 * 100) if default_f1 > 0 else 0\n",
    "                \n",
    "                print(f\"\\n  Improvement Analysis:\")\n",
    "                print(f\"    Default F1:     {default_f1:.4f}\")\n",
    "                print(f\"    Tuned F1:       {best_value:.4f}\")\n",
    "                print(f\"    Improvement:    {improvement:+.2f}%\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Show retrain recommendations\n",
    "        if show_retrain_recommendation:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\" RETRAIN RECOMMENDATIONS\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            recommendations = []\n",
    "            for model_name, results in TUNING_RESULTS.items():\n",
    "                best_value = results.get('best_value', 0.0)\n",
    "                \n",
    "                # Calculate improvement\n",
    "                if model_name in TRAINING_RESULTS:\n",
    "                    default_f1 = TRAINING_RESULTS[model_name].get('metrics', {}).get('macro_f1', 0.0)\n",
    "                    improvement = ((best_value - default_f1) / default_f1 * 100) if default_f1 > 0 else 0\n",
    "                    \n",
    "                    recommendations.append({\n",
    "                        'Model': model_name,\n",
    "                        'Default F1': default_f1,\n",
    "                        'Tuned F1': best_value,\n",
    "                        'Improvement': improvement,\n",
    "                        'Action': 'RETRAIN' if improvement > 2.0 else 'Optional'\n",
    "                    })\n",
    "            \n",
    "            if recommendations:\n",
    "                rec_df = pd.DataFrame(recommendations)\n",
    "                rec_df = rec_df.sort_values('Improvement', ascending=False)\n",
    "                \n",
    "                # Format for display\n",
    "                rec_df['Default F1'] = rec_df['Default F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "                rec_df['Tuned F1'] = rec_df['Tuned F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "                rec_df['Improvement'] = rec_df['Improvement'].apply(lambda x: f\"{x:+.2f}%\")\n",
    "                \n",
    "                print(rec_df.to_string(index=False))\n",
    "                \n",
    "                # Highlight high-priority retrains\n",
    "                high_priority = [r for r in recommendations if r['Improvement'] > 2.0]\n",
    "                if high_priority:\n",
    "                    print(f\"\\n\u26a0 HIGH PRIORITY: {len(high_priority)} model(s) show >2% improvement:\")\n",
    "                    for rec in high_priority:\n",
    "                        print(f\"  - {rec['Model']}: {rec['Improvement']:+.2f}% improvement\")\n",
    "                    print(\"\\n  Recommendation: Retrain these models with tuned parameters\")\n",
    "                else:\n",
    "                    print(\"\\n\u2713 All models performing near-optimally with default parameters\")\n",
    "            else:\n",
    "                print(\"No comparison data available (models not trained with defaults)\")\n",
    "        \n",
    "        # Optimization plots (optional)\n",
    "        if show_optimization_plots:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\" OPTIMIZATION HISTORY\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            print(\"[Info] Optimization plots require Optuna visualization.\")\n",
    "            print(\"      In Colab, install: !pip install optuna plotly\")\n",
    "            print(\"      Then re-run this cell to see optimization history.\")\n",
    "            \n",
    "            try:\n",
    "                import optuna\n",
    "                print(\"\\n\u2713 Optuna available - plots can be generated\")\n",
    "                print(\"  (Full plot integration coming in next update)\")\n",
    "            except ImportError:\n",
    "                print(\"\\n\u2717 Optuna not installed - plots unavailable\")\n",
    "        \n",
    "        # Save tuning results\n",
    "        try:\n",
    "            if 'EXPERIMENTS_DIR' in dir():\n",
    "                tuning_results_path = EXPERIMENTS_DIR / 'tuning_results.json'\n",
    "                import json\n",
    "                with open(tuning_results_path, 'w') as f:\n",
    "                    json.dump(TUNING_RESULTS, f, indent=2, default=str)\n",
    "                print(f\"\\n[Saved] Tuning results: {tuning_results_path}\")\n",
    "        except Exception as e:\n",
    "            pass  # Silently skip if can't save\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Hyperparameter tuning analysis complete\")\n",
    "        print(f\"{'='*70}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. PHASE 4: ENSEMBLE (Optional)\n",
    "\n",
    "Combine multiple models for improved predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Train Ensemble { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#@markdown ### Ensemble Training Options\n",
    "show_base_model_validation = True  #@param {type: \"boolean\"}\n",
    "filter_by_cv_stability = False  #@param {type: \"boolean\"}\n",
    "show_ensemble_comparison = True  #@param {type: \"boolean\"}\n",
    "min_diversity_threshold = 0.1  #@param {type: \"number\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "# Define clear_memory if not available\n",
    "if 'clear_memory' not in dir():\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        if GPU_AVAILABLE:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize ensemble results dict\n",
    "ENSEMBLE_RESULTS = {}\n",
    "\n",
    "# Filter out failed models from ensemble base models\n",
    "successful_models = [\n",
    "    model for model, data in TRAINING_RESULTS.items()\n",
    "    if data.get('run_id') != 'failed' and data.get('metrics')\n",
    "]\n",
    "\n",
    "# Check if any ensemble is enabled\n",
    "any_ensemble_enabled = TRAIN_VOTING or TRAIN_STACKING or TRAIN_BLENDING\n",
    "\n",
    "if not any_ensemble_enabled:\n",
    "    print(\"[Skipped] No ensemble training enabled.\")\n",
    "    print(\"Enable TRAIN_VOTING, TRAIN_STACKING, or TRAIN_BLENDING in Section 1.\")\n",
    "elif len(successful_models) < 2:\n",
    "    print(\"[Error] Need at least 2 successfully trained models for ensemble.\")\n",
    "    print(f\"Successfully trained: {successful_models}\")\n",
    "    if len(TRAINING_RESULTS) > len(successful_models):\n",
    "        failed = [m for m in TRAINING_RESULTS if m not in successful_models]\n",
    "        print(f\"Failed models (excluded): {failed}\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 4: ENSEMBLE TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Helper function to parse base models and validate\n",
    "    def parse_and_validate_base_models(base_models_str, ensemble_name):\n",
    "        \"\"\"Parse comma-separated base models and validate availability.\"\"\"\n",
    "        # Parse base models\n",
    "        base_model_names = [m.strip() for m in base_models_str.split(',') if m.strip()]\n",
    "        \n",
    "        if show_base_model_validation:\n",
    "            print(f\"\\n[{ensemble_name.upper()}] Base Model Validation:\")\n",
    "            print(f\"  Requested: {base_model_names}\")\n",
    "        \n",
    "        # Validate: only use successfully trained models\n",
    "        valid_base_models = [\n",
    "            m for m in base_model_names \n",
    "            if m in successful_models\n",
    "        ]\n",
    "        \n",
    "        invalid_models = [m for m in base_model_names if m not in successful_models]\n",
    "        if invalid_models:\n",
    "            print(f\"  \u26a0 Skipped (not trained/failed): {invalid_models}\")\n",
    "        \n",
    "        # Optionally filter by CV stability\n",
    "        if filter_by_cv_stability and 'CV_RESULTS' in dir() and CV_RESULTS:\n",
    "            stable_models = [\n",
    "                m for m in valid_base_models\n",
    "                if m in CV_RESULTS and CV_RESULTS[m].get('stability') in ['Excellent', 'Good']\n",
    "            ]\n",
    "            if len(stable_models) < len(valid_base_models):\n",
    "                unstable = [m for m in valid_base_models if m not in stable_models]\n",
    "                print(f\"  \u26a0 Filtered (low CV stability): {unstable}\")\n",
    "                valid_base_models = stable_models\n",
    "        \n",
    "        if show_base_model_validation:\n",
    "            print(f\"  \u2713 Valid base models: {valid_base_models}\")\n",
    "        \n",
    "        return valid_base_models\n",
    "    \n",
    "    # Helper function to parse weights\n",
    "    def parse_weights(weights_str):\n",
    "        \"\"\"Parse comma-separated weights string into list of floats.\"\"\"\n",
    "        if not weights_str or not weights_str.strip():\n",
    "            return None\n",
    "        try:\n",
    "            weights = [float(w.strip()) for w in weights_str.split(',')]\n",
    "            return weights\n",
    "        except ValueError:\n",
    "            print(f\"  \u26a0 Invalid weights format: {weights_str}\")\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        from src.models.trainer import Trainer\n",
    "        from src.models.config import TrainerConfig\n",
    "        \n",
    "        # Load data container\n",
    "        print(f\"\\n[Data Loading]\")\n",
    "        print(f\"  Splits directory: {SPLITS_DIR}\")\n",
    "        print(f\"  Horizon: {TRAINING_HORIZON}\")\n",
    "        \n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(path=SPLITS_DIR, horizon=TRAINING_HORIZON)\n",
    "        print(f\"  \u2713 Loaded: {container.X_train.shape[0]:,} train samples\")\n",
    "        \n",
    "        # ===================================================================\n",
    "        # TRAIN VOTING ENSEMBLE\n",
    "        # ===================================================================\n",
    "        if TRAIN_VOTING:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" VOTING ENSEMBLE\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            valid_voting_models = parse_and_validate_base_models(\n",
    "                VOTING_BASE_MODELS, \n",
    "                \"voting\"\n",
    "            )\n",
    "            \n",
    "            if len(valid_voting_models) < 2:\n",
    "                print(f\"  \u2717 Need at least 2 valid base models (got {len(valid_voting_models)})\")\n",
    "                print(\"  Skipping Voting ensemble.\")\n",
    "            else:\n",
    "                # Parse weights if provided\n",
    "                weights = parse_weights(VOTING_WEIGHTS) if VOTING_WEIGHTS else None\n",
    "                if weights and len(weights) != len(valid_voting_models):\n",
    "                    print(f\"  \u26a0 Weights count ({len(weights)}) != models count ({len(valid_voting_models)})\")\n",
    "                    print(\"  Using equal weights instead.\")\n",
    "                    weights = None\n",
    "                \n",
    "                # Create config\n",
    "                voting_config = TrainerConfig(\n",
    "                    model_name='voting',\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    model_config={\n",
    "                        'base_model_names': valid_voting_models,\n",
    "                        'voting_type': 'soft',  # Soft voting (avg probabilities)\n",
    "                        'weights': weights\n",
    "                    },\n",
    "                    device='cuda' if GPU_AVAILABLE else 'cpu'\n",
    "                )\n",
    "                \n",
    "                print(f\"\\n  Base models: {valid_voting_models}\")\n",
    "                if weights:\n",
    "                    print(f\"  Weights: {weights}\")\n",
    "                else:\n",
    "                    print(f\"  Weights: Equal (1/{len(valid_voting_models)})\")\n",
    "                print(f\"  Voting type: soft\")\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(voting_config)\n",
    "                print(\"\\n  Training Voting ensemble...\")\n",
    "                results = trainer.run(container)\n",
    "                \n",
    "                # Store results\n",
    "                ENSEMBLE_RESULTS['voting'] = results\n",
    "                \n",
    "                # Display metrics\n",
    "                metrics = results['metrics']\n",
    "                print(f\"\\n  \u2713 Voting Ensemble Results:\")\n",
    "                print(f\"     Accuracy:  {metrics['accuracy']:.2%}\")\n",
    "                print(f\"     Macro F1:  {metrics['macro_f1']:.4f}\")\n",
    "                print(f\"     Precision: {metrics['macro_precision']:.4f}\")\n",
    "                print(f\"     Recall:    {metrics['macro_recall']:.4f}\")\n",
    "                \n",
    "                clear_memory()\n",
    "        \n",
    "        # ===================================================================\n",
    "        # TRAIN STACKING ENSEMBLE\n",
    "        # ===================================================================\n",
    "        if TRAIN_STACKING:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" STACKING ENSEMBLE\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            valid_stacking_models = parse_and_validate_base_models(\n",
    "                STACKING_BASE_MODELS, \n",
    "                \"stacking\"\n",
    "            )\n",
    "            \n",
    "            if len(valid_stacking_models) < 2:\n",
    "                print(f\"  \u2717 Need at least 2 valid base models (got {len(valid_stacking_models)})\")\n",
    "                print(\"  Skipping Stacking ensemble.\")\n",
    "            else:\n",
    "                # Create config\n",
    "                stacking_config = TrainerConfig(\n",
    "                    model_name='stacking',\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    model_config={\n",
    "                        'base_model_names': valid_stacking_models,\n",
    "                        'meta_learner': STACKING_META_LEARNER,\n",
    "                        'n_folds': STACKING_N_FOLDS,\n",
    "                        'use_probas': True  # Use class probabilities\n",
    "                    },\n",
    "                    device='cuda' if GPU_AVAILABLE else 'cpu'\n",
    "                )\n",
    "                \n",
    "                print(f\"\\n  Base models: {valid_stacking_models}\")\n",
    "                print(f\"  Meta-learner: {STACKING_META_LEARNER}\")\n",
    "                print(f\"  CV folds: {STACKING_N_FOLDS}\")\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(stacking_config)\n",
    "                print(\"\\n  Training Stacking ensemble...\")\n",
    "                print(\"  (Generating out-of-fold predictions...)\")\n",
    "                results = trainer.run(container)\n",
    "                \n",
    "                # Store results\n",
    "                ENSEMBLE_RESULTS['stacking'] = results\n",
    "                \n",
    "                # Display metrics\n",
    "                metrics = results['metrics']\n",
    "                print(f\"\\n  \u2713 Stacking Ensemble Results:\")\n",
    "                print(f\"     Accuracy:  {metrics['accuracy']:.2%}\")\n",
    "                print(f\"     Macro F1:  {metrics['macro_f1']:.4f}\")\n",
    "                print(f\"     Precision: {metrics['macro_precision']:.4f}\")\n",
    "                print(f\"     Recall:    {metrics['macro_recall']:.4f}\")\n",
    "                \n",
    "                clear_memory()\n",
    "        \n",
    "        # ===================================================================\n",
    "        # TRAIN BLENDING ENSEMBLE\n",
    "        # ===================================================================\n",
    "        if TRAIN_BLENDING:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" BLENDING ENSEMBLE\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            valid_blending_models = parse_and_validate_base_models(\n",
    "                BLENDING_BASE_MODELS, \n",
    "                \"blending\"\n",
    "            )\n",
    "            \n",
    "            if len(valid_blending_models) < 2:\n",
    "                print(f\"  \u2717 Need at least 2 valid base models (got {len(valid_blending_models)})\")\n",
    "                print(\"  Skipping Blending ensemble.\")\n",
    "            else:\n",
    "                # Create config\n",
    "                blending_config = TrainerConfig(\n",
    "                    model_name='blending',\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    model_config={\n",
    "                        'base_model_names': valid_blending_models,\n",
    "                        'meta_learner': BLENDING_META_LEARNER,\n",
    "                        'holdout_ratio': BLENDING_HOLDOUT_RATIO\n",
    "                    },\n",
    "                    device='cuda' if GPU_AVAILABLE else 'cpu'\n",
    "                )\n",
    "                \n",
    "                print(f\"\\n  Base models: {valid_blending_models}\")\n",
    "                print(f\"  Meta-learner: {BLENDING_META_LEARNER}\")\n",
    "                print(f\"  Holdout ratio: {BLENDING_HOLDOUT_RATIO:.0%}\")\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(blending_config)\n",
    "                print(\"\\n  Training Blending ensemble...\")\n",
    "                print(\"  (Using holdout set for meta-learner...)\")\n",
    "                results = trainer.run(container)\n",
    "                \n",
    "                # Store results\n",
    "                ENSEMBLE_RESULTS['blending'] = results\n",
    "                \n",
    "                # Display metrics\n",
    "                metrics = results['metrics']\n",
    "                print(f\"\\n  \u2713 Blending Ensemble Results:\")\n",
    "                print(f\"     Accuracy:  {metrics['accuracy']:.2%}\")\n",
    "                print(f\"     Macro F1:  {metrics['macro_f1']:.4f}\")\n",
    "                print(f\"     Precision: {metrics['macro_precision']:.4f}\")\n",
    "                print(f\"     Recall:    {metrics['macro_recall']:.4f}\")\n",
    "                \n",
    "                clear_memory()\n",
    "        \n",
    "        # ===================================================================\n",
    "        # ENSEMBLE vs BASE MODEL COMPARISON\n",
    "        # ===================================================================\n",
    "        if show_ensemble_comparison and ENSEMBLE_RESULTS:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" ENSEMBLE PERFORMANCE COMPARISON\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "                ensemble_f1 = results['metrics']['macro_f1']\n",
    "                ensemble_acc = results['metrics']['accuracy']\n",
    "                \n",
    "                # Get base model names from config\n",
    "                base_model_names = results['config']['base_model_names']\n",
    "                \n",
    "                # Find best base model\n",
    "                base_f1_scores = {\n",
    "                    m: TRAINING_RESULTS[m]['metrics']['macro_f1'] \n",
    "                    for m in base_model_names\n",
    "                }\n",
    "                best_base_model = max(base_f1_scores, key=base_f1_scores.get)\n",
    "                best_base_f1 = base_f1_scores[best_base_model]\n",
    "                best_base_acc = TRAINING_RESULTS[best_base_model]['metrics']['accuracy']\n",
    "                \n",
    "                # Calculate improvements\n",
    "                f1_improvement = (ensemble_f1 - best_base_f1) / best_base_f1 * 100\n",
    "                acc_improvement = (ensemble_acc - best_base_acc) / best_base_acc * 100\n",
    "                \n",
    "                print(f\"\\n[{ensemble_name.upper()}]\")\n",
    "                print(f\"  Ensemble F1:    {ensemble_f1:.4f}\")\n",
    "                print(f\"  Best Base F1:   {best_base_f1:.4f} ({best_base_model})\")\n",
    "                print(f\"  F1 Improvement: {f1_improvement:+.2f}%\")\n",
    "                print(f\"  Acc Improvement: {acc_improvement:+.2f}%\")\n",
    "                \n",
    "                if f1_improvement > 0:\n",
    "                    print(f\"  \u2713 Ensemble outperforms best base model\")\n",
    "                elif f1_improvement > -1:\n",
    "                    print(f\"  \u2248 Ensemble comparable to best base model\")\n",
    "                else:\n",
    "                    print(f\"  \u26a0 Ensemble underperforms best base model\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\" ENSEMBLE TRAINING COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\n  Ensembles trained: {len(ENSEMBLE_RESULTS)}\")\n",
    "        if ENSEMBLE_RESULTS:\n",
    "            print(f\"  Available: {list(ENSEMBLE_RESULTS.keys())}\")\n",
    "            print(\"\\n  \u2713 Results stored in ENSEMBLE_RESULTS dict\")\n",
    "            print(\"  \u2713 Ready for ensemble analysis in next cell\")\n",
    "        else:\n",
    "            print(\"  No ensembles successfully trained.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u2717 Ensemble training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        ENSEMBLE_RESULTS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.2 Ensemble Analysis & Diversity { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#@markdown ### Analysis Options\n",
    "show_diversity_metrics = True  #@param {type: \"boolean\"}\n",
    "show_base_contributions = True  #@param {type: \"boolean\"}\n",
    "show_disagreement_analysis = False  #@param {type: \"boolean\"}\n",
    "plot_contribution_charts = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables\n",
    "if 'ENSEMBLE_RESULTS' not in dir():\n",
    "    ENSEMBLE_RESULTS = {}\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if 'IS_COLAB' in dir() and IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "\n",
    "\n",
    "if not ENSEMBLE_RESULTS or not ENSEMBLE_RESULTS:\n",
    "    print(\"[Skipped] No ensemble models trained.\")\n",
    "    print(\"Enable TRAIN_VOTING, TRAIN_STACKING, or TRAIN_BLENDING in Section 1\")\n",
    "    print(\"and run Cell 6.1 to train ensembles.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" ENSEMBLE ANALYSIS & DIVERSITY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        # Load data for predictions\n",
    "        if 'SPLITS_DIR' not in dir():\n",
    "            PROJECT_ROOT = get_project_root()\n",
    "            SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "        \n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(path=SPLITS_DIR, horizon=TRAINING_HORIZON)\n",
    "        \n",
    "        # ===================================================================\n",
    "        # DIVERSITY ANALYSIS\n",
    "        # ===================================================================\n",
    "        if show_diversity_metrics:\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(\" DIVERSITY METRICS\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "                print(f\"\\n[{ensemble_name.upper()}]\")\n",
    "                \n",
    "                base_model_names = results['config']['base_model_names']\n",
    "                print(f\"  Base models: {base_model_names}\")\n",
    "                \n",
    "                # Get predictions from each base model on validation set\n",
    "                base_predictions = {}\n",
    "                for model_name in base_model_names:\n",
    "                    if model_name in TRAINING_RESULTS:\n",
    "                        model_result = TRAINING_RESULTS[model_name]\n",
    "                        if 'val_predictions' in model_result:\n",
    "                            base_predictions[model_name] = model_result['val_predictions']\n",
    "                \n",
    "                if len(base_predictions) >= 2:\n",
    "                    # Calculate pairwise agreement\n",
    "                    model_names = list(base_predictions.keys())\n",
    "                    n_models = len(model_names)\n",
    "                    \n",
    "                    agreements = []\n",
    "                    for i in range(n_models):\n",
    "                        for j in range(i + 1, n_models):\n",
    "                            pred_i = base_predictions[model_names[i]]\n",
    "                            pred_j = base_predictions[model_names[j]]\n",
    "                            agreement = np.mean(pred_i == pred_j)\n",
    "                            agreements.append(agreement)\n",
    "                    \n",
    "                    avg_agreement = np.mean(agreements)\n",
    "                    diversity_score = 1 - avg_agreement\n",
    "                    \n",
    "                    print(f\"\\n  Pairwise Agreement: {avg_agreement:.3f}\")\n",
    "                    print(f\"  Diversity Score:    {diversity_score:.3f}\")\n",
    "                    \n",
    "                    # Interpret diversity\n",
    "                    if diversity_score > 0.3:\n",
    "                        print(f\"  \u2713 Good diversity - models complement each other\")\n",
    "                    elif diversity_score > 0.15:\n",
    "                        print(f\"  \u2248 Moderate diversity - some complementarity\")\n",
    "                    else:\n",
    "                        print(f\"  \u26a0 Low diversity - models may be redundant\")\n",
    "                    \n",
    "                    # Q-statistic (measure of diversity for pairs)\n",
    "                    print(f\"\\n  Pairwise Diversity Details:\")\n",
    "                    idx = 0\n",
    "                    for i in range(n_models):\n",
    "                        for j in range(i + 1, n_models):\n",
    "                            pred_i = base_predictions[model_names[i]]\n",
    "                            pred_j = base_predictions[model_names[j]]\n",
    "                            agreement = np.mean(pred_i == pred_j)\n",
    "                            print(f\"    {model_names[i]} <-> {model_names[j]}: {agreement:.3f} agreement\")\n",
    "                            idx += 1\n",
    "                else:\n",
    "                    print(f\"  \u26a0 Predictions not available for diversity analysis\")\n",
    "        \n",
    "        # ===================================================================\n",
    "        # BASE MODEL CONTRIBUTIONS\n",
    "        # ===================================================================\n",
    "        if show_base_contributions:\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(\" BASE MODEL CONTRIBUTIONS\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            contributions_data = []\n",
    "            \n",
    "            for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "                print(f\"\\n[{ensemble_name.upper()}]\")\n",
    "                \n",
    "                base_model_names = results['config']['base_model_names']\n",
    "                \n",
    "                if ensemble_name == 'voting':\n",
    "                    # For voting: show weights\n",
    "                    weights = results['config'].get('weights')\n",
    "                    if weights:\n",
    "                        print(f\"  Voting weights (explicit):\")\n",
    "                        for model, weight in zip(base_model_names, weights):\n",
    "                            print(f\"    {model}: {weight:.3f}\")\n",
    "                            contributions_data.append({\n",
    "                                'Ensemble': 'Voting',\n",
    "                                'Model': model,\n",
    "                                'Contribution': weight\n",
    "                            })\n",
    "                    else:\n",
    "                        # Equal weights\n",
    "                        weight = 1.0 / len(base_model_names)\n",
    "                        print(f\"  Voting weights (equal):\")\n",
    "                        for model in base_model_names:\n",
    "                            print(f\"    {model}: {weight:.3f}\")\n",
    "                            contributions_data.append({\n",
    "                                'Ensemble': 'Voting',\n",
    "                                'Model': model,\n",
    "                                'Contribution': weight\n",
    "                            })\n",
    "                \n",
    "                elif ensemble_name in ['stacking', 'blending']:\n",
    "                    # For stacking/blending: show meta-learner importance\n",
    "                    # This would require access to meta-learner internals\n",
    "                    # For now, show equal contributions as placeholder\n",
    "                    print(f\"  Meta-learner: {results['config'].get('meta_learner', 'unknown')}\")\n",
    "                    print(f\"  Base model contributions (estimated from performance):\")\n",
    "                    \n",
    "                    # Estimate contribution by individual model F1 scores\n",
    "                    contributions = {}\n",
    "                    for model in base_model_names:\n",
    "                        if model in TRAINING_RESULTS:\n",
    "                            f1 = TRAINING_RESULTS[model]['metrics']['macro_f1']\n",
    "                            contributions[model] = f1\n",
    "                    \n",
    "                    # Normalize to sum to 1\n",
    "                    total = sum(contributions.values())\n",
    "                    if total > 0:\n",
    "                        for model in sorted(contributions, key=contributions.get, reverse=True):\n",
    "                            contrib = contributions[model] / total\n",
    "                            print(f\"    {model}: {contrib:.3f} (based on F1)\")\n",
    "                            contributions_data.append({\n",
    "                                'Ensemble': ensemble_name.capitalize(),\n",
    "                                'Model': model,\n",
    "                                'Contribution': contrib\n",
    "                            })\n",
    "            \n",
    "            # Plot contributions\n",
    "            if plot_contribution_charts and contributions_data:\n",
    "                df_contrib = pd.DataFrame(contributions_data)\n",
    "                \n",
    "                fig, axes = plt.subplots(1, len(ENSEMBLE_RESULTS), figsize=(5 * len(ENSEMBLE_RESULTS), 4))\n",
    "                if len(ENSEMBLE_RESULTS) == 1:\n",
    "                    axes = [axes]\n",
    "                \n",
    "                for idx, (ensemble_name, results) in enumerate(ENSEMBLE_RESULTS.items()):\n",
    "                    ensemble_data = df_contrib[df_contrib['Ensemble'] == ensemble_name.capitalize()]\n",
    "                    \n",
    "                    axes[idx].barh(ensemble_data['Model'], ensemble_data['Contribution'])\n",
    "                    axes[idx].set_xlabel('Contribution')\n",
    "                    axes[idx].set_title(f'{ensemble_name.capitalize()} Ensemble')\n",
    "                    axes[idx].set_xlim(0, max(ensemble_data['Contribution']) * 1.1)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # ===================================================================\n",
    "        # ENSEMBLE COMPARISON TABLE\n",
    "        # ===================================================================\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\" ENSEMBLE COMPARISON TABLE\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        comparison_data = []\n",
    "        for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "            metrics = results['metrics']\n",
    "            base_models = results['config']['base_model_names']\n",
    "            \n",
    "            # Calculate diversity if predictions available\n",
    "            diversity = 0.0\n",
    "            base_predictions = {}\n",
    "            for model_name in base_models:\n",
    "                if model_name in TRAINING_RESULTS:\n",
    "                    if 'val_predictions' in TRAINING_RESULTS[model_name]:\n",
    "                        base_predictions[model_name] = TRAINING_RESULTS[model_name]['val_predictions']\n",
    "            \n",
    "            if len(base_predictions) >= 2:\n",
    "                model_names = list(base_predictions.keys())\n",
    "                n_models = len(model_names)\n",
    "                agreements = []\n",
    "                for i in range(n_models):\n",
    "                    for j in range(i + 1, n_models):\n",
    "                        pred_i = base_predictions[model_names[i]]\n",
    "                        pred_j = base_predictions[model_names[j]]\n",
    "                        agreement = np.mean(pred_i == pred_j)\n",
    "                        agreements.append(agreement)\n",
    "                diversity = 1 - np.mean(agreements)\n",
    "            \n",
    "            # Best base model improvement\n",
    "            base_f1_scores = {\n",
    "                m: TRAINING_RESULTS[m]['metrics']['macro_f1'] \n",
    "                for m in base_models if m in TRAINING_RESULTS\n",
    "            }\n",
    "            if base_f1_scores:\n",
    "                best_base_model = max(base_f1_scores, key=base_f1_scores.get)\n",
    "                best_base_f1 = base_f1_scores[best_base_model]\n",
    "                improvement = (metrics['macro_f1'] - best_base_f1) / best_base_f1 * 100\n",
    "            else:\n",
    "                best_base_model = 'N/A'\n",
    "                improvement = 0.0\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Ensemble': ensemble_name.capitalize(),\n",
    "                'Accuracy': f\"{metrics['accuracy']:.2%}\",\n",
    "                'F1 Score': f\"{metrics['macro_f1']:.4f}\",\n",
    "                'Base Models': len(base_models),\n",
    "                'Diversity': f\"{diversity:.3f}\",\n",
    "                'Best Base': best_base_model,\n",
    "                'Improvement': f\"{improvement:+.2f}%\"\n",
    "            })\n",
    "        \n",
    "        df_comparison = pd.DataFrame(comparison_data)\n",
    "        print(\"\\n\", df_comparison.to_string(index=False))\n",
    "        \n",
    "        # ===================================================================\n",
    "        # RECOMMENDATION\n",
    "        # ===================================================================\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\" RECOMMENDATION\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Find best ensemble by F1\n",
    "        best_ensemble_name = max(\n",
    "            ENSEMBLE_RESULTS, \n",
    "            key=lambda x: ENSEMBLE_RESULTS[x]['metrics']['macro_f1']\n",
    "        )\n",
    "        best_ensemble = ENSEMBLE_RESULTS[best_ensemble_name]\n",
    "        \n",
    "        print(f\"\\n  Best Ensemble: {best_ensemble_name.upper()}\")\n",
    "        print(f\"  Metrics:\")\n",
    "        print(f\"    - Accuracy: {best_ensemble['metrics']['accuracy']:.2%}\")\n",
    "        print(f\"    - F1 Score: {best_ensemble['metrics']['macro_f1']:.4f}\")\n",
    "        print(f\"    - Precision: {best_ensemble['metrics']['macro_precision']:.4f}\")\n",
    "        print(f\"    - Recall: {best_ensemble['metrics']['macro_recall']:.4f}\")\n",
    "        \n",
    "        # Reason\n",
    "        base_models = best_ensemble['config']['base_model_names']\n",
    "        print(f\"\\n  Reason: Highest F1 score among {len(ENSEMBLE_RESULTS)} ensembles\")\n",
    "        print(f\"  Base models: {base_models}\")\n",
    "        \n",
    "        # Check diversity\n",
    "        if show_diversity_metrics:\n",
    "            base_predictions = {}\n",
    "            for model_name in base_models:\n",
    "                if model_name in TRAINING_RESULTS and 'val_predictions' in TRAINING_RESULTS[model_name]:\n",
    "                    base_predictions[model_name] = TRAINING_RESULTS[model_name]['val_predictions']\n",
    "            \n",
    "            if len(base_predictions) >= 2:\n",
    "                model_names = list(base_predictions.keys())\n",
    "                n_models = len(model_names)\n",
    "                agreements = []\n",
    "                for i in range(n_models):\n",
    "                    for j in range(i + 1, n_models):\n",
    "                        pred_i = base_predictions[model_names[i]]\n",
    "                        pred_j = base_predictions[model_names[j]]\n",
    "                        agreement = np.mean(pred_i == pred_j)\n",
    "                        agreements.append(agreement)\n",
    "                diversity_score = 1 - np.mean(agreements)\n",
    "                \n",
    "                if diversity_score > 0.3:\n",
    "                    print(f\"  \u2713 Good diversity ({diversity_score:.3f}) - models complement each other\")\n",
    "                elif diversity_score > 0.15:\n",
    "                    print(f\"  \u2248 Moderate diversity ({diversity_score:.3f})\")\n",
    "                else:\n",
    "                    print(f\"  \u26a0 Low diversity ({diversity_score:.3f}) - consider different base models\")\n",
    "        \n",
    "        # ===================================================================\n",
    "        # DISAGREEMENT ANALYSIS\n",
    "        # ===================================================================\n",
    "        if show_disagreement_analysis:\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(\" DISAGREEMENT ANALYSIS\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "                print(f\"\\n[{ensemble_name.upper()}]\")\n",
    "                \n",
    "                base_model_names = results['config']['base_model_names']\n",
    "                \n",
    "                # Get predictions\n",
    "                base_predictions = {}\n",
    "                for model_name in base_model_names:\n",
    "                    if model_name in TRAINING_RESULTS and 'val_predictions' in TRAINING_RESULTS[model_name]:\n",
    "                        base_predictions[model_name] = TRAINING_RESULTS[model_name]['val_predictions']\n",
    "                \n",
    "                if len(base_predictions) >= 2:\n",
    "                    # Find samples where models disagree\n",
    "                    pred_arrays = [base_predictions[m] for m in base_predictions.keys()]\n",
    "                    pred_matrix = np.array(pred_arrays)\n",
    "                    \n",
    "                    # Check disagreement (not all predictions are the same)\n",
    "                    disagreements = np.apply_along_axis(lambda x: len(np.unique(x)) > 1, axis=0, arr=pred_matrix)\n",
    "                    disagreement_rate = np.mean(disagreements)\n",
    "                    \n",
    "                    print(f\"  Disagreement rate: {disagreement_rate:.2%}\")\n",
    "                    print(f\"  Samples with disagreement: {np.sum(disagreements):,} / {len(disagreements):,}\")\n",
    "                    \n",
    "                    # Show a few examples\n",
    "                    disagreement_indices = np.where(disagreements)[0]\n",
    "                    if len(disagreement_indices) > 0:\n",
    "                        print(f\"\\n  Example disagreements (first 5):\")\n",
    "                        for idx in disagreement_indices[:5]:\n",
    "                            predictions = {m: base_predictions[m][idx] for m in base_predictions.keys()}\n",
    "                            print(f\"    Sample {idx}: {predictions}\")\n",
    "                else:\n",
    "                    print(f\"  \u26a0 Predictions not available\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" ENSEMBLE ANALYSIS COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u2717 Ensemble analysis failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. RESULTS & EXPORT\n",
    "\n",
    "Summary of all results and export options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.1 Final Summary { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" PIPELINE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n Configuration:\")\n",
    "print(f\"   Symbol: {SYMBOL}\")\n",
    "\n",
    "# Show auto-detected date range (with safety checks)\n",
    "if 'DATA_START' in dir() and DATA_START is not None:\n",
    "    print(f\"   Date Range: {DATA_START.strftime('%Y-%m-%d')} to {DATA_END.strftime('%Y-%m-%d')}\")\n",
    "    if 'DATA_START_YEAR' in dir() and 'DATA_END_YEAR' in dir():\n",
    "        print(f\"   Years: {DATA_START_YEAR} - {DATA_END_YEAR}\")\n",
    "else:\n",
    "    print(f\"   Date Range: Not detected (run Section 3.1)\")\n",
    "\n",
    "print(f\"   Training Horizon: H{TRAINING_HORIZON}\")\n",
    "\n",
    "if 'TRAIN_LEN' in dir():\n",
    "    print(f\"\\n Data:\")\n",
    "    print(f\"   Train: {TRAIN_LEN:,} samples\")\n",
    "    if 'VAL_LEN' in dir():\n",
    "        print(f\"   Val: {VAL_LEN:,} samples\")\n",
    "    if 'TEST_LEN' in dir():\n",
    "        print(f\"   Test: {TEST_LEN:,} samples\")\n",
    "\n",
    "if 'TRAINING_RESULTS' in dir() and TRAINING_RESULTS:\n",
    "    print(f\"\\n Model Results:\")\n",
    "    for model, data in sorted(TRAINING_RESULTS.items(), \n",
    "                              key=lambda x: x[1]['metrics'].get('macro_f1', 0), \n",
    "                              reverse=True):\n",
    "        metrics = data['metrics']\n",
    "        print(f\"   {model}: Acc={metrics.get('accuracy', 0):.2%}, F1={metrics.get('macro_f1', 0):.4f}\")\n",
    "    \n",
    "    best = max(TRAINING_RESULTS, key=lambda x: TRAINING_RESULTS[x]['metrics'].get('macro_f1', 0))\n",
    "    print(f\"\\n Best Model: {best}\")\n",
    "\n",
    "print(f\"\\n Saved Artifacts:\")\n",
    "print(f\"   Data: {SPLITS_DIR}\")\n",
    "print(f\"   Models: {EXPERIMENTS_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" PIPELINE COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Export Model Package { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DRIVE EXPORT UTILITY (Colab)\n",
    "# ============================================================================\n",
    "def export_to_drive(results, name: str, subdir: str = \"ml_pipeline_results\"):\n",
    "    \"\"\"\n",
    "    Export results to Google Drive for persistence across sessions.\n",
    "\n",
    "    Args:\n",
    "        results: Any picklable object (dict, DataFrame, model, etc.)\n",
    "        name: Base name for the export file\n",
    "        subdir: Subdirectory in Drive to save to\n",
    "\n",
    "    Returns:\n",
    "        Path to exported file, or None if not in Colab/Drive not mounted\n",
    "    \"\"\"\n",
    "    if not IS_COLAB or not USE_DRIVE:\n",
    "        print(\"[Export] Not in Colab or Drive not mounted, skipping Drive export\")\n",
    "        return None\n",
    "\n",
    "    export_dir = Path('/content/drive/MyDrive') / subdir\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = export_dir / f\"{name}_{timestamp}.pkl\"\n",
    "\n",
    "    joblib.dump(results, filepath)\n",
    "    print(f\"[Export] Results saved to Drive: {filepath}\")\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "# Project root helper (uses configuration from Section 1)\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root - works in Colab and locally.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        return Path(COLAB_PROJECT_PATH) if 'COLAB_PROJECT_PATH' in dir() else Path('/content/research')\n",
    "    elif 'LOCAL_PROJECT_ROOT' in dir() and LOCAL_PROJECT_ROOT:\n",
    "        return Path(LOCAL_PROJECT_ROOT)\n",
    "    else:\n",
    "        return Path.cwd()\n",
    "\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'RESULTS_DIR' not in dir():\n",
    "    RESULTS_DIR = PROJECT_ROOT / 'experiments'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'TEST_RESULTS' not in dir():\n",
    "    TEST_RESULTS = {}\n",
    "if 'CV_RESULTS' not in dir():\n",
    "    CV_RESULTS = {}\n",
    "if 'ENSEMBLE_RESULTS' not in dir():\n",
    "    ENSEMBLE_RESULTS = {}\n",
    "\n",
    "#@markdown ### Export Configuration\n",
    "\n",
    "export_model = False  #@param {type: \"boolean\"}\n",
    "#@markdown Enable to export model package\n",
    "\n",
    "export_selection = \"Best Model\"  #@param [\"Best Model\", \"All Models\", \"Ensembles Only\", \"Top 3 Models\", \"Custom Selection\"]\n",
    "#@markdown Select which models to export\n",
    "\n",
    "custom_models_to_export = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated model names (only used if Custom Selection)\n",
    "\n",
    "export_format = \"Standard Package\"  #@param [\"Standard Package\", \"Production Bundle\", \"Research Archive\", \"Minimal (Model Only)\"]\n",
    "#@markdown Export package type\n",
    "\n",
    "#@markdown ### Export Options\n",
    "\n",
    "include_onnx = False  #@param {type: \"boolean\"}\n",
    "#@markdown Export to ONNX format for production (XGBoost, LightGBM, CatBoost only)\n",
    "\n",
    "include_predictions = True  #@param {type: \"boolean\"}\n",
    "#@markdown Include validation and test predictions\n",
    "\n",
    "include_visualizations = True  #@param {type: \"boolean\"}\n",
    "#@markdown Include generated plots and charts\n",
    "\n",
    "include_model_card = True  #@param {type: \"boolean\"}\n",
    "#@markdown Generate model cards with performance details\n",
    "\n",
    "create_zip_archive = True  #@param {type: \"boolean\"}\n",
    "#@markdown Create ZIP archive of export package\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_models_to_export():\n",
    "    \"\"\"Determine which models to export based on selection.\"\"\"\n",
    "    all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "\n",
    "    if not all_results:\n",
    "        return []\n",
    "\n",
    "    if export_selection == \"Best Model\":\n",
    "        best_model = max(all_results, key=lambda x: all_results[x]['metrics'].get('macro_f1', 0))\n",
    "        return [best_model]\n",
    "\n",
    "    elif export_selection == \"All Models\":\n",
    "        return list(all_results.keys())\n",
    "\n",
    "    elif export_selection == \"Ensembles Only\":\n",
    "        return [m for m in all_results.keys() if 'ensemble' in m or 'voting' in m or 'stacking' in m or 'blending' in m]\n",
    "\n",
    "    elif export_selection == \"Top 3 Models\":\n",
    "        sorted_models = sorted(all_results.items(), key=lambda x: x[1]['metrics'].get('macro_f1', 0), reverse=True)\n",
    "        return [m[0] for m in sorted_models[:3]]\n",
    "\n",
    "    elif export_selection == \"Custom Selection\":\n",
    "        if not custom_models_to_export:\n",
    "            print(\"\u26a0 Custom selection requires model names in 'custom_models_to_export'\")\n",
    "            return []\n",
    "        models = [m.strip() for m in custom_models_to_export.split(',')]\n",
    "        valid_models = [m for m in models if m in all_results]\n",
    "        if len(valid_models) < len(models):\n",
    "            invalid = set(models) - set(valid_models)\n",
    "            print(f\"\u26a0 Invalid models: {invalid}\")\n",
    "        return valid_models\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_model_card(model_name, model_info, test_info=None, cv_info=None):\n",
    "    \"\"\"Generate model card in Markdown format.\"\"\"\n",
    "    metrics = model_info.get('metrics', {})\n",
    "    config = model_info.get('config', {})\n",
    "\n",
    "    card = f\"\"\"# Model Card: {model_name.upper()}\n",
    "\n",
    "## Model Information\n",
    "- **Type:** {model_info.get('model_type', 'Unknown')}\n",
    "- **Symbol:** {SYMBOL if 'SYMBOL' in dir() else 'N/A'}\n",
    "- **Horizon:** {TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A'} bars\n",
    "- **Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Run ID:** {model_info.get('run_id', 'Unknown')}\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Validation Set\n",
    "- **Accuracy:** {metrics.get('accuracy', 0):.4f}\n",
    "- **Macro F1:** {metrics.get('macro_f1', 0):.4f}\n",
    "- **Precision:** {metrics.get('precision', 0):.4f}\n",
    "- **Recall:** {metrics.get('recall', 0):.4f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add test results if available\n",
    "    if test_info:\n",
    "        test_metrics = test_info.get('metrics', {})\n",
    "        val_f1 = metrics.get('macro_f1', 0)\n",
    "        test_f1 = test_metrics.get('macro_f1', 0)\n",
    "        gap = ((test_f1 - val_f1) / val_f1 * 100) if val_f1 > 0 else 0\n",
    "\n",
    "        card += f\"\"\"### Test Set\n",
    "- **Accuracy:** {test_metrics.get('accuracy', 0):.4f}\n",
    "- **Macro F1:** {test_metrics.get('macro_f1', 0):.4f}\n",
    "- **Precision:** {test_metrics.get('precision', 0):.4f}\n",
    "- **Recall:** {test_metrics.get('recall', 0):.4f}\n",
    "- **Generalization Gap:** {gap:+.2f}%\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add CV results if available\n",
    "    if cv_info:\n",
    "        cv_metrics = cv_info.get('cv_metrics', {})\n",
    "        card += f\"\"\"### Cross-Validation\n",
    "- **Mean F1:** {cv_metrics.get('mean_f1', 0):.4f} \u00b1 {cv_metrics.get('std_f1', 0):.4f}\n",
    "- **Mean Accuracy:** {cv_metrics.get('mean_accuracy', 0):.4f} \u00b1 {cv_metrics.get('std_accuracy', 0):.4f}\n",
    "- **Folds:** {cv_info.get('n_splits', 'N/A')}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add configuration\n",
    "    if config:\n",
    "        card += f\"\"\"## Configuration\n",
    "\n",
    "```json\n",
    "{json.dumps(config, indent=2)}\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add feature information\n",
    "    if 'feature_importance' in model_info:\n",
    "        importance = model_info['feature_importance']\n",
    "        top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        card += f\"\"\"## Top 10 Features\n",
    "\n",
    "\"\"\"\n",
    "        for i, (feature, score) in enumerate(top_features, 1):\n",
    "            card += f\"{i}. **{feature}**: {score:.4f}\\n\"\n",
    "        card += \"\\n\"\n",
    "\n",
    "    # Add training details\n",
    "    train_time = model_info.get('training_time_sec', 0)\n",
    "    card += f\"\"\"## Training Details\n",
    "- **Training Time:** {train_time:.2f}s\n",
    "- **Model Size:** {model_info.get('model_size_mb', 'N/A')} MB\n",
    "- **Framework:** {model_info.get('framework', 'Unknown')}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    return card\n",
    "\n",
    "\n",
    "def export_to_onnx(model, model_name, model_path, feature_names):\n",
    "    \"\"\"Export model to ONNX format (boosting models only).\"\"\"\n",
    "    try:\n",
    "        # Check if model type supports ONNX\n",
    "        onnx_compatible = ['xgboost', 'lightgbm', 'catboost']\n",
    "        if not any(m in model_name.lower() for m in onnx_compatible):\n",
    "            return False, \"Model type not compatible with ONNX\"\n",
    "\n",
    "        # Try to import ONNX libraries\n",
    "        try:\n",
    "            from skl2onnx import convert_sklearn\n",
    "            from skl2onnx.common.data_types import FloatTensorType\n",
    "            import onnx\n",
    "        except ImportError:\n",
    "            return False, \"ONNX libraries not installed (skl2onnx, onnx)\"\n",
    "\n",
    "        # Load the model\n",
    "        loaded_model = joblib.load(model_path)\n",
    "\n",
    "        # Determine number of features\n",
    "        n_features = len(feature_names) if feature_names else 150\n",
    "\n",
    "        # Define input type\n",
    "        initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "\n",
    "        # Convert to ONNX\n",
    "        onnx_model = convert_sklearn(loaded_model, initial_types=initial_type)\n",
    "\n",
    "        # Save ONNX model\n",
    "        onnx_path = model_path.parent / 'model.onnx'\n",
    "        with open(onnx_path, 'wb') as f:\n",
    "            f.write(onnx_model.SerializeToString())\n",
    "\n",
    "        # Get file size\n",
    "        size_mb = onnx_path.stat().st_size / 1e6\n",
    "\n",
    "        return True, f\"ONNX export successful ({size_mb:.2f} MB)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"ONNX export failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def create_manifest(export_dir, models_exported, export_info):\n",
    "    \"\"\"Create manifest.json with export metadata.\"\"\"\n",
    "    all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "\n",
    "    # Find best model\n",
    "    best_model = max(all_results, key=lambda x: all_results[x]['metrics'].get('macro_f1', 0))\n",
    "    best_f1 = all_results[best_model]['metrics'].get('macro_f1', 0)\n",
    "\n",
    "    # Collect model formats\n",
    "    formats = {}\n",
    "    for model_name in models_exported:\n",
    "        model_formats = ['pkl']\n",
    "        onnx_path = export_dir / 'models' / model_name / 'model.onnx'\n",
    "        if onnx_path.exists():\n",
    "            model_formats.append('onnx')\n",
    "        formats[model_name] = model_formats\n",
    "\n",
    "    manifest = {\n",
    "        'export_timestamp': datetime.now().isoformat(),\n",
    "        'symbol': SYMBOL if 'SYMBOL' in dir() else 'N/A',\n",
    "        'horizon': TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A',\n",
    "        'models_exported': models_exported,\n",
    "        'best_model': best_model,\n",
    "        'best_test_f1': best_f1,\n",
    "        'export_format': export_format,\n",
    "        'formats': formats,\n",
    "        'data_stats': export_info.get('data_stats', {}),\n",
    "        'export_options': {\n",
    "            'include_onnx': include_onnx,\n",
    "            'include_predictions': include_predictions,\n",
    "            'include_visualizations': include_visualizations,\n",
    "            'include_model_card': include_model_card\n",
    "        }\n",
    "    }\n",
    "\n",
    "    manifest_path = export_dir / 'manifest.json'\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    return manifest_path\n",
    "\n",
    "\n",
    "def create_readme(export_dir, models_exported):\n",
    "    \"\"\"Create README.md with setup and usage instructions.\"\"\"\n",
    "    readme = f\"\"\"# ML Model Export Package\n",
    "\n",
    "**Export Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Symbol:** {SYMBOL if 'SYMBOL' in dir() else 'N/A'}\n",
    "**Horizon:** {TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A'} bars\n",
    "\n",
    "## Package Contents\n",
    "\n",
    "This export package contains:\n",
    "\n",
    "- **Models:** {len(models_exported)} trained model(s)\n",
    "- **Predictions:** Validation and test set predictions\n",
    "- **Metrics:** Training, validation, and test performance metrics\n",
    "- **Visualizations:** Confusion matrices, feature importance, learning curves\n",
    "- **Model Cards:** Detailed model documentation and performance analysis\n",
    "- **Data Info:** Feature names, label mappings, data statistics\n",
    "\n",
    "## Models Included\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    for model_name in models_exported:\n",
    "        readme += f\"- `{model_name}`\\n\"\n",
    "\n",
    "    readme += \"\"\"\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "```\n",
    "\u251c\u2500\u2500 models/              # Trained models (PKL, ONNX)\n",
    "\u251c\u2500\u2500 predictions/         # Model predictions (CSV)\n",
    "\u251c\u2500\u2500 metrics/             # Performance metrics (JSON)\n",
    "\u251c\u2500\u2500 visualizations/      # Plots and charts (PNG)\n",
    "\u251c\u2500\u2500 model_cards/         # Model documentation (MD)\n",
    "\u251c\u2500\u2500 data/                # Feature info and stats\n",
    "\u251c\u2500\u2500 manifest.json        # Export metadata\n",
    "\u2514\u2500\u2500 README.md            # This file\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### Load a Model\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('models/xgboost/model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### Load ONNX Model (Production)\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Create inference session\n",
    "session = ort.InferenceSession('models/xgboost/model.onnx')\n",
    "\n",
    "# Run inference\n",
    "input_name = session.get_inputs()[0].name\n",
    "predictions = session.run(None, {input_name: X_test.astype('float32')})[0]\n",
    "```\n",
    "\n",
    "### Load Predictions\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load test predictions\n",
    "test_preds = pd.read_csv('predictions/test_predictions.csv')\n",
    "print(test_preds.head())\n",
    "```\n",
    "\n",
    "## Model Cards\n",
    "\n",
    "Each model has a detailed model card in `model_cards/` with:\n",
    "- Performance metrics (validation, test, CV)\n",
    "- Configuration parameters\n",
    "- Feature importance\n",
    "- Training details\n",
    "- Usage examples\n",
    "\n",
    "## Performance Summary\n",
    "\n",
    "See `metrics/test_metrics.json` for detailed performance metrics across all models.\n",
    "\n",
    "## Support\n",
    "\n",
    "For questions or issues:\n",
    "1. Review model cards for specific model details\n",
    "2. Check manifest.json for export metadata\n",
    "3. Consult feature documentation in data/\n",
    "\n",
    "---\n",
    "\n",
    "Generated by ML Model Factory\n",
    "\"\"\"\n",
    "\n",
    "    readme_path = export_dir / 'README.md'\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    return readme_path\n",
    "\n",
    "\n",
    "def export_predictions(model_name, model_info, export_dir):\n",
    "    \"\"\"Export validation and test predictions to CSV.\"\"\"\n",
    "    pred_dir = export_dir / 'predictions' / model_name\n",
    "    pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Export validation predictions if available\n",
    "    if 'val_predictions' in model_info:\n",
    "        val_preds = model_info['val_predictions']\n",
    "        val_df = pd.DataFrame({\n",
    "            'index': range(len(val_preds['actual'])),\n",
    "            'actual': val_preds['actual'],\n",
    "            'predicted': val_preds['predicted']\n",
    "        })\n",
    "        if 'confidence' in val_preds:\n",
    "            val_df['confidence'] = val_preds['confidence']\n",
    "        val_df['correct'] = val_df['actual'] == val_df['predicted']\n",
    "\n",
    "        val_path = pred_dir / 'val_predictions.csv'\n",
    "        val_df.to_csv(val_path, index=False)\n",
    "\n",
    "    # Export test predictions if available\n",
    "    test_info = TEST_RESULTS.get(model_name, {})\n",
    "    if 'predictions' in test_info:\n",
    "        test_preds = test_info['predictions']\n",
    "        test_df = pd.DataFrame({\n",
    "            'index': range(len(test_preds['actual'])),\n",
    "            'actual': test_preds['actual'],\n",
    "            'predicted': test_preds['predicted']\n",
    "        })\n",
    "        if 'confidence' in test_preds:\n",
    "            test_df['confidence'] = test_preds['confidence']\n",
    "        test_df['correct'] = test_df['actual'] == test_df['predicted']\n",
    "\n",
    "        test_path = pred_dir / 'test_predictions.csv'\n",
    "        test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    # Create predictions summary\n",
    "    summary = {\n",
    "        'model_name': model_name,\n",
    "        'val_samples': len(val_preds['actual']) if 'val_predictions' in model_info else 0,\n",
    "        'test_samples': len(test_preds['actual']) if 'predictions' in test_info else 0,\n",
    "        'val_accuracy': (val_df['correct'].sum() / len(val_df)) if 'val_predictions' in model_info else None,\n",
    "        'test_accuracy': (test_df['correct'].sum() / len(test_df)) if 'predictions' in test_info else None\n",
    "    }\n",
    "\n",
    "    summary_path = pred_dir / 'predictions_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPORT LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "if export_model:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL EXPORT PACKAGE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Get models to export\n",
    "    models_to_export = get_models_to_export()\n",
    "\n",
    "    if not models_to_export:\n",
    "        print(\"\\n\u26a0 No models to export. Check your selection criteria.\")\n",
    "    else:\n",
    "        print(f\"\\n\ud83d\udce6 Exporting {len(models_to_export)} model(s): {', '.join(models_to_export)}\")\n",
    "\n",
    "        # Create export directory\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        symbol = SYMBOL if 'SYMBOL' in dir() else 'UNKNOWN'\n",
    "        horizon = TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'XX'\n",
    "        export_name = f\"{timestamp}_{symbol}_H{horizon}\"\n",
    "\n",
    "        export_dir = RESULTS_DIR / 'exports' / export_name\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n\ud83d\udcc1 Export directory: {export_dir}\")\n",
    "\n",
    "        # Track export statistics\n",
    "        export_stats = {\n",
    "            'models_exported': 0,\n",
    "            'onnx_exports': 0,\n",
    "            'predictions_exported': 0,\n",
    "            'visualizations_exported': 0,\n",
    "            'model_cards_generated': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "\n",
    "        export_info = {\n",
    "            'data_stats': {\n",
    "                'train_samples': TRAINING_RESULTS.get(models_to_export[0], {}).get('train_samples', 0),\n",
    "                'val_samples': TRAINING_RESULTS.get(models_to_export[0], {}).get('val_samples', 0),\n",
    "                'test_samples': TEST_RESULTS.get(models_to_export[0], {}).get('test_samples', 0),\n",
    "                'n_features': TRAINING_RESULTS.get(models_to_export[0], {}).get('n_features', 0)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Export each model\n",
    "        for model_name in models_to_export:\n",
    "            print(f\"\\n\ud83d\udcca Processing: {model_name}\")\n",
    "\n",
    "            all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "            model_info = all_results.get(model_name, {})\n",
    "\n",
    "            if not model_info:\n",
    "                print(f\"  \u26a0 No training results found for {model_name}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: No training results\")\n",
    "                continue\n",
    "\n",
    "            run_id = model_info.get('run_id')\n",
    "            if not run_id:\n",
    "                print(f\"  \u26a0 No run_id found for {model_name}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: No run_id\")\n",
    "                continue\n",
    "\n",
    "            # Create model directory\n",
    "            model_dir = export_dir / 'models' / model_name\n",
    "            model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Find and copy model file\n",
    "            source_dir = EXPERIMENTS_DIR / run_id\n",
    "            model_file = source_dir / 'model.pkl'\n",
    "\n",
    "            if not model_file.exists():\n",
    "                print(f\"  \u26a0 Model file not found: {model_file}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: Model file not found\")\n",
    "                continue\n",
    "\n",
    "            # Copy model\n",
    "            dest_model = model_dir / 'model.pkl'\n",
    "            shutil.copy2(model_file, dest_model)\n",
    "            model_size = dest_model.stat().st_size / 1e6\n",
    "            print(f\"  \u2713 Model copied ({model_size:.2f} MB)\")\n",
    "            export_stats['models_exported'] += 1\n",
    "\n",
    "            # Export to ONNX if requested\n",
    "            if include_onnx:\n",
    "                feature_names = model_info.get('feature_names', [])\n",
    "                success, message = export_to_onnx(model_info, model_name, dest_model, feature_names)\n",
    "                if success:\n",
    "                    print(f\"  \u2713 ONNX: {message}\")\n",
    "                    export_stats['onnx_exports'] += 1\n",
    "                else:\n",
    "                    print(f\"  \u26a0 ONNX: {message}\")\n",
    "\n",
    "            # Save configuration\n",
    "            config = model_info.get('config', {})\n",
    "            if config:\n",
    "                config_path = model_dir / 'config.json'\n",
    "                with open(config_path, 'w') as f:\n",
    "                    json.dump(config, f, indent=2)\n",
    "                print(f\"  \u2713 Configuration saved\")\n",
    "\n",
    "            # Export predictions\n",
    "            if include_predictions:\n",
    "                try:\n",
    "                    export_predictions(model_name, model_info, export_dir)\n",
    "                    print(f\"  \u2713 Predictions exported\")\n",
    "                    export_stats['predictions_exported'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  \u26a0 Predictions export failed: {e}\")\n",
    "\n",
    "            # Generate model card\n",
    "            if include_model_card:\n",
    "                try:\n",
    "                    card_dir = export_dir / 'model_cards'\n",
    "                    card_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                    test_info = TEST_RESULTS.get(model_name, {})\n",
    "                    cv_info = CV_RESULTS.get(model_name, {})\n",
    "\n",
    "                    card_content = generate_model_card(model_name, model_info, test_info, cv_info)\n",
    "                    card_path = card_dir / f\"{model_name}_card.md\"\n",
    "\n",
    "                    with open(card_path, 'w') as f:\n",
    "                        f.write(card_content)\n",
    "\n",
    "                    print(f\"  \u2713 Model card generated\")\n",
    "                    export_stats['model_cards_generated'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  \u26a0 Model card generation failed: {e}\")\n",
    "\n",
    "        # Copy visualizations\n",
    "        if include_visualizations:\n",
    "            print(f\"\\n\ud83c\udfa8 Copying visualizations...\")\n",
    "            viz_dir = export_dir / 'visualizations'\n",
    "            viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Copy from experiments directory\n",
    "            for model_name in models_to_export:\n",
    "                model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(model_name, {})\n",
    "                run_id = model_info.get('run_id')\n",
    "                if run_id:\n",
    "                    source_viz = EXPERIMENTS_DIR / run_id / 'visualizations'\n",
    "                    if source_viz.exists():\n",
    "                        dest_viz = viz_dir / model_name\n",
    "                        shutil.copytree(source_viz, dest_viz, dirs_exist_ok=True)\n",
    "                        viz_count = len(list(dest_viz.rglob('*.png')))\n",
    "                        export_stats['visualizations_exported'] += viz_count\n",
    "\n",
    "            if export_stats['visualizations_exported'] > 0:\n",
    "                print(f\"  \u2713 {export_stats['visualizations_exported']} visualizations copied\")\n",
    "\n",
    "        # Export metrics\n",
    "        print(f\"\\n\ud83d\udcc8 Exporting metrics...\")\n",
    "        metrics_dir = export_dir / 'metrics'\n",
    "        metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Training metrics\n",
    "        training_metrics = {m: TRAINING_RESULTS[m]['metrics'] for m in models_to_export if m in TRAINING_RESULTS}\n",
    "        with open(metrics_dir / 'training_metrics.json', 'w') as f:\n",
    "            json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "        # Test metrics\n",
    "        test_metrics = {m: TEST_RESULTS[m]['metrics'] for m in models_to_export if m in TEST_RESULTS}\n",
    "        if test_metrics:\n",
    "            with open(metrics_dir / 'test_metrics.json', 'w') as f:\n",
    "                json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "        # CV results\n",
    "        cv_metrics = {m: CV_RESULTS[m] for m in models_to_export if m in CV_RESULTS}\n",
    "        if cv_metrics:\n",
    "            with open(metrics_dir / 'cv_results.json', 'w') as f:\n",
    "                json.dump(cv_metrics, f, indent=2)\n",
    "\n",
    "        print(f\"  \u2713 Metrics exported\")\n",
    "\n",
    "        # Export data info\n",
    "        print(f\"\\n\ud83d\udcca Exporting data information...\")\n",
    "        data_dir = export_dir / 'data'\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Feature names\n",
    "        if models_to_export:\n",
    "            first_model = models_to_export[0]\n",
    "            model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(first_model, {})\n",
    "            feature_names = model_info.get('feature_names', [])\n",
    "\n",
    "            if feature_names:\n",
    "                with open(data_dir / 'feature_names.txt', 'w') as f:\n",
    "                    f.write('\\n'.join(feature_names))\n",
    "\n",
    "        # Label mapping\n",
    "        label_mapping = {-1: 'SHORT', 0: 'NEUTRAL', 1: 'LONG'}\n",
    "        with open(data_dir / 'label_mapping.json', 'w') as f:\n",
    "            json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "        # Data stats\n",
    "        with open(data_dir / 'data_stats.json', 'w') as f:\n",
    "            json.dump(export_info['data_stats'], f, indent=2)\n",
    "\n",
    "        print(f\"  \u2713 Data info exported\")\n",
    "\n",
    "        # Create manifest\n",
    "        print(f\"\\n\ud83d\udccb Creating manifest...\")\n",
    "        manifest_path = create_manifest(export_dir, models_to_export, export_info)\n",
    "        print(f\"  \u2713 Manifest created: {manifest_path.name}\")\n",
    "\n",
    "        # Create README\n",
    "        print(f\"\\n\ud83d\udcdd Creating README...\")\n",
    "        readme_path = create_readme(export_dir, models_to_export)\n",
    "        print(f\"  \u2713 README created: {readme_path.name}\")\n",
    "\n",
    "        # Calculate total size\n",
    "        total_size = sum(f.stat().st_size for f in export_dir.rglob('*') if f.is_file())\n",
    "        total_size_mb = total_size / 1e6\n",
    "\n",
    "        # Create ZIP archive\n",
    "        zip_path = None\n",
    "        if create_zip_archive:\n",
    "            print(f\"\\n\ud83d\udce6 Creating ZIP archive...\")\n",
    "            zip_base = export_dir.parent / export_name\n",
    "            zip_path = Path(shutil.make_archive(str(zip_base), 'zip', export_dir))\n",
    "            zip_size_mb = zip_path.stat().st_size / 1e6\n",
    "            print(f\"  \u2713 Archive created: {zip_path.name} ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"EXPORT SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n\ud83d\udcc1 Export Path: {export_dir}\")\n",
    "        print(f\"\\n\ud83d\udcca Models Exported: {export_stats['models_exported']}\")\n",
    "        for model_name in models_to_export:\n",
    "            model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(model_name, {})\n",
    "            formats = ['PKL']\n",
    "            if (export_dir / 'models' / model_name / 'model.onnx').exists():\n",
    "                formats.append('ONNX')\n",
    "            print(f\"  \u2713 {model_name} ({', '.join(formats)})\")\n",
    "\n",
    "        print(f\"\\n\ud83d\udce6 Package Contents:\")\n",
    "        print(f\"  \u2713 Models: {export_stats['models_exported']}\")\n",
    "        if export_stats['onnx_exports'] > 0:\n",
    "            print(f\"  \u2713 ONNX exports: {export_stats['onnx_exports']}\")\n",
    "        if include_predictions:\n",
    "            print(f\"  \u2713 Predictions: Val + Test\")\n",
    "        print(f\"  \u2713 Metrics: Training, Test, CV\")\n",
    "        if export_stats['visualizations_exported'] > 0:\n",
    "            print(f\"  \u2713 Visualizations: {export_stats['visualizations_exported']} plots\")\n",
    "        if export_stats['model_cards_generated'] > 0:\n",
    "            print(f\"  \u2713 Model Cards: {export_stats['model_cards_generated']}\")\n",
    "        print(f\"  \u2713 Data Info: Features, labels, stats\")\n",
    "        print(f\"  \u2713 README: Setup and usage guide\")\n",
    "\n",
    "        print(f\"\\n\ud83d\udcbe Total Size: {total_size_mb:.1f} MB\", end='')\n",
    "        if zip_path:\n",
    "            zip_size_mb = zip_path.stat().st_size / 1e6\n",
    "            print(f\" (compressed: {zip_size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "        if export_stats['errors']:\n",
    "            print(f\"\\n\u26a0 Errors ({len(export_stats['errors'])}):\")\n",
    "            for error in export_stats['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "\n",
    "        print(f\"\\n\u2705 Next Steps:\")\n",
    "        print(f\"1. Extract ZIP to deployment environment\")\n",
    "        print(f\"2. Review model cards for performance details\")\n",
    "        if export_stats['onnx_exports'] > 0:\n",
    "            print(f\"3. Use ONNX models for production inference\")\n",
    "        print(f\"4. Check README for usage examples\")\n",
    "\n",
    "        # Colab download helper\n",
    "        if IS_COLAB and create_zip_archive and zip_path:\n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(\"DOWNLOAD TO LOCAL\")\n",
    "            print(\"=\" * 80)\n",
    "            download_export = False  #@param {type: \"boolean\"}\n",
    "\n",
    "            if download_export:\n",
    "                try:\n",
    "                    from google.colab import files\n",
    "                    files.download(str(zip_path))\n",
    "                    print(f\"\\n\u2713 Download started: {zip_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n\u26a0 Download failed: {e}\")\n",
    "                    print(f\"Manual download from: {zip_path}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\"Model export skipped. Enable 'export_model' checkbox above to export.\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Quick Reference\n",
    "\n",
    "## Command Line Usage\n",
    "\n",
    "```bash\n",
    "# Train single model\n",
    "python scripts/train_model.py --model xgboost --horizon 20\n",
    "\n",
    "# Train neural model\n",
    "python scripts/train_model.py --model lstm --horizon 20 --seq-len 60\n",
    "\n",
    "# Run cross-validation\n",
    "python scripts/run_cv.py --models xgboost,lightgbm --horizons 20 --n-splits 5\n",
    "\n",
    "# Train ensemble\n",
    "python scripts/train_model.py --model voting --horizon 20\n",
    "\n",
    "# List all available models\n",
    "python scripts/train_model.py --list-models\n",
    "```\n",
    "\n",
    "## Model Families\n",
    "\n",
    "| Family | Models | Best For |\n",
    "|--------|--------|----------|\n",
    "| Boosting | XGBoost, LightGBM, CatBoost | Fast, accurate, tabular data |\n",
    "| Classical | Random Forest, Logistic, SVM | Baselines, interpretability |\n",
    "| Neural | LSTM, GRU, TCN | Sequential patterns, temporal dependencies |\n",
    "| Ensemble | Voting, Stacking, Blending | Combined predictions, robustness |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}