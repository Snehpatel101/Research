{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Unified Pipeline\n",
    "\n",
    "**Complete ML training pipeline for OHLCV time series with 13 models.**\n",
    "\n",
    "## \u2728 Features (Production Ready)\n",
    "\n",
    "### \ud83e\udd16 Model Support (13 Models)\n",
    "- **Boosting (3):** XGBoost, LightGBM, CatBoost\n",
    "- **Neural (4):** LSTM, GRU, TCN, Transformer\n",
    "- **Classical (3):** Random Forest, Logistic Regression, SVM\n",
    "- **Ensemble (3):** Voting, Stacking, Blending\n",
    "\n",
    "### \ud83d\udd2c Advanced Capabilities\n",
    "- **Transformer Support:** Self-attention with 8-head architecture + attention visualization\n",
    "- **Hyperparameter Tuning:** Optuna integration with 20+ trials per model\n",
    "- **Cross-Validation:** Purged K-fold for time series (prevents lookahead bias)\n",
    "- **Ensemble Intelligence:** Diversity analysis, contribution metrics, production recommendations\n",
    "- **Professional Export:** ONNX conversion, model cards, ZIP packages\n",
    "\n",
    "### \ud83d\udcca Rich Visualizations\n",
    "- Confusion matrices with class-wise accuracy\n",
    "- Feature importance (boosting models)\n",
    "- Learning curves (training/validation loss)\n",
    "- Prediction distribution analysis\n",
    "- Per-class precision/recall/F1 metrics\n",
    "- **Transformer attention heatmaps** (NEW)\n",
    "\n",
    "### \ud83d\udcc8 Evaluation & Analysis\n",
    "- Test set evaluation with generalization gap analysis\n",
    "- Out-of-fold predictions for stacking\n",
    "- Cross-validation with time series purge/embargo\n",
    "- Ensemble diversity metrics (disagreement, correlation, Q-statistic)\n",
    "- Production readiness scoring\n",
    "\n",
    "## Pipeline Phases\n",
    "1. **Configuration** - All settings in one place (13 model toggles)\n",
    "2. **Environment Setup** - Auto-detects Colab vs Local\n",
    "3. **Phase 1: Data Pipeline** - Clean \u2192 Features \u2192 Labels \u2192 Splits \u2192 Scale\n",
    "4. **Phase 2: Model Training** - Train any of 13 model types\n",
    "   - 4.1 Train Models\n",
    "   - 4.2 Training Summary\n",
    "   - 4.3 Visualizations (5 types)\n",
    "   - 4.4 Transformer Attention (NEW)\n",
    "   - 4.5 Test Set Performance\n",
    "5. **Phase 3: Cross-Validation** - Robust evaluation with tuning (optional)\n",
    "6. **Phase 4: Ensemble** - Combine models intelligently (optional)\n",
    "7. **Results & Export** - Professional packages with ONNX\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MASTER CONFIGURATION\n",
    "\n",
    "**Configure ALL settings here. No need to modify any other cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 Master Configuration Panel { display-mode: \"form\" }\n",
    "#@markdown ## Data Configuration\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ### Contract Selection\n",
    "SYMBOL = \"SI\"  #@param [\"SI\", \"MES\", \"MGC\", \"ES\", \"GC\", \"NQ\", \"CL\", \"HG\", \"ZB\", \"ZN\"]\n",
    "#@markdown Select ONE contract. Each contract is trained in complete isolation.\n",
    "#@markdown - **SI** = Silver, **MES** = Micro E-mini S&P, **MGC** = Micro Gold\n",
    "#@markdown - **ES** = E-mini S&P, **GC** = Gold, **NQ** = E-mini Nasdaq\n",
    "#@markdown - **CL** = Crude Oil, **HG** = Copper, **ZB/ZN** = Bonds\n",
    "\n",
    "#@markdown ### Date Range Selection\n",
    "DATE_RANGE = \"2019-2024\"  #@param [\"2019-2024\", \"2020-2024\", \"2021-2024\", \"2022-2024\", \"2023-2024\", \"Full Dataset\"]\n",
    "#@markdown Select the date range for your data\n",
    "\n",
    "#@markdown ### Data Source\n",
    "DRIVE_DATA_PATH = \"research/data/raw\"  #@param {type: \"string\"}\n",
    "#@markdown Google Drive path relative to My Drive\n",
    "\n",
    "#@markdown ### Custom Data File (optional)\n",
    "CUSTOM_DATA_FILE = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Leave empty for auto-detection, or specify exact filename (e.g., `si_historical_2019_2024.parquet`)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Pipeline Configuration\n",
    "\n",
    "#@markdown ### Label Horizons (bars)\n",
    "HORIZONS = \"5,10,15,20\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated prediction horizons\n",
    "\n",
    "#@markdown ### Train/Val/Test Split Ratios\n",
    "TRAIN_RATIO = 0.70  #@param {type: \"number\"}\n",
    "VAL_RATIO = 0.15  #@param {type: \"number\"}\n",
    "TEST_RATIO = 0.15  #@param {type: \"number\"}\n",
    "\n",
    "#@markdown ### Leakage Prevention\n",
    "PURGE_BARS = 60  #@param {type: \"integer\"}\n",
    "#@markdown Bars to purge around train/val boundary (3x max horizon)\n",
    "EMBARGO_BARS = 1440  #@param {type: \"integer\"}\n",
    "#@markdown Embargo period after validation (~5 days at 5-min)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Model Training Configuration\n",
    "\n",
    "#@markdown ### Training Horizon\n",
    "TRAINING_HORIZON = 20  #@param [5, 10, 15, 20]\n",
    "#@markdown Which horizon to train models on\n",
    "\n",
    "#@markdown ### Model Selection\n",
    "#@markdown #### Boosting Models\n",
    "TRAIN_XGBOOST = True  #@param {type: \"boolean\"}\n",
    "TRAIN_LIGHTGBM = True  #@param {type: \"boolean\"}\n",
    "TRAIN_CATBOOST = True  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Classical Models\n",
    "TRAIN_RANDOM_FOREST = False  #@param {type: \"boolean\"}\n",
    "TRAIN_LOGISTIC = False  #@param {type: \"boolean\"}\n",
    "TRAIN_SVM = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Neural Network Models\n",
    "TRAIN_LSTM = False  #@param {type: \"boolean\"}\n",
    "TRAIN_GRU = False  #@param {type: \"boolean\"}\n",
    "TRAIN_TCN = False  #@param {type: \"boolean\"}\n",
    "TRAIN_TRANSFORMER = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown #### Ensemble Models\n",
    "TRAIN_VOTING = False  #@param {type: \"boolean\"}\n",
    "TRAIN_STACKING = False  #@param {type: \"boolean\"}\n",
    "TRAIN_BLENDING = False  #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ### Neural Network Settings\n",
    "SEQUENCE_LENGTH = 60  #@param {type: \"slider\", min: 30, max: 120, step: 10}\n",
    "BATCH_SIZE = 256  #@param [64, 128, 256, 512, 1024]\n",
    "MAX_EPOCHS = 50  #@param {type: \"integer\"}\n",
    "EARLY_STOPPING_PATIENCE = 10  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Transformer Settings (when enabled)\n",
    "TRANSFORMER_SEQUENCE_LENGTH = 128  #@param {type: \"integer\"}\n",
    "TRANSFORMER_N_HEADS = 8  #@param [4, 8, 16]\n",
    "TRANSFORMER_N_LAYERS = 3  #@param [2, 3, 4, 6]\n",
    "TRANSFORMER_D_MODEL = 256  #@param [128, 256, 512]\n",
    "\n",
    "#@markdown ### Boosting Settings\n",
    "N_ESTIMATORS = 500  #@param {type: \"integer\"}\n",
    "BOOSTING_EARLY_STOPPING = 50  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Voting Ensemble Configuration (when enabled)\n",
    "VOTING_BASE_MODELS = \"xgboost,lightgbm,catboost\"  #@param {type: \"string\"}\n",
    "VOTING_WEIGHTS = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Leave weights empty for equal weighting\n",
    "\n",
    "#@markdown ### Stacking Ensemble Configuration (when enabled)\n",
    "STACKING_BASE_MODELS = \"xgboost,lightgbm,lstm\"  #@param {type: \"string\"}\n",
    "STACKING_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"xgboost\", \"random_forest\"]\n",
    "STACKING_N_FOLDS = 5  #@param {type: \"integer\"}\n",
    "\n",
    "#@markdown ### Blending Ensemble Configuration (when enabled)\n",
    "BLENDING_BASE_MODELS = \"xgboost,lightgbm,random_forest\"  #@param {type: \"string\"}\n",
    "BLENDING_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"xgboost\", \"random_forest\"]\n",
    "BLENDING_HOLDOUT_RATIO = 0.2  #@param {type: \"number\"}\n",
    "\n",
    "#@markdown ### Class Balancing\n",
    "USE_CLASS_WEIGHTS = True  #@param {type: \"boolean\"}\n",
    "#@markdown Automatically balance classes during training (recommended for imbalanced data)\n",
    "USE_SAMPLE_WEIGHTS = True  #@param {type: \"boolean\"}\n",
    "#@markdown Use quality-based sample weights from pipeline (if available)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Optional Phases\n",
    "\n",
    "#@markdown ### Cross-Validation\n",
    "RUN_CROSS_VALIDATION = False  #@param {type: \"boolean\"}\n",
    "CV_N_SPLITS = 5  #@param {type: \"integer\"}\n",
    "CV_TUNE_HYPERPARAMS = False  #@param {type: \"boolean\"}\n",
    "CV_N_TRIALS = 20  #@param {type: \"integer\"}\n",
    "CV_USE_PRESCALED = True  #@param {type: \"boolean\"}\n",
    "#@markdown Use pre-scaled data (faster) or scale per-fold (stricter, slower)\n",
    "\n",
    "#@markdown ### Ensemble Training\n",
    "TRAIN_ENSEMBLE = False  #@param {type: \"boolean\"}\n",
    "ENSEMBLE_TYPE = \"voting\"  #@param [\"voting\", \"stacking\", \"blending\"]\n",
    "ENSEMBLE_META_LEARNER = \"logistic\"  #@param [\"logistic\", \"random_forest\", \"xgboost\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Execution Options\n",
    "\n",
    "#@markdown ### What to Run\n",
    "RUN_DATA_PIPELINE = True  #@param {type: \"boolean\"}\n",
    "#@markdown Run Phase 1 data pipeline\n",
    "RUN_MODEL_TRAINING = True  #@param {type: \"boolean\"}\n",
    "#@markdown Run Phase 2 model training\n",
    "\n",
    "#@markdown ### Memory Management\n",
    "SAFE_MODE = False  #@param {type: \"boolean\"}\n",
    "#@markdown Enable for low-memory environments (reduces batch size, limits iterations)\n",
    "\n",
    "#@markdown ### Reproducibility\n",
    "RANDOM_SEED = 42  #@param {type: \"integer\"}\n",
    "#@markdown Random seed for reproducibility (set to 0 for random initialization)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD CONFIGURATION (DO NOT MODIFY BELOW)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Parse horizons\n",
    "HORIZON_LIST = [int(h.strip()) for h in HORIZONS.split(',')]\n",
    "\n",
    "# Parse date range\n",
    "if DATE_RANGE == \"Full Dataset\":\n",
    "    YEAR_START = None\n",
    "    YEAR_END = None\n",
    "else:\n",
    "    years = DATE_RANGE.split('-')\n",
    "    YEAR_START = int(years[0])\n",
    "    YEAR_END = int(years[1])\n",
    "\n",
    "# Build model list\n",
    "MODELS_TO_TRAIN = []\n",
    "if TRAIN_XGBOOST: MODELS_TO_TRAIN.append('xgboost')\n",
    "if TRAIN_LIGHTGBM: MODELS_TO_TRAIN.append('lightgbm')\n",
    "if TRAIN_CATBOOST: MODELS_TO_TRAIN.append('catboost')\n",
    "if TRAIN_RANDOM_FOREST: MODELS_TO_TRAIN.append('random_forest')\n",
    "if TRAIN_LOGISTIC: MODELS_TO_TRAIN.append('logistic')\n",
    "if TRAIN_SVM: MODELS_TO_TRAIN.append('svm')\n",
    "if TRAIN_LSTM: MODELS_TO_TRAIN.append('lstm')\n",
    "if TRAIN_GRU: MODELS_TO_TRAIN.append('gru')\n",
    "if TRAIN_TCN: MODELS_TO_TRAIN.append('tcn')\n",
    "if TRAIN_TRANSFORMER: MODELS_TO_TRAIN.append('transformer')\n",
    "if TRAIN_VOTING: MODELS_TO_TRAIN.append('voting')\n",
    "if TRAIN_STACKING: MODELS_TO_TRAIN.append('stacking')\n",
    "if TRAIN_BLENDING: MODELS_TO_TRAIN.append('blending')\n",
    "\n",
    "# Date range will be auto-detected from data file\n",
    "DATA_START = None  # Auto-detected\n",
    "DATA_END = None    # Auto-detected\n",
    "\n",
    "# Safe mode adjustments\n",
    "if SAFE_MODE:\n",
    "    BATCH_SIZE = min(BATCH_SIZE, 64)\n",
    "    N_ESTIMATORS = min(N_ESTIMATORS, 300)\n",
    "    SEQUENCE_LENGTH = min(SEQUENCE_LENGTH, 30)\n",
    "    TRANSFORMER_SEQUENCE_LENGTH = min(TRANSFORMER_SEQUENCE_LENGTH, 64)\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=\" * 70)\n",
    "print(\" ML PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  Contract:        {SYMBOL}\")\n",
    "print(f\"  Date Range:      {DATE_RANGE}\")\n",
    "if CUSTOM_DATA_FILE:\n",
    "    print(f\"  Custom File:     {CUSTOM_DATA_FILE}\")\n",
    "print(f\"  Horizons:        {HORIZON_LIST}\")\n",
    "print(f\"  Split Ratios:    {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")\n",
    "print(f\"  Training Horizon: H{TRAINING_HORIZON}\")\n",
    "print(f\"  Models:          {MODELS_TO_TRAIN if MODELS_TO_TRAIN else 'None selected'}\")\n",
    "if MODELS_TO_TRAIN:\n",
    "    boosting_models = [m for m in MODELS_TO_TRAIN if m in ['xgboost', 'lightgbm', 'catboost']]\n",
    "    classical_models = [m for m in MODELS_TO_TRAIN if m in ['random_forest', 'logistic', 'svm']]\n",
    "    neural_models = [m for m in MODELS_TO_TRAIN if m in ['lstm', 'gru', 'tcn', 'transformer']]\n",
    "    ensemble_models = [m for m in MODELS_TO_TRAIN if m in ['voting', 'stacking', 'blending']]\n",
    "    if boosting_models:\n",
    "        print(f\"    Boosting:      {boosting_models}\")\n",
    "    if classical_models:\n",
    "        print(f\"    Classical:     {classical_models}\")\n",
    "    if neural_models:\n",
    "        print(f\"    Neural:        {neural_models}\")\n",
    "    if ensemble_models:\n",
    "        print(f\"    Ensemble:      {ensemble_models}\")\n",
    "print(f\"\\n  Run Pipeline:    {RUN_DATA_PIPELINE}\")\n",
    "print(f\"  Run Training:    {RUN_MODEL_TRAINING}\")\n",
    "print(f\"  Cross-Validation: {RUN_CROSS_VALIDATION}\")\n",
    "print(f\"  Class Weights:   {USE_CLASS_WEIGHTS}\")\n",
    "print(f\"  Sample Weights:  {USE_SAMPLE_WEIGHTS}\")\n",
    "print(f\"  Ensemble:        {TRAIN_ENSEMBLE}\")\n",
    "print(f\"  Safe Mode:       {SAFE_MODE}\")\n",
    "print(f\"  Random Seed:     {RANDOM_SEED}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration complete! Run the next cells sequentially.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ENVIRONMENT SETUP\n",
    "\n",
    "Auto-detects Colab vs Local environment and sets up paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Environment Detection & Setup { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# ENVIRONMENT DETECTION\n",
    "# ============================================================\n",
    "IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\n[Environment] Google Colab detected\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone/update repository\n",
    "    REPO_PATH = Path('/content/research')\n",
    "    if not REPO_PATH.exists():\n",
    "        print(\"\\n[Setup] Cloning repository...\")\n",
    "        !git clone https://github.com/Snehpatel101/research.git /content/research\n",
    "    else:\n",
    "        print(\"\\n[Setup] Updating repository...\")\n",
    "        !cd /content/research && git pull --quiet\n",
    "    \n",
    "    # Set paths\n",
    "    PROJECT_ROOT = REPO_PATH\n",
    "    DRIVE_ROOT = Path('/content/drive/MyDrive')\n",
    "    RAW_DATA_DIR = DRIVE_ROOT / DRIVE_DATA_PATH\n",
    "    RESULTS_DIR = DRIVE_ROOT / 'research/experiments'\n",
    "    \n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[Environment] Local environment detected\")\n",
    "    \n",
    "    PROJECT_ROOT = Path('.')\n",
    "    DRIVE_ROOT = None\n",
    "    RAW_DATA_DIR = PROJECT_ROOT / 'data/raw'\n",
    "    RESULTS_DIR = PROJECT_ROOT / 'experiments'\n",
    "    \n",
    "    os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Add to Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Create output directories\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "EXPERIMENTS_DIR = RESULTS_DIR / 'runs'\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n  Project Root:  {PROJECT_ROOT}\")\n",
    "print(f\"  Raw Data:      {RAW_DATA_DIR}\")\n",
    "print(f\"  Splits:        {SPLITS_DIR}\")\n",
    "print(f\"  Experiments:   {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.2 Install Dependencies { display-mode: \"form\" }\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"[Dependencies] Installing packages...\")\n",
    "    !pip install -q torch xgboost lightgbm catboost optuna ta pywavelets scikit-learn pandas numpy matplotlib tqdm pyarrow numba psutil\n",
    "    print(\"[Dependencies] Installation complete!\")\n",
    "else:\n",
    "    print(\"[Dependencies] Local environment - assuming packages installed.\")\n",
    "    print(\"  If needed: pip install torch xgboost lightgbm catboost optuna ta pywavelets psutil\")\n",
    "\n",
    "# Verify imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"\\n  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.3 GPU Detection { display-mode: \"form\" }\n",
    "\n",
    "import torch\n",
    "\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_NAME = None\n",
    "GPU_MEMORY = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" HARDWARE DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    GPU_NAME = props.name\n",
    "    GPU_MEMORY = props.total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"\\n  GPU: {GPU_NAME}\")\n",
    "    print(f\"  Memory: {GPU_MEMORY:.1f} GB\")\n",
    "    print(f\"  Compute: {props.major}.{props.minor}\")\n",
    "    \n",
    "    # Adjust batch size based on GPU memory\n",
    "    if GPU_MEMORY >= 40:\n",
    "        RECOMMENDED_BATCH = 1024\n",
    "    elif GPU_MEMORY >= 15:\n",
    "        RECOMMENDED_BATCH = 512\n",
    "    else:\n",
    "        RECOMMENDED_BATCH = 256\n",
    "    \n",
    "    print(f\"  Recommended batch: {RECOMMENDED_BATCH}\")\n",
    "else:\n",
    "    print(\"\\n  GPU: Not available (using CPU)\")\n",
    "    print(\"  Tip: Runtime -> Change runtime type -> GPU\")\n",
    "    RECOMMENDED_BATCH = 128\n",
    "\n",
    "# Check for neural models without GPU\n",
    "NEURAL_MODELS = {'lstm', 'gru', 'tcn'}\n",
    "selected_neural = set(MODELS_TO_TRAIN) & NEURAL_MODELS\n",
    "if selected_neural and not GPU_AVAILABLE:\n",
    "    print(f\"\\n  [WARNING] Neural models selected but no GPU: {selected_neural}\")\n",
    "    print(\"  Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.4 Reproducibility Setup { display-mode: \"form\" }\n",
    "\n",
    "# Set random seeds for reproducibility across all frameworks\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Ensure RANDOM_SEED is defined (in case cells run out of order)\n",
    "if 'RANDOM_SEED' not in dir():\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" REPRODUCIBILITY SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if RANDOM_SEED > 0:\n",
    "    # Python random\n",
    "    random.seed(RANDOM_SEED)\n",
    "    print(f\"  Python random seed:  {RANDOM_SEED}\")\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    print(f\"  NumPy seed:          {RANDOM_SEED}\")\n",
    "    \n",
    "    # PyTorch (if available)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(RANDOM_SEED)\n",
    "        print(f\"  PyTorch seed:        {RANDOM_SEED}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(RANDOM_SEED)\n",
    "            torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "            # Enable deterministic operations (may reduce performance)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            print(f\"  PyTorch CUDA seed:   {RANDOM_SEED}\")\n",
    "            print(\"  cuDNN:               deterministic=True, benchmark=False\")\n",
    "    except ImportError:\n",
    "        print(\"  PyTorch:             not installed\")\n",
    "    \n",
    "    print(\"\\n  [Reproducibility] All seeds set successfully!\")\n",
    "else:\n",
    "    print(\"  [Reproducibility] RANDOM_SEED=0, using random initialization\")\n",
    "    print(\"  Note: Results will NOT be reproducible across runs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.5 Memory Utilities { display-mode: \"form\" }\n",
    "\n",
    "import psutil\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Ensure GPU_AVAILABLE is defined (in case cells run out of order)\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "def print_memory_status(label: str = \"Current\"):\n",
    "    \"\"\"Print current RAM and GPU memory usage.\"\"\"\n",
    "    print(f\"\\n--- Memory: {label} ---\")\n",
    "    \n",
    "    # RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM: {ram.used/1e9:.1f}GB / {ram.total/1e9:.1f}GB ({ram.percent}%)\")\n",
    "    \n",
    "    # GPU\n",
    "    if GPU_AVAILABLE:\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"GPU: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear RAM and GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "print(\"Memory utilities loaded.\")\n",
    "print_memory_status(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. PHASE 1: DATA PIPELINE\n",
    "\n",
    "Processes raw OHLCV data into training-ready datasets.\n",
    "\n",
    "**Pipeline stages:**\n",
    "1. Load raw 1-minute data\n",
    "2. Clean and resample to 5-minute bars\n",
    "3. Generate 150+ technical features\n",
    "4. Apply triple-barrier labeling\n",
    "5. Create train/val/test splits with purge/embargo\n",
    "6. Scale features (train-only fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Verify Raw Data & Detect Date Range { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined (in case cells run out of order)\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'RAW_DATA_DIR' not in dir():\n",
    "    if IS_COLAB:\n",
    "        RAW_DATA_DIR = Path('/content/drive/MyDrive') / DRIVE_DATA_PATH\n",
    "    else:\n",
    "        RAW_DATA_DIR = Path('./data/raw')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" RAW DATA VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nLooking for {SYMBOL} data in: {RAW_DATA_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# FLEXIBLE FILE DETECTION\n",
    "# ============================================================\n",
    "RAW_DATA_FILE = None\n",
    "\n",
    "# If custom file specified, use it directly\n",
    "if CUSTOM_DATA_FILE:\n",
    "    custom_path = RAW_DATA_DIR / CUSTOM_DATA_FILE\n",
    "    if custom_path.exists():\n",
    "        RAW_DATA_FILE = custom_path\n",
    "        print(f\"\\n  Using custom file: {CUSTOM_DATA_FILE}\")\n",
    "    else:\n",
    "        print(f\"\\n  [WARNING] Custom file not found: {CUSTOM_DATA_FILE}\")\n",
    "\n",
    "# Auto-detect file with flexible patterns\n",
    "if RAW_DATA_FILE is None and RAW_DATA_DIR.exists():\n",
    "    symbol_lower = SYMBOL.lower()\n",
    "    symbol_upper = SYMBOL.upper()\n",
    "    \n",
    "    # Build list of all matching files\n",
    "    matching_files = []\n",
    "    \n",
    "    for f in RAW_DATA_DIR.iterdir():\n",
    "        if f.suffix not in ['.parquet', '.csv']:\n",
    "            continue\n",
    "        \n",
    "        fname_lower = f.name.lower()\n",
    "        \n",
    "        # Check if file contains the symbol (case-insensitive)\n",
    "        if symbol_lower in fname_lower:\n",
    "            # Priority scoring: prefer files with date range matching config\n",
    "            priority = 0\n",
    "            \n",
    "            # Boost priority if filename contains the configured date range\n",
    "            if YEAR_START and YEAR_END:\n",
    "                date_pattern = f\"{YEAR_START}_{YEAR_END}|{YEAR_START}-{YEAR_END}\"\n",
    "                if re.search(date_pattern, fname_lower):\n",
    "                    priority += 10\n",
    "            \n",
    "            # Boost priority for common naming patterns\n",
    "            if '_1m' in fname_lower or '_1min' in fname_lower:\n",
    "                priority += 5\n",
    "            if 'historical' in fname_lower:\n",
    "                priority += 3\n",
    "            if f.suffix == '.parquet':\n",
    "                priority += 2  # Prefer parquet over CSV\n",
    "            \n",
    "            matching_files.append((priority, f))\n",
    "    \n",
    "    # Sort by priority (highest first) and pick best match\n",
    "    if matching_files:\n",
    "        matching_files.sort(key=lambda x: x[0], reverse=True)\n",
    "        RAW_DATA_FILE = matching_files[0][1]\n",
    "        \n",
    "        if len(matching_files) > 1:\n",
    "            print(f\"\\n  Found {len(matching_files)} matching files:\")\n",
    "            for pri, f in matching_files[:5]:\n",
    "                marker = \"\u2192\" if f == RAW_DATA_FILE else \" \"\n",
    "                print(f\"    {marker} {f.name} (priority: {pri})\")\n",
    "\n",
    "# ============================================================\n",
    "# VALIDATE AND LOAD DATA\n",
    "# ============================================================\n",
    "if RAW_DATA_FILE:\n",
    "    size_mb = RAW_DATA_FILE.stat().st_size / 1e6\n",
    "    print(f\"\\n  Selected: {RAW_DATA_FILE.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Load and validate\n",
    "    print(\"  Loading data...\")\n",
    "    if RAW_DATA_FILE.suffix == '.parquet':\n",
    "        df_raw = pd.read_parquet(RAW_DATA_FILE)\n",
    "    else:\n",
    "        df_raw = pd.read_csv(RAW_DATA_FILE)\n",
    "    \n",
    "    print(f\"  Rows: {len(df_raw):,}\")\n",
    "    print(f\"  Columns: {list(df_raw.columns)}\")\n",
    "    \n",
    "    # Validate OHLCV columns (case-insensitive)\n",
    "    required = {'open', 'high', 'low', 'close', 'volume'}\n",
    "    found = {c.lower() for c in df_raw.columns}\n",
    "    if required.issubset(found):\n",
    "        print(\"  OHLCV columns: \u2713 OK\")\n",
    "    else:\n",
    "        missing = required - found\n",
    "        print(f\"  [ERROR] Missing columns: {missing}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # AUTO-DETECT DATE RANGE FROM DATA\n",
    "    # ============================================================\n",
    "    date_col = None\n",
    "    for c in df_raw.columns:\n",
    "        if 'date' in c.lower() or 'time' in c.lower():\n",
    "            date_col = c\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        df_raw[date_col] = pd.to_datetime(df_raw[date_col])\n",
    "        \n",
    "        # Store globally for pipeline use\n",
    "        DATA_START = df_raw[date_col].min()\n",
    "        DATA_END = df_raw[date_col].max()\n",
    "        DATA_START_YEAR = DATA_START.year\n",
    "        DATA_END_YEAR = DATA_END.year\n",
    "        \n",
    "        print(f\"\\n  [DATE RANGE DETECTED]\")\n",
    "        print(f\"  Start: {DATA_START.strftime('%Y-%m-%d %H:%M')} ({DATA_START_YEAR})\")\n",
    "        print(f\"  End:   {DATA_END.strftime('%Y-%m-%d %H:%M')} ({DATA_END_YEAR})\")\n",
    "        print(f\"  Span:  {(DATA_END - DATA_START).days:,} days ({DATA_END_YEAR - DATA_START_YEAR + 1} years)\")\n",
    "        \n",
    "        # Validate against configured date range\n",
    "        if YEAR_START and YEAR_END:\n",
    "            if DATA_START_YEAR <= YEAR_START and DATA_END_YEAR >= YEAR_END:\n",
    "                print(f\"  Config Match: \u2713 Data covers {DATE_RANGE}\")\n",
    "            else:\n",
    "                print(f\"  [WARNING] Data range ({DATA_START_YEAR}-{DATA_END_YEAR}) differs from config ({DATE_RANGE})\")\n",
    "    else:\n",
    "        print(\"  [WARNING] No datetime column found - using index\")\n",
    "        DATA_START = None\n",
    "        DATA_END = None\n",
    "        DATA_START_YEAR = YEAR_START or 2019\n",
    "        DATA_END_YEAR = YEAR_END or 2024\n",
    "    \n",
    "    del df_raw\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\n  \u2713 Data verified and ready for processing!\")\n",
    "else:\n",
    "    print(f\"\\n  [ERROR] No data file found for {SYMBOL}!\")\n",
    "    print(f\"  Expected location: {RAW_DATA_DIR}\")\n",
    "    print(f\"\\n  Tried patterns matching '{SYMBOL}' (case-insensitive)\")\n",
    "    print(f\"\\n  Available files in directory:\")\n",
    "    if RAW_DATA_DIR.exists():\n",
    "        for f in sorted(RAW_DATA_DIR.iterdir()):\n",
    "            if f.suffix in ['.csv', '.parquet']:\n",
    "                print(f\"    - {f.name}\")\n",
    "    else:\n",
    "        print(f\"    Directory does not exist!\")\n",
    "    \n",
    "    print(f\"\\n  [FIX] Set CUSTOM_DATA_FILE in Section 1 to your exact filename\")\n",
    "    \n",
    "    RAW_DATA_FILE = None\n",
    "    DATA_START = None\n",
    "    DATA_END = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 Run Data Pipeline { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'RUN_DATA_PIPELINE' not in dir():\n",
    "    RUN_DATA_PIPELINE = True\n",
    "if 'RAW_DATA_FILE' not in dir():\n",
    "    RAW_DATA_FILE = None\n",
    "\n",
    "if not RUN_DATA_PIPELINE:\n",
    "    print(\"[Skipped] Data pipeline disabled in configuration.\")\n",
    "    print(\"Set RUN_DATA_PIPELINE = True in Section 1 to enable.\")\n",
    "elif RAW_DATA_FILE is None:\n",
    "    print(\"[Error] No raw data file found. Cannot run pipeline.\")\n",
    "    print(\"Run Section 3.1 first to detect data files.\")\n",
    "else:\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 1: DATA PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n  Symbol: {SYMBOL}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # COPY DATA FROM DRIVE TO PROJECT (Colab only)\n",
    "    # ============================================================\n",
    "    if IS_COLAB and RAW_DATA_FILE is not None:\n",
    "        # The pipeline expects data in PROJECT_ROOT/data/raw/\n",
    "        project_raw_dir = PROJECT_ROOT / 'data/raw'\n",
    "        project_raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create standardized filename for pipeline: SYMBOL_1m.parquet\n",
    "        target_filename = f\"{SYMBOL}_1m{RAW_DATA_FILE.suffix}\"\n",
    "        target_path = project_raw_dir / target_filename\n",
    "        \n",
    "        # Only copy if source is not already in project directory\n",
    "        source_in_project = str(RAW_DATA_FILE).startswith(str(PROJECT_ROOT))\n",
    "        if not source_in_project:\n",
    "            if not target_path.exists() or target_path.stat().st_size != RAW_DATA_FILE.stat().st_size:\n",
    "                print(f\"\\n  [Setup] Copying data from Drive to project...\")\n",
    "                print(f\"    From: {RAW_DATA_FILE}\")\n",
    "                print(f\"    To:   {target_path}\")\n",
    "                shutil.copy2(RAW_DATA_FILE, target_path)\n",
    "                print(f\"    Done! ({target_path.stat().st_size / 1e6:.1f} MB)\")\n",
    "            else:\n",
    "                print(f\"\\n  [Setup] Data already in project: {target_path.name}\")\n",
    "        else:\n",
    "            print(f\"\\n  [Setup] Data already in project directory\")\n",
    "    \n",
    "    # Use auto-detected date range\n",
    "    if 'DATA_START' in dir() and DATA_START is not None:\n",
    "        print(f\"  Date Range: {DATA_START.strftime('%Y-%m-%d')} to {DATA_END.strftime('%Y-%m-%d')} (auto-detected)\")\n",
    "        start_date_str = DATA_START.strftime('%Y-%m-%d')\n",
    "        end_date_str = DATA_END.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        print(f\"  Date Range: Full dataset (no filter)\")\n",
    "        start_date_str = None\n",
    "        end_date_str = None\n",
    "    \n",
    "    print(f\"  Horizons: {HORIZON_LIST}\")\n",
    "    print(f\"  Purge: {PURGE_BARS} bars, Embargo: {EMBARGO_BARS} bars\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        from src.phase1.pipeline_config import PipelineConfig\n",
    "        from src.pipeline.runner import PipelineRunner\n",
    "        \n",
    "        # Configure pipeline with auto-detected dates\n",
    "        # NOTE: auto_scale_purge_embargo=False uses our explicit PURGE_BARS/EMBARGO_BARS\n",
    "        config = PipelineConfig(\n",
    "            symbols=[SYMBOL],\n",
    "            project_root=PROJECT_ROOT,\n",
    "            label_horizons=HORIZON_LIST,\n",
    "            train_ratio=TRAIN_RATIO,\n",
    "            val_ratio=VAL_RATIO,\n",
    "            test_ratio=TEST_RATIO,\n",
    "            purge_bars=PURGE_BARS,\n",
    "            embargo_bars=EMBARGO_BARS,\n",
    "            start_date=start_date_str,\n",
    "            end_date=end_date_str,\n",
    "            allow_batch_symbols=False,  # Single-contract architecture\n",
    "            auto_scale_purge_embargo=False,  # Use explicit purge/embargo values\n",
    "        )\n",
    "        \n",
    "        # Run pipeline\n",
    "        runner = PipelineRunner(config)\n",
    "        success = runner.run()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\n  Pipeline completed in {elapsed/60:.1f} minutes\")\n",
    "            \n",
    "            # Verify output\n",
    "            if (SPLITS_DIR / 'train_scaled.parquet').exists():\n",
    "                for split in ['train', 'val', 'test']:\n",
    "                    df = pd.read_parquet(SPLITS_DIR / f'{split}_scaled.parquet')\n",
    "                    print(f\"  {split}: {len(df):,} samples\")\n",
    "                    del df\n",
    "                gc.collect()\n",
    "                print(\"\\n  Data ready for training!\")\n",
    "        else:\n",
    "            print(\"\\n  [ERROR] Pipeline failed. Check logs above.\")\n",
    "        \n",
    "        del runner, config\n",
    "        if 'clear_memory' in dir():\n",
    "            clear_memory()\n",
    "        else:\n",
    "            gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  [ERROR] Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.3 Verify Processed Data { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined (in case cells run out of order)\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" PROCESSED DATA VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for pre-processed data (local) or pipeline output (Colab)\n",
    "if not IS_COLAB:\n",
    "    # Local: check pre-processed data\n",
    "    local_splits = PROJECT_ROOT / 'data/splits/final_correct/scaled'\n",
    "    if (local_splits / 'train_scaled.parquet').exists():\n",
    "        SPLITS_DIR = local_splits\n",
    "        print(f\"\\nUsing pre-processed data: {SPLITS_DIR}\")\n",
    "\n",
    "if (SPLITS_DIR / 'train_scaled.parquet').exists():\n",
    "    # Load metadata without keeping DataFrames\n",
    "    train_df = pd.read_parquet(SPLITS_DIR / 'train_scaled.parquet')\n",
    "    \n",
    "    FEATURE_COLS = [c for c in train_df.columns \n",
    "                   if not c.startswith(('label_', 'sample_weight', 'quality_', 'datetime', 'symbol'))]\n",
    "    LABEL_COLS = [c for c in train_df.columns if c.startswith('label_')]\n",
    "    TRAIN_LEN = len(train_df)\n",
    "    \n",
    "    # Label distribution with safety check\n",
    "    label_dists = {}\n",
    "    for col in LABEL_COLS:\n",
    "        label_dists[col] = train_df[col].value_counts().sort_index().to_dict()\n",
    "    \n",
    "    del train_df\n",
    "    \n",
    "    # Get val/test sizes\n",
    "    val_df = pd.read_parquet(SPLITS_DIR / 'val_scaled.parquet')\n",
    "    VAL_LEN = len(val_df)\n",
    "    del val_df\n",
    "    \n",
    "    test_df = pd.read_parquet(SPLITS_DIR / 'test_scaled.parquet')\n",
    "    TEST_LEN = len(test_df)\n",
    "    del test_df\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"  Train: {TRAIN_LEN:,} samples\")\n",
    "    print(f\"  Val:   {VAL_LEN:,} samples\")\n",
    "    print(f\"  Test:  {TEST_LEN:,} samples\")\n",
    "    print(f\"  Total: {TRAIN_LEN + VAL_LEN + TEST_LEN:,} samples\")\n",
    "    print(f\"\\n  Features: {len(FEATURE_COLS)}\")\n",
    "    print(f\"  Labels: {LABEL_COLS}\")\n",
    "    \n",
    "    print(f\"\\nLabel Distribution (train):\")\n",
    "    for col, dist in label_dists.items():\n",
    "        total = sum(dist.values())\n",
    "        if total == 0:\n",
    "            print(f\"  {col}: No valid samples!\")\n",
    "            continue\n",
    "        long_pct = dist.get(1, 0) / total * 100\n",
    "        neutral_pct = dist.get(0, 0) / total * 100\n",
    "        short_pct = dist.get(-1, 0) / total * 100\n",
    "        print(f\"  {col}: Long={long_pct:.1f}% | Neutral={neutral_pct:.1f}% | Short={short_pct:.1f}%\")\n",
    "    \n",
    "    # Validate TRAINING_HORIZON is in available labels\n",
    "    if 'TRAINING_HORIZON' in dir() and 'HORIZON_LIST' in dir():\n",
    "        if TRAINING_HORIZON not in HORIZON_LIST:\n",
    "            print(f\"\\n  [WARNING] TRAINING_HORIZON={TRAINING_HORIZON} not in HORIZON_LIST={HORIZON_LIST}\")\n",
    "            print(f\"  Model training may fail. Update TRAINING_HORIZON in Section 1.\")\n",
    "    \n",
    "    DATA_READY = True\n",
    "    print(\"\\n  Data verified and ready for training!\")\n",
    "else:\n",
    "    print(\"\\n[ERROR] Processed data not found!\")\n",
    "    print(f\"  Expected: {SPLITS_DIR}/train_scaled.parquet\")\n",
    "    print(\"  Run Section 3.2 to process raw data.\")\n",
    "    DATA_READY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. PHASE 2: MODEL TRAINING\n",
    "\n",
    "Train selected models on the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Train Models { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'RUN_MODEL_TRAINING' not in dir():\n",
    "    RUN_MODEL_TRAINING = True\n",
    "if 'DATA_READY' not in dir():\n",
    "    DATA_READY = (SPLITS_DIR / 'train_scaled.parquet').exists()\n",
    "if 'MODELS_TO_TRAIN' not in dir():\n",
    "    MODELS_TO_TRAIN = ['xgboost', 'lightgbm', 'catboost']\n",
    "if 'HORIZON_LIST' not in dir():\n",
    "    HORIZON_LIST = [5, 10, 15, 20]\n",
    "if 'USE_CLASS_WEIGHTS' not in dir():\n",
    "    USE_CLASS_WEIGHTS = True\n",
    "if 'USE_SAMPLE_WEIGHTS' not in dir():\n",
    "    USE_SAMPLE_WEIGHTS = True\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "# Define clear_memory if not available\n",
    "if 'clear_memory' not in dir():\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        if GPU_AVAILABLE:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Validate training horizon before starting\n",
    "horizon_valid = True\n",
    "if 'TRAINING_HORIZON' in dir() and TRAINING_HORIZON not in HORIZON_LIST:\n",
    "    print(f\"[ERROR] TRAINING_HORIZON={TRAINING_HORIZON} not in processed horizons {HORIZON_LIST}\")\n",
    "    print(f\"  Update TRAINING_HORIZON in Section 1 to one of: {HORIZON_LIST}\")\n",
    "    horizon_valid = False\n",
    "\n",
    "if not RUN_MODEL_TRAINING:\n",
    "    print(\"[Skipped] Model training disabled in configuration.\")\n",
    "elif not DATA_READY:\n",
    "    print(\"[Error] Data not ready. Run Section 3 first.\")\n",
    "elif not MODELS_TO_TRAIN:\n",
    "    print(\"[Error] No models selected. Enable models in Section 1.\")\n",
    "elif not horizon_valid:\n",
    "    print(\"[Error] Invalid training horizon. See error above.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 2: MODEL TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n  Models: {MODELS_TO_TRAIN}\")\n",
    "    print(f\"  Horizon: H{TRAINING_HORIZON}\")\n",
    "    \n",
    "    # Initialize results dict before training loop\n",
    "    TRAINING_RESULTS = {}\n",
    "    \n",
    "    try:\n",
    "        from src.models import ModelRegistry, Trainer, TrainerConfig\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        # Load data container\n",
    "        print(\"\\nLoading data...\")\n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        print(f\"  Train: {container.splits['train'].n_samples:,}\")\n",
    "        print(f\"  Val: {container.splits['val'].n_samples:,}\")\n",
    "        \n",
    "        \n",
    "        # Create wrapper to control sample weight usage\n",
    "        class ContainerWrapper:\n",
    "            '''Wrapper to optionally disable sample weights.'''\n",
    "            def __init__(self, container, use_weights=True):\n",
    "                self._container = container\n",
    "                self._use_weights = use_weights\n",
    "            \n",
    "            def __getattr__(self, name):\n",
    "                return getattr(self._container, name)\n",
    "            \n",
    "            def get_sklearn_arrays(self, split, return_df=False):\n",
    "                X, y, w = self._container.get_sklearn_arrays(split, return_df)\n",
    "                if not self._use_weights:\n",
    "                    w = None if not return_df else pd.Series(np.ones(len(y)), index=y.index)\n",
    "                return X, y, w\n",
    "            \n",
    "            def get_pytorch_sequences(self, *args, **kwargs):\n",
    "                # Neural models use sequences; sample weights handled differently\n",
    "                return self._container.get_pytorch_sequences(*args, **kwargs)\n",
    "        \n",
    "        # Wrap container if sample weights are disabled\n",
    "        if not USE_SAMPLE_WEIGHTS:\n",
    "            container = ContainerWrapper(container, use_weights=False)\n",
    "            print(\"  Note: Sample weights disabled for training\")\n",
    "        \n",
    "        # Class balance configuration\n",
    "        print(f\"\\n  Class Weights:   {'Enabled' if USE_CLASS_WEIGHTS else 'Disabled'}\")\n",
    "        print(f\"  Sample Weights:  {'Enabled' if USE_SAMPLE_WEIGHTS else 'Disabled'}\")\n",
    "        \n",
    "        # Get training data for class distribution\n",
    "        _, y_train, w_train = container.get_sklearn_arrays('train')\n",
    "        from collections import Counter\n",
    "        class_counts = Counter(y_train)\n",
    "        total_samples = len(y_train)\n",
    "        \n",
    "        print(\"\\n  Class Distribution:\")\n",
    "        class_names = {-1: 'Short', 0: 'Neutral', 1: 'Long'}\n",
    "        for cls in sorted(class_counts.keys()):\n",
    "            count = class_counts[cls]\n",
    "            pct = 100 * count / total_samples\n",
    "            print(f\"    {class_names.get(cls, cls):8s}: {count:7,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Calculate class weights if enabled\n",
    "        class_weights = None\n",
    "        if USE_CLASS_WEIGHTS:\n",
    "            n_classes = len(class_counts)\n",
    "            class_weights = {cls: total_samples / (n_classes * count) for cls, count in class_counts.items()}\n",
    "            print(\"\\n  Computed Class Weights:\")\n",
    "            for cls in sorted(class_weights.keys()):\n",
    "                print(f\"    {class_names.get(cls, cls):8s}: {class_weights[cls]:.4f}\")\n",
    "        \n",
    "        # Sample weights info\n",
    "        if USE_SAMPLE_WEIGHTS and w_train is not None:\n",
    "            print(f\"\\n  Sample Weights: min={w_train.min():.2f}, max={w_train.max():.2f}, mean={w_train.mean():.2f}\")\n",
    "        elif USE_SAMPLE_WEIGHTS:\n",
    "            print(\"\\n  Sample Weights: Not available in data, using uniform weights\")\n",
    "        \n",
    "        # Determine class weight setting for sklearn models\n",
    "        sklearn_class_weight = 'balanced' if USE_CLASS_WEIGHTS else None\n",
    "        \n",
    "        # Control sample weight usage\n",
    "        use_sample_weights = USE_SAMPLE_WEIGHTS\n",
    "        \n",
    "        # Train each model with per-model error handling\n",
    "        for i, model_name in enumerate(MODELS_TO_TRAIN, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\" [{i}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            clear_memory()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Configure model\n",
    "                if model_name in ['lstm', 'gru', 'tcn']:\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        sequence_length=SEQUENCE_LENGTH,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        max_epochs=MAX_EPOCHS,\n",
    "                        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n",
    "                    )\n",
    "                elif model_name == 'catboost':\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        model_config={\n",
    "                            \"iterations\": N_ESTIMATORS,\n",
    "                            \"early_stopping_rounds\": BOOSTING_EARLY_STOPPING,\n",
    "                            \"use_gpu\": False,\n",
    "                            \"task_type\": \"CPU\",\n",
    "                            \"verbose\": False,\n",
    "                        },\n",
    "                    )\n",
    "                elif model_name in ['random_forest', 'logistic', 'svm']:\n",
    "                    # Classical sklearn models with class_weight support\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        model_config={\n",
    "                            'class_weight': sklearn_class_weight,\n",
    "                        },\n",
    "                    )\n",
    "                else:\n",
    "                    # Boosting models (xgboost, lightgbm) and others\n",
    "                    config = TrainerConfig(\n",
    "                        model_name=model_name,\n",
    "                        horizon=TRAINING_HORIZON,\n",
    "                        output_dir=EXPERIMENTS_DIR,\n",
    "                        model_config={\n",
    "                            'n_estimators': N_ESTIMATORS,\n",
    "                            'early_stopping_rounds': BOOSTING_EARLY_STOPPING,\n",
    "                        } if model_name in ['xgboost', 'lightgbm'] else {},\n",
    "                    )\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(config)\n",
    "                results = trainer.run(container)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Store results\n",
    "                metrics = results.get('evaluation_metrics', {})\n",
    "                TRAINING_RESULTS[model_name] = {\n",
    "                    'metrics': metrics,\n",
    "                    'time': elapsed,\n",
    "                    'run_id': results.get('run_id', 'unknown'),\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n  Accuracy: {metrics.get('accuracy', 0):.2%}\")\n",
    "                print(f\"  Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n",
    "                print(f\"  Time: {elapsed:.1f}s\")\n",
    "                \n",
    "                del trainer, config\n",
    "                \n",
    "            except Exception as model_error:\n",
    "                # Per-model error handling - continue to next model\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"\\n  [ERROR] {model_name} training failed: {model_error}\")\n",
    "                TRAINING_RESULTS[model_name] = {\n",
    "                    'metrics': {},\n",
    "                    'time': elapsed,\n",
    "                    'run_id': 'failed',\n",
    "                    'error': str(model_error),\n",
    "                }\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            clear_memory()\n",
    "        \n",
    "        # Save results\n",
    "        results_file = EXPERIMENTS_DIR / 'training_results.json'\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(TRAINING_RESULTS, f, indent=2)\n",
    "        \n",
    "        # Summary\n",
    "        successful = [m for m, r in TRAINING_RESULTS.items() if r.get('run_id') != 'failed']\n",
    "        failed = [m for m, r in TRAINING_RESULTS.items() if r.get('run_id') == 'failed']\n",
    "        print(f\"\\n  Completed: {len(successful)}/{len(MODELS_TO_TRAIN)} models\")\n",
    "        if failed:\n",
    "            print(f\"  Failed: {failed}\")\n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "        \n",
    "        del container\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Training setup failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Compare Models { display-mode: \"form\" }\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure TRAINING_RESULTS is defined\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Check if training results are populated\n",
    "if not TRAINING_RESULTS:\n",
    "    print(\"[WARNING] No trained models found in TRAINING_RESULTS.\")\n",
    "    print(\"Please run Section 4.1 (Model Training) first.\")\n",
    "elif TRAINING_RESULTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" MODEL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Build comparison table\n",
    "    rows = []\n",
    "    for model, data in TRAINING_RESULTS.items():\n",
    "        metrics = data.get('metrics', {})\n",
    "        rows.append({\n",
    "            'Model': model,\n",
    "            'Accuracy': metrics.get('accuracy', 0),\n",
    "            'Macro F1': metrics.get('macro_f1', 0),\n",
    "            'Weighted F1': metrics.get('weighted_f1', 0),\n",
    "            'Time (s)': data.get('time', 0),\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(rows)\n",
    "    comparison_df = comparison_df.sort_values('Macro F1', ascending=False)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Best model\n",
    "    best_model = comparison_df.iloc[0]['Model']\n",
    "    best_f1 = comparison_df.iloc[0]['Macro F1']\n",
    "    print(f\"\\n  Best Model: {best_model} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    # Visualization\n",
    "    if len(TRAINING_RESULTS) > 1:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        sorted_df = comparison_df.sort_values('Accuracy', ascending=True)\n",
    "        axes[0].barh(sorted_df['Model'], sorted_df['Accuracy'], color='steelblue')\n",
    "        axes[0].set_xlabel('Accuracy')\n",
    "        axes[0].set_title('Model Accuracy')\n",
    "        axes[0].set_xlim(0, 1)\n",
    "        \n",
    "        # Training time\n",
    "        sorted_df = comparison_df.sort_values('Time (s)', ascending=True)\n",
    "        axes[1].barh(sorted_df['Model'], sorted_df['Time (s)'], color='coral')\n",
    "        axes[1].set_xlabel('Training Time (seconds)')\n",
    "        axes[1].set_title('Training Time')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 4.3 Visualize Training Results { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "\n",
    "# Visualization toggles\n",
    "show_confusion_matrix = True  #@param {type: \"boolean\"}\n",
    "show_feature_importance = True  #@param {type: \"boolean\"}\n",
    "show_learning_curves = True  #@param {type: \"boolean\"}\n",
    "show_prediction_dist = True  #@param {type: \"boolean\"}\n",
    "show_per_class_metrics = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Check if we have training results\n",
    "if not TRAINING_RESULTS:\n",
    "    print(\"No training results available.\")\n",
    "    print(\"Run Section 4.1 to train models first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" TRAINING VISUALIZATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Filter successful models only\n",
    "    successful_models = {\n",
    "        name: data for name, data in TRAINING_RESULTS.items()\n",
    "        if data.get('run_id') != 'failed' and data.get('metrics')\n",
    "    }\n",
    "    \n",
    "    if not successful_models:\n",
    "        print(\"\\nNo successful models to visualize.\")\n",
    "        print(\"All models failed during training.\")\n",
    "    else:\n",
    "        print(f\"\\nVisualizing {len(successful_models)} models: {list(successful_models.keys())}\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 1. CONFUSION MATRICES\n",
    "        # ============================================================\n",
    "        if show_confusion_matrix:\n",
    "            print(\"\\n[1/5] Generating confusion matrices...\")\n",
    "            \n",
    "            n_models = len(successful_models)\n",
    "            n_cols = min(3, n_models)\n",
    "            n_rows = (n_models + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "            if n_models == 1:\n",
    "                axes = np.array([axes])\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for idx, (model_name, data) in enumerate(successful_models.items()):\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_true = np.array(pred_data.get('y_true', []))\n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_true) > 0 and len(y_pred) > 0:\n",
    "                        cm = confusion_matrix(y_true, y_pred, labels=[-1, 0, 1])\n",
    "                        disp = ConfusionMatrixDisplay(\n",
    "                            confusion_matrix=cm,\n",
    "                            display_labels=['Short', 'Neutral', 'Long']\n",
    "                        )\n",
    "                        disp.plot(ax=axes[idx], cmap='Blues', values_format='d')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}', fontweight='bold')\n",
    "                    else:\n",
    "                        axes[idx].text(0.5, 0.5, 'No predictions available',\n",
    "                                     ha='center', va='center')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}')\n",
    "                else:\n",
    "                    axes[idx].text(0.5, 0.5, 'Predictions not found',\n",
    "                                 ha='center', va='center')\n",
    "                    axes[idx].set_title(f'{model_name.upper()}')\n",
    "            \n",
    "            # Hide extra subplots\n",
    "            for idx in range(n_models, len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # ============================================================\n",
    "        # 2. FEATURE IMPORTANCE (Top 20)\n",
    "        # ============================================================\n",
    "        if show_feature_importance:\n",
    "            print(\"\\n[2/5] Generating feature importance plots...\")\n",
    "            \n",
    "            # Models that support feature importance\n",
    "            boosting_models = ['xgboost', 'lightgbm', 'catboost']\n",
    "            classical_models = ['random_forest']\n",
    "            \n",
    "            fi_models = {\n",
    "                name: data for name, data in successful_models.items()\n",
    "                if name in boosting_models + classical_models\n",
    "            }\n",
    "            \n",
    "            if fi_models:\n",
    "                n_models = len(fi_models)\n",
    "                n_cols = min(2, n_models)\n",
    "                n_rows = (n_models + n_cols - 1) // n_cols\n",
    "                \n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(10*n_cols, 6*n_rows))\n",
    "                if n_models == 1:\n",
    "                    axes = np.array([axes])\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for idx, (model_name, data) in enumerate(fi_models.items()):\n",
    "                    run_id = data.get('run_id', 'unknown')\n",
    "                    fi_file = EXPERIMENTS_DIR / run_id / 'feature_importance.json'\n",
    "                    \n",
    "                    if fi_file.exists():\n",
    "                        with open(fi_file, 'r') as f:\n",
    "                            fi_data = json.load(f)\n",
    "                        \n",
    "                        # Convert to DataFrame and sort\n",
    "                        fi_df = pd.DataFrame(list(fi_data.items()),\n",
    "                                            columns=['feature', 'importance'])\n",
    "                        fi_df = fi_df.sort_values('importance', ascending=False).head(20)\n",
    "                        \n",
    "                        # Plot\n",
    "                        axes[idx].barh(range(len(fi_df)), fi_df['importance'], color='steelblue')\n",
    "                        axes[idx].set_yticks(range(len(fi_df)))\n",
    "                        axes[idx].set_yticklabels(fi_df['feature'], fontsize=8)\n",
    "                        axes[idx].invert_yaxis()\n",
    "                        axes[idx].set_xlabel('Importance', fontweight='bold')\n",
    "                        axes[idx].set_title(f'{model_name.upper()} - Top 20 Features',\n",
    "                                          fontweight='bold')\n",
    "                        axes[idx].grid(axis='x', alpha=0.3)\n",
    "                    else:\n",
    "                        axes[idx].text(0.5, 0.5, 'Feature importance not available',\n",
    "                                     ha='center', va='center')\n",
    "                        axes[idx].set_title(f'{model_name.upper()}')\n",
    "                \n",
    "                # Hide extra subplots\n",
    "                for idx in range(n_models, len(axes)):\n",
    "                    axes[idx].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No models with feature importance (boosting/classical models only)\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 3. LEARNING CURVES (Neural Models)\n",
    "        # ============================================================\n",
    "        if show_learning_curves:\n",
    "            print(\"\\n[3/5] Generating learning curves...\")\n",
    "            \n",
    "            # Neural models that have training history\n",
    "            neural_models = ['lstm', 'gru', 'tcn', 'transformer']\n",
    "            \n",
    "            lc_models = {\n",
    "                name: data for name, data in successful_models.items()\n",
    "                if name in neural_models\n",
    "            }\n",
    "            \n",
    "            if lc_models:\n",
    "                n_models = len(lc_models)\n",
    "                \n",
    "                fig, axes = plt.subplots(n_models, 2, figsize=(12, 4*n_models))\n",
    "                if n_models == 1:\n",
    "                    axes = axes.reshape(1, -1)\n",
    "                \n",
    "                for idx, (model_name, data) in enumerate(lc_models.items()):\n",
    "                    run_id = data.get('run_id', 'unknown')\n",
    "                    history_file = EXPERIMENTS_DIR / run_id / 'training_history.json'\n",
    "                    \n",
    "                    if history_file.exists():\n",
    "                        with open(history_file, 'r') as f:\n",
    "                            history = json.load(f)\n",
    "                        \n",
    "                        epochs = range(1, len(history.get('train_loss', [])) + 1)\n",
    "                        \n",
    "                        # Loss plot\n",
    "                        axes[idx, 0].plot(epochs, history.get('train_loss', []),\n",
    "                                        label='Train', linewidth=2, color='steelblue')\n",
    "                        axes[idx, 0].plot(epochs, history.get('val_loss', []),\n",
    "                                        label='Val', linewidth=2, color='coral')\n",
    "                        axes[idx, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "                        axes[idx, 0].set_ylabel('Loss', fontweight='bold')\n",
    "                        axes[idx, 0].set_title(f'{model_name.upper()} - Loss',\n",
    "                                              fontweight='bold')\n",
    "                        axes[idx, 0].legend()\n",
    "                        axes[idx, 0].grid(alpha=0.3)\n",
    "                        \n",
    "                        # Accuracy plot\n",
    "                        axes[idx, 1].plot(epochs, history.get('train_acc', []),\n",
    "                                        label='Train', linewidth=2, color='steelblue')\n",
    "                        axes[idx, 1].plot(epochs, history.get('val_acc', []),\n",
    "                                        label='Val', linewidth=2, color='coral')\n",
    "                        axes[idx, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "                        axes[idx, 1].set_ylabel('Accuracy', fontweight='bold')\n",
    "                        axes[idx, 1].set_title(f'{model_name.upper()} - Accuracy',\n",
    "                                              fontweight='bold')\n",
    "                        axes[idx, 1].legend()\n",
    "                        axes[idx, 1].grid(alpha=0.3)\n",
    "                    else:\n",
    "                        for col in [0, 1]:\n",
    "                            axes[idx, col].text(0.5, 0.5, 'History not available',\n",
    "                                              ha='center', va='center')\n",
    "                            axes[idx, col].set_title(f'{model_name.upper()}')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No neural models with training history\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 4. PREDICTION DISTRIBUTION\n",
    "        # ============================================================\n",
    "        if show_prediction_dist:\n",
    "            print(\"\\n[4/5] Generating prediction distribution...\")\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Prepare data for stacked bar chart\n",
    "            model_names = []\n",
    "            long_counts = []\n",
    "            neutral_counts = []\n",
    "            short_counts = []\n",
    "            \n",
    "            for model_name, data in successful_models.items():\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_pred) > 0:\n",
    "                        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "                        count_dict = dict(zip(unique, counts))\n",
    "                        total = len(y_pred)\n",
    "                        \n",
    "                        model_names.append(model_name)\n",
    "                        short_counts.append(count_dict.get(-1, 0) / total * 100)\n",
    "                        neutral_counts.append(count_dict.get(0, 0) / total * 100)\n",
    "                        long_counts.append(count_dict.get(1, 0) / total * 100)\n",
    "            \n",
    "            if model_names:\n",
    "                x = np.arange(len(model_names))\n",
    "                width = 0.6\n",
    "                \n",
    "                p1 = ax.bar(x, short_counts, width, label='Short (-1)', color='#d62728')\n",
    "                p2 = ax.bar(x, neutral_counts, width, bottom=short_counts,\n",
    "                           label='Neutral (0)', color='#7f7f7f')\n",
    "                p3 = ax.bar(x, long_counts, width,\n",
    "                           bottom=np.array(short_counts) + np.array(neutral_counts),\n",
    "                           label='Long (1)', color='#2ca02c')\n",
    "                \n",
    "                ax.set_ylabel('Percentage (%)', fontweight='bold')\n",
    "                ax.set_title('Prediction Distribution by Model', fontweight='bold', fontsize=14)\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                ax.legend(loc='upper right')\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                # Add percentage labels\n",
    "                for i, (s, n, l) in enumerate(zip(short_counts, neutral_counts, long_counts)):\n",
    "                    if s > 5:\n",
    "                        ax.text(i, s/2, f'{s:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                    if n > 5:\n",
    "                        ax.text(i, s + n/2, f'{n:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                    if l > 5:\n",
    "                        ax.text(i, s + n + l/2, f'{l:.1f}%', ha='center', va='center',\n",
    "                               fontweight='bold', color='white', fontsize=8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No predictions available for distribution plot\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 5. PER-CLASS METRICS\n",
    "        # ============================================================\n",
    "        if show_per_class_metrics:\n",
    "            print(\"\\n[5/5] Generating per-class metrics...\")\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            model_names = []\n",
    "            short_precision = []\n",
    "            neutral_precision = []\n",
    "            long_precision = []\n",
    "            short_recall = []\n",
    "            neutral_recall = []\n",
    "            long_recall = []\n",
    "            short_f1 = []\n",
    "            neutral_f1 = []\n",
    "            long_f1 = []\n",
    "            \n",
    "            for model_name, data in successful_models.items():\n",
    "                run_id = data.get('run_id', 'unknown')\n",
    "                predictions_file = EXPERIMENTS_DIR / run_id / 'predictions.json'\n",
    "                \n",
    "                if predictions_file.exists():\n",
    "                    with open(predictions_file, 'r') as f:\n",
    "                        pred_data = json.load(f)\n",
    "                    \n",
    "                    y_true = np.array(pred_data.get('y_true', []))\n",
    "                    y_pred = np.array(pred_data.get('y_pred', []))\n",
    "                    \n",
    "                    if len(y_true) > 0 and len(y_pred) > 0:\n",
    "                        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                            y_true, y_pred, labels=[-1, 0, 1], average=None, zero_division=0\n",
    "                        )\n",
    "                        \n",
    "                        model_names.append(model_name)\n",
    "                        short_precision.append(precision[0])\n",
    "                        neutral_precision.append(precision[1])\n",
    "                        long_precision.append(precision[2])\n",
    "                        short_recall.append(recall[0])\n",
    "                        neutral_recall.append(recall[1])\n",
    "                        long_recall.append(recall[2])\n",
    "                        short_f1.append(f1[0])\n",
    "                        neutral_f1.append(f1[1])\n",
    "                        long_f1.append(f1[2])\n",
    "            \n",
    "            if model_names:\n",
    "                x = np.arange(len(model_names))\n",
    "                width = 0.25\n",
    "                \n",
    "                # Precision\n",
    "                axes[0].bar(x - width, short_precision, width, label='Short', color='#d62728')\n",
    "                axes[0].bar(x, neutral_precision, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[0].bar(x + width, long_precision, width, label='Long', color='#2ca02c')\n",
    "                axes[0].set_ylabel('Precision', fontweight='bold')\n",
    "                axes[0].set_title('Precision by Class', fontweight='bold')\n",
    "                axes[0].set_xticks(x)\n",
    "                axes[0].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(axis='y', alpha=0.3)\n",
    "                axes[0].set_ylim(0, 1)\n",
    "                \n",
    "                # Recall\n",
    "                axes[1].bar(x - width, short_recall, width, label='Short', color='#d62728')\n",
    "                axes[1].bar(x, neutral_recall, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[1].bar(x + width, long_recall, width, label='Long', color='#2ca02c')\n",
    "                axes[1].set_ylabel('Recall', fontweight='bold')\n",
    "                axes[1].set_title('Recall by Class', fontweight='bold')\n",
    "                axes[1].set_xticks(x)\n",
    "                axes[1].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[1].legend()\n",
    "                axes[1].grid(axis='y', alpha=0.3)\n",
    "                axes[1].set_ylim(0, 1)\n",
    "                \n",
    "                # F1 Score\n",
    "                axes[2].bar(x - width, short_f1, width, label='Short', color='#d62728')\n",
    "                axes[2].bar(x, neutral_f1, width, label='Neutral', color='#7f7f7f')\n",
    "                axes[2].bar(x + width, long_f1, width, label='Long', color='#2ca02c')\n",
    "                axes[2].set_ylabel('F1 Score', fontweight='bold')\n",
    "                axes[2].set_title('F1 Score by Class', fontweight='bold')\n",
    "                axes[2].set_xticks(x)\n",
    "                axes[2].set_xticklabels([m.upper() for m in model_names], rotation=45, ha='right')\n",
    "                axes[2].legend()\n",
    "                axes[2].grid(axis='y', alpha=0.3)\n",
    "                axes[2].set_ylim(0, 1)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"  No predictions available for per-class metrics\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" VISUALIZATIONS COMPLETE\")\n",
    "        print(\"=\" * 70)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.4 Transformer Attention Visualization { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Visualization settings\n",
    "visualize_attention = True  #@param {type: \"boolean\"}\n",
    "sample_index = 0  #@param {type: \"integer\"}\n",
    "#@markdown Sample index from validation set to visualize\n",
    "layer_to_visualize = -1  #@param {type: \"integer\"}\n",
    "#@markdown Layer index (-1 for last layer, 0 for first)\n",
    "head_to_visualize = 0  #@param {type: \"integer\"}\n",
    "#@markdown Head index to visualize in detail (0-7)\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "if IS_COLAB:\n",
    "    PROJECT_ROOT = Path('/content/research')\n",
    "else:\n",
    "    PROJECT_ROOT = Path.home() / 'Research'\n",
    "\n",
    "EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments' / 'runs'\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data' / 'splits' / 'scaled'\n",
    "\n",
    "if not visualize_attention:\n",
    "    print(\"\u2713 Attention visualization disabled.\")\n",
    "    print(\"  Enable 'visualize_attention' to see transformer attention patterns.\")\n",
    "elif 'TRAINING_RESULTS' not in dir() or not TRAINING_RESULTS:\n",
    "    print(\"\u26a0 No training results found.\")\n",
    "    print(\"  Run Section 4.1 (Model Training) first.\")\n",
    "elif 'transformer' not in TRAINING_RESULTS:\n",
    "    print(\"\u26a0 Transformer model not trained.\")\n",
    "    print(\"  Enable TRAIN_TRANSFORMER in Section 1 and run Section 4.1.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"=\"*80)\n",
    "        print(\"TRANSFORMER ATTENTION VISUALIZATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Get transformer run ID\n",
    "        run_id = TRAINING_RESULTS['transformer']['run_id']\n",
    "        print(f\"\\n[Loading Model]\")\n",
    "        print(f\"  Run ID: {run_id}\")\n",
    "        \n",
    "        # Load container\n",
    "        from src.phase1.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "            path=SPLITS_DIR,\n",
    "            horizon=TRAINING_HORIZON\n",
    "        )\n",
    "        \n",
    "        print(f\"  Horizon: {TRAINING_HORIZON}\")\n",
    "        print(f\"  Validation samples: {len(container.val_X)}\")\n",
    "        \n",
    "        # Load trained transformer\n",
    "        model_path = EXPERIMENTS_DIR / run_id / 'checkpoints'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            print(f\"\\n\u26a0 Model checkpoint not found at {model_path}\")\n",
    "            print(\"  The model may not have been saved during training.\")\n",
    "        else:\n",
    "            # Import transformer model\n",
    "            from src.models import ModelRegistry\n",
    "            from src.models.config import TrainerConfig\n",
    "            import torch\n",
    "            \n",
    "            # Create model instance with same config\n",
    "            config = TrainerConfig(\n",
    "                model_type='transformer',\n",
    "                horizon=TRAINING_HORIZON,\n",
    "                seq_len=TRANSFORMER_SEQUENCE_LENGTH,\n",
    "                d_model=TRANSFORMER_D_MODEL,\n",
    "                n_heads=TRANSFORMER_N_HEADS,\n",
    "                n_layers=TRANSFORMER_N_LAYERS,\n",
    "                dropout=0.1\n",
    "            )\n",
    "            \n",
    "            model = ModelRegistry.create('transformer', config=config.to_dict())\n",
    "            \n",
    "            # Load trained weights\n",
    "            checkpoint_file = list(model_path.glob('*.pt'))\n",
    "            if checkpoint_file:\n",
    "                model.load(model_path)\n",
    "                print(f\"  \u2713 Model loaded from {checkpoint_file[0].name}\")\n",
    "            else:\n",
    "                print(f\"\\n\u26a0 No .pt checkpoint files found in {model_path}\")\n",
    "                raise FileNotFoundError(\"Model checkpoint not found\")\n",
    "            \n",
    "            # Get validation sample\n",
    "            print(f\"\\n[Extracting Sample]\")\n",
    "            print(f\"  Sample index: {sample_index}\")\n",
    "            \n",
    "            if sample_index >= len(container.val_X):\n",
    "                print(f\"  \u26a0 Sample index {sample_index} out of range (max: {len(container.val_X)-1})\")\n",
    "                print(f\"  Using index 0 instead.\")\n",
    "                sample_index = 0\n",
    "            \n",
    "            # Prepare sample\n",
    "            X_val = container.val_X.iloc[[sample_index]]\n",
    "            y_val = container.val_y.iloc[sample_index]\n",
    "            \n",
    "            # Convert to torch tensor\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            # Reshape for transformer: (batch, seq_len, features)\n",
    "            seq_len = config.seq_len\n",
    "            n_features = X_val.shape[1] // seq_len\n",
    "            \n",
    "            X_tensor = torch.FloatTensor(X_val.values).reshape(1, seq_len, n_features).to(device)\n",
    "            \n",
    "            print(f\"  Input shape: {X_tensor.shape}\")\n",
    "            print(f\"  True label: {y_val}\")\n",
    "            \n",
    "            # Get model prediction and attention weights\n",
    "            model.model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Forward pass to get attention\n",
    "                if hasattr(model.model, 'get_attention_weights'):\n",
    "                    attention_weights, prediction = model.model.get_attention_weights(X_tensor, layer_idx=layer_to_visualize)\n",
    "                else:\n",
    "                    # Fallback: hook into transformer layers\n",
    "                    print(\"  \u26a0 Model doesn't have get_attention_weights method\")\n",
    "                    print(\"  Attention visualization requires model modifications\")\n",
    "                    raise NotImplementedError(\"Attention extraction not implemented\")\n",
    "            \n",
    "            # Convert to numpy\n",
    "            attention_weights = attention_weights.cpu().numpy()  # Shape: (batch, n_heads, seq_len, seq_len)\n",
    "            attention_weights = attention_weights[0]  # Remove batch dim: (n_heads, seq_len, seq_len)\n",
    "            prediction = prediction.cpu().numpy()[0]\n",
    "            \n",
    "            print(f\"\\n[Model Output]\")\n",
    "            print(f\"  Prediction: {prediction.argmax()}\")\n",
    "            print(f\"  Confidence: {prediction.max():.2%}\")\n",
    "            print(f\"  Attention shape: {attention_weights.shape}\")\n",
    "            \n",
    "            # Visualize attention heatmaps for all heads\n",
    "            print(f\"\\n[Visualizing Attention Patterns]\")\n",
    "            \n",
    "            n_heads = attention_weights.shape[0]\n",
    "            n_rows = 2\n",
    "            n_cols = (n_heads + n_rows - 1) // n_rows  # Ceiling division\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 8))\n",
    "            if n_heads == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif n_rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for head_idx in range(n_heads):\n",
    "                ax = axes[head_idx]\n",
    "                \n",
    "                # Plot heatmap\n",
    "                sns.heatmap(\n",
    "                    attention_weights[head_idx],\n",
    "                    cmap='viridis',\n",
    "                    ax=ax,\n",
    "                    cbar=True,\n",
    "                    square=True,\n",
    "                    vmin=0,\n",
    "                    vmax=attention_weights[head_idx].max(),\n",
    "                    cbar_kws={'label': 'Attention Weight'}\n",
    "                )\n",
    "                \n",
    "                ax.set_title(f'Head {head_idx+1}', fontsize=12, fontweight='bold')\n",
    "                ax.set_xlabel('Key Position (Source)')\n",
    "                ax.set_ylabel('Query Position (Target)')\n",
    "                \n",
    "                # Add grid for readability\n",
    "                ax.grid(False)\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for idx in range(n_heads, len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "            \n",
    "            layer_name = f\"Layer {layer_to_visualize}\" if layer_to_visualize >= 0 else \"Final Layer\"\n",
    "            plt.suptitle(\n",
    "                f'Transformer Attention Weights - {layer_name}\\nSample {sample_index} | True: {y_val} | Pred: {prediction.argmax()}',\n",
    "                fontsize=14,\n",
    "                fontweight='bold',\n",
    "                y=1.02\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Detailed analysis for selected head\n",
    "            print(f\"\\n[Attention Analysis - Head {head_to_visualize + 1}]\")\n",
    "            \n",
    "            if head_to_visualize >= n_heads:\n",
    "                print(f\"  \u26a0 Head {head_to_visualize} not available (max: {n_heads-1})\")\n",
    "                head_to_visualize = 0\n",
    "            \n",
    "            head_attention = attention_weights[head_to_visualize]\n",
    "            \n",
    "            # Average attention per position (what positions are attended to)\n",
    "            avg_attention_received = head_attention.mean(axis=0)  # Average over queries\n",
    "            avg_attention_given = head_attention.mean(axis=1)     # Average over keys\n",
    "            \n",
    "            print(f\"\\n  Most attended positions (received):\")\n",
    "            top_positions = avg_attention_received.argsort()[-5:][::-1]\n",
    "            for pos in top_positions:\n",
    "                print(f\"    Position {pos:3d}: {avg_attention_received[pos]:.4f}\")\n",
    "            \n",
    "            print(f\"\\n  Most attentive positions (given):\")\n",
    "            top_giving = avg_attention_given.argsort()[-5:][::-1]\n",
    "            for pos in top_giving:\n",
    "                print(f\"    Position {pos:3d}: {avg_attention_given[pos]:.4f}\")\n",
    "            \n",
    "            # Attention entropy (uniformity)\n",
    "            attention_entropy = [entropy(head_attention[i]) for i in range(len(head_attention))]\n",
    "            avg_entropy = np.mean(attention_entropy)\n",
    "            \n",
    "            print(f\"\\n  Attention entropy: {avg_entropy:.4f}\")\n",
    "            print(f\"    (Higher = more uniform, Lower = more focused)\")\n",
    "            \n",
    "            # Interpretability insights\n",
    "            print(f\"\\n[Interpretability Insights]\")\n",
    "            \n",
    "            # Check recency bias\n",
    "            recent_positions = seq_len // 10  # Last 10% of sequence\n",
    "            recent_attention = avg_attention_received[-recent_positions:].sum()\n",
    "            \n",
    "            if recent_attention > 0.3:  # >30% on recent bars\n",
    "                print(f\"  \u2192 Strong recency bias ({recent_attention:.1%} on last {recent_positions} positions)\")\n",
    "                print(f\"     Model relies heavily on most recent observations\")\n",
    "            \n",
    "            # Check long-range dependencies\n",
    "            early_positions = seq_len // 10  # First 10% of sequence\n",
    "            early_attention = avg_attention_received[:early_positions].sum()\n",
    "            \n",
    "            if early_attention > 0.15:  # >15% on early bars\n",
    "                print(f\"  \u2192 Long-range context ({early_attention:.1%} on first {early_positions} positions)\")\n",
    "                print(f\"     Model uses historical information beyond recent bars\")\n",
    "            \n",
    "            # Check attention focus vs spread\n",
    "            if avg_entropy < 2.0:\n",
    "                print(f\"  \u2192 Focused attention (entropy={avg_entropy:.2f})\")\n",
    "                print(f\"     Model concentrates on specific positions\")\n",
    "            elif avg_entropy > 4.0:\n",
    "                print(f\"  \u2192 Distributed attention (entropy={avg_entropy:.2f})\")\n",
    "                print(f\"     Model spreads attention broadly across sequence\")\n",
    "            \n",
    "            # Diagonal attention (position attends to itself)\n",
    "            self_attention = np.diag(head_attention).mean()\n",
    "            if self_attention > 0.2:\n",
    "                print(f\"  \u2192 Self-attention ({self_attention:.1%} average)\")\n",
    "                print(f\"     Positions attend to themselves (local context)\")\n",
    "            \n",
    "            print(f\"\\n\u2713 Attention visualization complete\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n\u26a0 Error: {e}\")\n",
    "        print(\"  The transformer model checkpoint was not found.\")\n",
    "        print(\"  Make sure the model completed training in Section 4.1.\")\n",
    "        \n",
    "    except NotImplementedError as e:\n",
    "        print(f\"\\n\u26a0 {e}\")\n",
    "        print(\"  The transformer model needs modifications to extract attention weights.\")\n",
    "        print(\"  Add a 'get_attention_weights' method to the transformer model class.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u26a0 Error during attention visualization:\")\n",
    "        print(f\"  {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title 4.5 Test Set Performance { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Configuration options\n",
    "run_test_evaluation = True  #@param {type: \"boolean\"}\n",
    "show_sample_predictions = True  #@param {type: \"boolean\"}\n",
    "n_samples_to_show = 20  #@param {type: \"integer\"}\n",
    "show_generalization_gap = True  #@param {type: \"boolean\"}\n",
    "save_test_predictions = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "\n",
    "# Check if we can run test evaluation\n",
    "if not run_test_evaluation:\n",
    "    print(\"[Skipped] Test evaluation disabled. Enable checkbox above to run.\")\n",
    "elif not TRAINING_RESULTS:\n",
    "    print(\"[Error] No training results available.\")\n",
    "    print(\"Run Section 4.1 to train models first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" TEST SET PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Filter successful models only\n",
    "    successful_models = {\n",
    "        name: data for name, data in TRAINING_RESULTS.items()\n",
    "        if data.get('run_id') != 'failed' and data.get('metrics')\n",
    "    }\n",
    "    \n",
    "    if not successful_models:\n",
    "        print(\"\\n[Error] No successful models to evaluate.\")\n",
    "        print(\"All models failed during training.\")\n",
    "    else:\n",
    "        try:\n",
    "            # ============================================================\n",
    "            # LOAD TEST DATA\n",
    "            # ============================================================\n",
    "            print(f\"\\n[1/5] Loading test data...\")\n",
    "            \n",
    "            from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "            \n",
    "            container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "                path=SPLITS_DIR,\n",
    "                horizon=TRAINING_HORIZON\n",
    "            )\n",
    "            \n",
    "            # Get test data\n",
    "            test_split = container.splits.get('test')\n",
    "            if test_split is None:\n",
    "                print(\"  [ERROR] Test split not found in container!\")\n",
    "                raise ValueError(\"Test data not available\")\n",
    "            \n",
    "            X_test = test_split.features\n",
    "            y_test = test_split.labels\n",
    "            \n",
    "            print(f\"  Test samples: {len(X_test):,}\")\n",
    "            print(f\"  Features: {X_test.shape[1]}\")\n",
    "            \n",
    "            # ============================================================\n",
    "            # RUN PREDICTIONS ON TEST SET\n",
    "            # ============================================================\n",
    "            print(f\"\\n[2/5] Running predictions on test set...\")\n",
    "            \n",
    "            TEST_RESULTS = {}\n",
    "            \n",
    "            for model_name, train_data in successful_models.items():\n",
    "                print(f\"\\n  Evaluating: {model_name.upper()}\")\n",
    "                \n",
    "                try:\n",
    "                    run_id = train_data.get('run_id', 'unknown')\n",
    "                    model_dir = EXPERIMENTS_DIR / run_id\n",
    "                    \n",
    "                    # Load model from checkpoints\n",
    "                    checkpoint_dir = model_dir / 'checkpoints'\n",
    "                    \n",
    "                    # Try different model file formats\n",
    "                    model_loaded = False\n",
    "                    model = None\n",
    "                    \n",
    "                    # Method 1: Try pickle format\n",
    "                    pickle_path = checkpoint_dir / 'model.pkl'\n",
    "                    if pickle_path.exists():\n",
    "                        with open(pickle_path, 'rb') as f:\n",
    "                            model = pickle.load(f)\n",
    "                        model_loaded = True\n",
    "                        print(f\"    Loaded from: {pickle_path.name}\")\n",
    "                    \n",
    "                    # Method 2: Try joblib format\n",
    "                    if not model_loaded:\n",
    "                        joblib_path = checkpoint_dir / 'model.joblib'\n",
    "                        if joblib_path.exists():\n",
    "                            model = joblib.load(joblib_path)\n",
    "                            model_loaded = True\n",
    "                            print(f\"    Loaded from: {joblib_path.name}\")\n",
    "                    \n",
    "                    # Method 3: Try PyTorch format (for neural models)\n",
    "                    if not model_loaded and model_name in ['lstm', 'gru', 'tcn', 'transformer']:\n",
    "                        torch_path = checkpoint_dir / 'model.pt'\n",
    "                        if torch_path.exists():\n",
    "                            import torch\n",
    "                            from src.models import ModelRegistry\n",
    "                            \n",
    "                            # Recreate model architecture\n",
    "                            model = ModelRegistry.create(model_name, config={\n",
    "                                'input_size': X_test.shape[1],\n",
    "                                'hidden_size': 128,\n",
    "                                'num_layers': 2,\n",
    "                            })\n",
    "                            \n",
    "                            # Load weights\n",
    "                            state_dict = torch.load(torch_path, map_location='cpu')\n",
    "                            model.model.load_state_dict(state_dict)\n",
    "                            model.model.eval()\n",
    "                            model_loaded = True\n",
    "                            print(f\"    Loaded from: {torch_path.name}\")\n",
    "                    \n",
    "                    if not model_loaded:\n",
    "                        print(f\"    [WARNING] Model file not found in {checkpoint_dir}\")\n",
    "                        print(f\"    Skipping {model_name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    if hasattr(model, 'predict'):\n",
    "                        # Sklearn-style models\n",
    "                        if model_name in ['xgboost', 'lightgbm', 'catboost', 'random_forest', 'logistic', 'svm']:\n",
    "                            y_pred = model.predict(X_test)\n",
    "                        else:\n",
    "                            # Neural models - may need special handling\n",
    "                            pred_result = model.predict(X_test)\n",
    "                            if hasattr(pred_result, 'class_predictions'):\n",
    "                                y_pred = pred_result.class_predictions\n",
    "                            else:\n",
    "                                y_pred = pred_result\n",
    "                    else:\n",
    "                        print(f\"    [WARNING] Model has no predict method\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate test metrics\n",
    "                    test_acc = accuracy_score(y_test, y_pred)\n",
    "                    test_macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    test_weighted_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                    test_precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    test_recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    # Per-class metrics\n",
    "                    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    per_class_precision = precision_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    per_class_recall = recall_score(y_test, y_pred, average=None, labels=[-1, 0, 1], zero_division=0)\n",
    "                    \n",
    "                    # Confusion matrix\n",
    "                    cm = confusion_matrix(y_test, y_pred, labels=[-1, 0, 1])\n",
    "                    \n",
    "                    # Store results\n",
    "                    val_metrics = train_data.get('metrics', {})\n",
    "                    \n",
    "                    TEST_RESULTS[model_name] = {\n",
    "                        'test_metrics': {\n",
    "                            'accuracy': test_acc,\n",
    "                            'macro_f1': test_macro_f1,\n",
    "                            'weighted_f1': test_weighted_f1,\n",
    "                            'precision': test_precision,\n",
    "                            'recall': test_recall,\n",
    "                            'per_class_f1': per_class_f1.tolist(),\n",
    "                            'per_class_precision': per_class_precision.tolist(),\n",
    "                            'per_class_recall': per_class_recall.tolist(),\n",
    "                            'confusion_matrix': cm.tolist(),\n",
    "                        },\n",
    "                        'val_metrics': val_metrics,\n",
    "                        'predictions': y_pred.tolist() if hasattr(y_pred, 'tolist') else list(y_pred),\n",
    "                        'run_id': run_id,\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"    Test Accuracy: {test_acc:.2%}\")\n",
    "                    print(f\"    Test Macro F1: {test_macro_f1:.4f}\")\n",
    "                    \n",
    "                    # Save predictions if requested\n",
    "                    if save_test_predictions:\n",
    "                        test_pred_file = model_dir / 'test_predictions.json'\n",
    "                        with open(test_pred_file, 'w') as f:\n",
    "                            json.dump({\n",
    "                                'y_true': y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test),\n",
    "                                'y_pred': y_pred.tolist() if hasattr(y_pred, 'tolist') else list(y_pred),\n",
    "                                'test_metrics': TEST_RESULTS[model_name]['test_metrics'],\n",
    "                            }, f, indent=2)\n",
    "                    \n",
    "                    del model\n",
    "                    \n",
    "                except Exception as model_error:\n",
    "                    print(f\"    [ERROR] Failed to evaluate {model_name}: {model_error}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            # ============================================================\n",
    "            # DISPLAY COMPARISON TABLE\n",
    "            # ============================================================\n",
    "            if TEST_RESULTS:\n",
    "                print(f\"\\n[3/5] Model Performance Comparison\")\n",
    "                print(\"=\" * 70)\n",
    "                \n",
    "                # Build comparison DataFrame\n",
    "                comparison_data = []\n",
    "                for model_name, results in TEST_RESULTS.items():\n",
    "                    val_metrics = results['val_metrics']\n",
    "                    test_metrics = results['test_metrics']\n",
    "                    \n",
    "                    val_acc = val_metrics.get('accuracy', 0)\n",
    "                    test_acc = test_metrics.get('accuracy', 0)\n",
    "                    val_f1 = val_metrics.get('macro_f1', 0)\n",
    "                    test_f1 = test_metrics.get('macro_f1', 0)\n",
    "                    \n",
    "                    # Calculate generalization gap\n",
    "                    acc_gap = (test_acc - val_acc) * 100\n",
    "                    f1_gap = (test_f1 - val_f1) * 100\n",
    "                    \n",
    "                    comparison_data.append({\n",
    "                        'Model': model_name,\n",
    "                        'Val Acc': f\"{val_acc:.2%}\",\n",
    "                        'Test Acc': f\"{test_acc:.2%}\",\n",
    "                        'Val F1': f\"{val_f1:.4f}\",\n",
    "                        'Test F1': f\"{test_f1:.4f}\",\n",
    "                        'Acc Gap (%)': f\"{acc_gap:+.2f}\",\n",
    "                        'F1 Gap (%)': f\"{f1_gap:+.2f}\",\n",
    "                    })\n",
    "                \n",
    "                comparison_df = pd.DataFrame(comparison_data)\n",
    "                \n",
    "                # Sort by test F1 score\n",
    "                comparison_df = comparison_df.sort_values(\n",
    "                    by='Test F1',\n",
    "                    ascending=False,\n",
    "                    key=lambda x: x.str.replace('%', '').astype(float) if x.dtype == 'object' else x\n",
    "                )\n",
    "                \n",
    "                print(\"\\n\")\n",
    "                print(comparison_df.to_string(index=False))\n",
    "                \n",
    "                # Best performing model\n",
    "                best_model_name = comparison_df.iloc[0]['Model']\n",
    "                best_test_f1 = comparison_df.iloc[0]['Test F1']\n",
    "                print(f\"\\n  Best Model on Test Set: {best_model_name} (F1: {best_test_f1})\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # GENERALIZATION ANALYSIS\n",
    "                # ============================================================\n",
    "                if show_generalization_gap:\n",
    "                    print(f\"\\n[4/5] Generalization Analysis\")\n",
    "                    print(\"=\" * 70)\n",
    "                    \n",
    "                    for model_name, results in TEST_RESULTS.items():\n",
    "                        val_metrics = results['val_metrics']\n",
    "                        test_metrics = results['test_metrics']\n",
    "                        \n",
    "                        val_f1 = val_metrics.get('macro_f1', 0)\n",
    "                        test_f1 = test_metrics.get('macro_f1', 0)\n",
    "                        \n",
    "                        gap_pct = ((test_f1 - val_f1) / val_f1 * 100) if val_f1 > 0 else 0\n",
    "                        \n",
    "                        # Color-code based on gap\n",
    "                        if abs(gap_pct) < 2:\n",
    "                            status = \"\u2713 EXCELLENT\"\n",
    "                            color = \"green\"\n",
    "                        elif abs(gap_pct) < 5:\n",
    "                            status = \"~ GOOD\"\n",
    "                            color = \"yellow\"\n",
    "                        else:\n",
    "                            status = \"\u26a0 POOR\"\n",
    "                            color = \"red\"\n",
    "                        \n",
    "                        print(f\"\\n  {model_name.upper()}:\")\n",
    "                        print(f\"    Val F1:  {val_f1:.4f}\")\n",
    "                        print(f\"    Test F1: {test_f1:.4f}\")\n",
    "                        print(f\"    Gap:     {gap_pct:+.2f}% [{status}]\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # SAMPLE PREDICTIONS\n",
    "                # ============================================================\n",
    "                if show_sample_predictions and n_samples_to_show > 0:\n",
    "                    print(f\"\\n[5/5] Sample Predictions (first {n_samples_to_show})\")\n",
    "                    print(\"=\" * 70)\n",
    "                    \n",
    "                    # Show actual labels\n",
    "                    sample_actual = y_test[:n_samples_to_show]\n",
    "                    print(f\"\\n  Actual:     {list(sample_actual)}\")\n",
    "                    \n",
    "                    # Show predictions for each model\n",
    "                    for model_name, results in TEST_RESULTS.items():\n",
    "                        predictions = results['predictions']\n",
    "                        sample_pred = predictions[:n_samples_to_show]\n",
    "                        \n",
    "                        # Calculate accuracy for this sample\n",
    "                        matches = sum(1 for a, p in zip(sample_actual, sample_pred) if a == p)\n",
    "                        sample_acc = matches / len(sample_actual) * 100\n",
    "                        \n",
    "                        print(f\"  {model_name:12s}: {sample_pred} ({sample_acc:.1f}% match)\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 70)\n",
    "                print(\" TEST EVALUATION COMPLETE\")\n",
    "                print(\"=\" * 70)\n",
    "                \n",
    "                print(f\"\\n  Evaluated: {len(TEST_RESULTS)} models\")\n",
    "                print(f\"  Test samples: {len(X_test):,}\")\n",
    "                \n",
    "                if save_test_predictions:\n",
    "                    print(f\"  Predictions saved to: {EXPERIMENTS_DIR}/[run_id]/test_predictions.json\")\n",
    "            else:\n",
    "                print(\"\\n[WARNING] No test results generated.\")\n",
    "                print(\"All models failed to load or predict.\")\n",
    "            \n",
    "            # Clean up\n",
    "            del container, X_test, y_test\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] Test evaluation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. PHASE 3: CROSS-VALIDATION (Optional)\n",
    "\n",
    "Run purged K-fold cross-validation for robust model evaluation.\n",
    "\n",
    "## Important Notes on Scaling\n",
    "\n",
    "**CV Scaling Approach:** By default, cross-validation uses pre-scaled data from the pipeline (scaled using train-split statistics). This approach:\n",
    "\n",
    "- **Pros:** Faster execution, consistent with training/test evaluation\n",
    "- **Cons:** Slight data leakage since scaler statistics include all train samples, not just the current CV fold\n",
    "\n",
    "**When Pre-Scaled CV is Acceptable:**\n",
    "- Model comparison (all models have identical bias)\n",
    "- Hyperparameter tuning (relative performance is what matters)\n",
    "- Quick iteration during development\n",
    "\n",
    "**When Per-Fold Scaling is Better:**\n",
    "- Final production model validation\n",
    "- Publishing results for academic/research purposes\n",
    "- Maximum methodological rigor required\n",
    "\n",
    "Set `CV_USE_PRESCALED = False` to enable per-fold scaling (slower but stricter).\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Run Cross-Validation { display-mode: \"form\" }\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'RUN_CROSS_VALIDATION' not in dir():\n",
    "    RUN_CROSS_VALIDATION = False\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'USE_CLASS_WEIGHTS' not in dir():\n",
    "    USE_CLASS_WEIGHTS = True\n",
    "if 'USE_SAMPLE_WEIGHTS' not in dir():\n",
    "    USE_SAMPLE_WEIGHTS = True\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "if 'CV_TUNE_HYPERPARAMS' not in dir():\n",
    "    CV_TUNE_HYPERPARAMS = False\n",
    "if 'CV_N_TRIALS' not in dir():\n",
    "    CV_N_TRIALS = 20\n",
    "if 'CV_USE_PRESCALED' not in dir():\n",
    "    CV_USE_PRESCALED = True\n",
    "\n",
    "# Define clear_memory if not available\n",
    "if 'clear_memory' not in dir():\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        if GPU_AVAILABLE:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize global results dictionaries\n",
    "if 'CV_RESULTS' not in dir():\n",
    "    CV_RESULTS = {}\n",
    "if 'TUNING_RESULTS' not in dir():\n",
    "    TUNING_RESULTS = {}\n",
    "\n",
    "# Main cross-validation logic\n",
    "if not RUN_CROSS_VALIDATION:\n",
    "    print(\"[Skipped] Cross-validation disabled in configuration.\")\n",
    "    print(\"Set RUN_CROSS_VALIDATION = True in Section 1 to enable.\")\n",
    "elif not TRAINING_RESULTS:\n",
    "    print(\"[WARNING] No trained models found in TRAINING_RESULTS.\")\n",
    "    print(\"Please run Section 4.1 (Model Training) first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 3: CROSS-VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # CV Scaling Documentation\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"CROSS-VALIDATION SCALING NOTES\")\n",
    "    print(\"-\" * 70)\n",
    "    if CV_USE_PRESCALED:\n",
    "        print(\"Mode: PRE-SCALED DATA (default)\")\n",
    "        print(\"  - Data is pre-scaled using train-split statistics (from pipeline)\")\n",
    "        print(\"  - Faster execution, suitable for model comparison\")\n",
    "        print(\"  - Minor scaling leakage: scaler fit on all train samples, not per-fold\")\n",
    "        print(\"  - Set CV_USE_PRESCALED=False for stricter per-fold scaling\")\n",
    "    else:\n",
    "        print(\"Mode: PER-FOLD SCALING (strict)\")\n",
    "        print(\"  - Scaler is fit only on each fold's training data\")\n",
    "        print(\"  - No scaling leakage between CV folds\")\n",
    "        print(\"  - Recommended for final validation and published results\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    try:\n",
    "        from src.cross_validation import PurgedKFold, PurgedKFoldConfig\n",
    "        from src.cross_validation.cv_runner import TimeSeriesOptunaTuner\n",
    "        from src.cross_validation.param_spaces import get_param_space\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        from src.models import ModelRegistry\n",
    "        from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "        # Load data\n",
    "        if CV_USE_PRESCALED:\n",
    "            # Use pre-scaled data (default - faster)\n",
    "            container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "                path=SPLITS_DIR,\n",
    "                horizon=TRAINING_HORIZON\n",
    "            )\n",
    "            X, y, _ = container.get_sklearn_arrays('train')\n",
    "            scaler = None\n",
    "            print(f\"\\nData (pre-scaled): {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "        else:\n",
    "            # Per-fold scaling: load unscaled data\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            unscaled_dir = PROJECT_ROOT / 'data/splits/unscaled'\n",
    "            if unscaled_dir.exists():\n",
    "                container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "                    path=unscaled_dir,\n",
    "                    horizon=TRAINING_HORIZON\n",
    "                )\n",
    "                X, y, _ = container.get_sklearn_arrays('train')\n",
    "                scaler = RobustScaler()  # Will fit per fold\n",
    "                print(f\"\\nData (unscaled): {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "                print(\"  Scaling will be applied per-fold (strict mode)\")\n",
    "            else:\n",
    "                print(f\"  [WARNING] Unscaled data not found at {unscaled_dir}\")\n",
    "                print(\"  Falling back to pre-scaled data\")\n",
    "                container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "                    path=SPLITS_DIR,\n",
    "                    horizon=TRAINING_HORIZON\n",
    "                )\n",
    "                X, y, _ = container.get_sklearn_arrays('train')\n",
    "                scaler = None\n",
    "                print(f\"\\nData (pre-scaled, fallback): {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "\n",
    "        # Configure CV\n",
    "        cv_config = PurgedKFoldConfig(\n",
    "            n_splits=CV_N_SPLITS,\n",
    "            purge_bars=PURGE_BARS,\n",
    "            embargo_bars=EMBARGO_BARS,\n",
    "        )\n",
    "        cv = PurgedKFold(cv_config)\n",
    "        print(f\"CV: {CV_N_SPLITS} folds, purge={PURGE_BARS}, embargo={EMBARGO_BARS}\")\n",
    "\n",
    "        # Get list of successfully trained models\n",
    "        successful_models = [\n",
    "            m for m in TRAINING_RESULTS.keys() \n",
    "            if TRAINING_RESULTS[m].get('status') == 'success'\n",
    "        ]\n",
    "        \n",
    "        if not successful_models:\n",
    "            print(\"\\n[WARNING] No successfully trained models found.\")\n",
    "            print(\"Train models first in Section 4.\")\n",
    "        else:\n",
    "            print(f\"\\nRunning CV for {len(successful_models)} models: {', '.join(successful_models)}\")\n",
    "            \n",
    "            # Run CV for ALL trained models\n",
    "            cv_summary_data = []\n",
    "            \n",
    "            for model_name in tqdm(successful_models, desc=\"Cross-Validation\"):\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Model: {model_name}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                \n",
    "                # Hyperparameter tuning (if enabled)\n",
    "                tuned_params = {}\n",
    "                if CV_TUNE_HYPERPARAMS:\n",
    "                    param_space = get_param_space(model_name)\n",
    "                    if param_space:\n",
    "                        print(f\"  Tuning hyperparameters ({CV_N_TRIALS} trials)...\")\n",
    "                        try:\n",
    "                            tuner = TimeSeriesOptunaTuner(\n",
    "                                model_name=model_name,\n",
    "                                cv=cv,\n",
    "                                n_trials=CV_N_TRIALS,\n",
    "                                direction=\"maximize\",\n",
    "                                metric=\"f1\"\n",
    "                            )\n",
    "                            tuning_result = tuner.tune(\n",
    "                                X=pd.DataFrame(X),\n",
    "                                y=pd.Series(y),\n",
    "                                sample_weights=None,\n",
    "                                param_space=param_space\n",
    "                            )\n",
    "                            \n",
    "                            if not tuning_result.get('skipped', False):\n",
    "                                tuned_params = tuning_result.get('best_params', {})\n",
    "                                best_value = tuning_result.get('best_value', 0.0)\n",
    "                                TUNING_RESULTS[model_name] = {\n",
    "                                    'best_params': tuned_params,\n",
    "                                    'best_value': best_value,\n",
    "                                    'n_trials': tuning_result.get('n_trials', 0)\n",
    "                                }\n",
    "                                print(f\"  Best F1: {best_value:.4f}\")\n",
    "                                print(f\"  Best params: {tuned_params}\")\n",
    "                            else:\n",
    "                                print(f\"    [Skipped] No tuning support or Optuna not installed\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"    [Warning] Tuning failed: {e}\")\n",
    "                    else:\n",
    "                        print(f\"  [Skipped] No param space defined for {model_name}\")\n",
    "                \n",
    "                # Get model config (use tuned params if available)\n",
    "                try:\n",
    "                    default_config = ModelRegistry.get_model_info(model_name).get('default_config', {})\n",
    "                except:\n",
    "                    default_config = {\n",
    "                        'n_estimators': N_ESTIMATORS,\n",
    "                        'early_stopping_rounds': BOOSTING_EARLY_STOPPING,\n",
    "                    }\n",
    "                model_config = {**default_config, **tuned_params}\n",
    "                \n",
    "                # Run cross-validation\n",
    "                print(f\"  Running {CV_N_SPLITS}-fold CV...\")\n",
    "                fold_scores = []\n",
    "                fold_details = []\n",
    "                \n",
    "                for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "                    X_train_raw, X_val_raw = X[train_idx], X[val_idx]\n",
    "                    y_train, y_val = y[train_idx], y[val_idx]\n",
    "                    \n",
    "                    # Apply per-fold scaling if enabled\n",
    "                    if scaler is not None and not CV_USE_PRESCALED:\n",
    "                        from sklearn.preprocessing import RobustScaler\n",
    "                        fold_scaler = RobustScaler()\n",
    "                        X_train = fold_scaler.fit_transform(X_train_raw)\n",
    "                        X_val = fold_scaler.transform(X_val_raw)\n",
    "                    else:\n",
    "                        X_train, X_val = X_train_raw, X_val_raw\n",
    "                    \n",
    "                    # Train model\n",
    "                    model = ModelRegistry.create(model_name, config=model_config)\n",
    "                    model.fit(X_train, y_train, X_val, y_val)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    predictions = model.predict(X_val)\n",
    "                    y_pred = predictions.class_predictions\n",
    "                    \n",
    "                    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "                    acc = accuracy_score(y_val, y_pred)\n",
    "                    prec = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "                    rec = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    fold_scores.append(f1)\n",
    "                    fold_details.append({\n",
    "                        'fold': fold_idx,\n",
    "                        'f1': f1,\n",
    "                        'accuracy': acc,\n",
    "                        'precision': prec,\n",
    "                        'recall': rec,\n",
    "                        'train_size': len(train_idx),\n",
    "                        'val_size': len(val_idx)\n",
    "                    })\n",
    "                    \n",
    "                    del model\n",
    "                    clear_memory()\n",
    "                \n",
    "                # Calculate CV statistics\n",
    "                mean_f1 = np.mean(fold_scores)\n",
    "                std_f1 = np.std(fold_scores)\n",
    "                best_f1 = np.max(fold_scores)\n",
    "                \n",
    "                # Stability grading\n",
    "                if std_f1 < 0.01:\n",
    "                    stability = \"Excellent\"\n",
    "                elif std_f1 < 0.02:\n",
    "                    stability = \"Good\"\n",
    "                elif std_f1 < 0.04:\n",
    "                    stability = \"Fair\"\n",
    "                else:\n",
    "                    stability = \"Poor\"\n",
    "                \n",
    "                # Store results\n",
    "                CV_RESULTS[model_name] = {\n",
    "                    'mean_f1': mean_f1,\n",
    "                    'std_f1': std_f1,\n",
    "                    'best_f1': best_f1,\n",
    "                    'fold_scores': fold_scores,\n",
    "                    'fold_details': fold_details,\n",
    "                    'stability': stability,\n",
    "                    'tuned_params': tuned_params\n",
    "                }\n",
    "                \n",
    "                cv_summary_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'CV Mean F1': mean_f1,\n",
    "                    'CV Std': std_f1,\n",
    "                    'Best F1': best_f1,\n",
    "                    'Stability': stability\n",
    "                })\n",
    "                \n",
    "                print(f\"  Mean F1: {mean_f1:.4f} (+/- {std_f1:.4f})\")\n",
    "                print(f\"  Best F1: {best_f1:.4f}\")\n",
    "                print(f\"  Stability: {stability}\")\n",
    "            \n",
    "            # Display CV summary table\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\" CROSS-VALIDATION SUMMARY\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            cv_summary_df = pd.DataFrame(cv_summary_data)\n",
    "            cv_summary_df = cv_summary_df.sort_values('CV Mean F1', ascending=False)\n",
    "            print(cv_summary_df.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Cross-validation complete for {len(successful_models)} models\")\n",
    "            print(f\"{'='*70}\")\n",
    "        \n",
    "        del container, X, y\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Cross-validation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.2 Hyperparameter Tuning Results { display-mode: \"form\" }\n",
    "#@markdown Display hyperparameter tuning results and recommendations.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "show_retrain_recommendation = True  #@param {type: \"boolean\"}\n",
    "show_optimization_plots = False  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'TUNING_RESULTS' not in dir():\n",
    "    TUNING_RESULTS = {}\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'CV_TUNE_HYPERPARAMS' not in dir():\n",
    "    CV_TUNE_HYPERPARAMS = False\n",
    "\n",
    "# Check if tuning was run\n",
    "if not CV_TUNE_HYPERPARAMS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n[Skipped] Hyperparameter tuning not enabled.\")\n",
    "    print(\"\\nTo enable tuning:\")\n",
    "    print(\"  1. Set CV_TUNE_HYPERPARAMS = True in Section 1\")\n",
    "    print(\"  2. Run Cross-Validation (Section 5.1)\")\n",
    "    print(f\"  3. Configure CV_N_TRIALS (currently: {globals().get('CV_N_TRIALS', 20)})\")\n",
    "elif not TUNING_RESULTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n[WARNING] No tuning results available.\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  - Cross-validation hasn't been run yet\")\n",
    "    print(\"  - No models have param spaces defined\")\n",
    "    print(\"  - Optuna is not installed\")\n",
    "    print(\"\\nPlease run Section 5.1 (Cross-Validation) first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nTuned {len(TUNING_RESULTS)} model(s)\")\n",
    "    print(f\"Trials per model: {globals().get('CV_N_TRIALS', 20)}\\n\")\n",
    "    \n",
    "    # Display results for each model\n",
    "    for model_name, results in TUNING_RESULTS.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        best_params = results.get('best_params', {})\n",
    "        best_value = results.get('best_value', 0.0)\n",
    "        n_trials = results.get('n_trials', 0)\n",
    "        \n",
    "        print(f\"\\nOptimization Summary:\")\n",
    "        print(f\"  Trials completed: {n_trials}\")\n",
    "        print(f\"  Best F1 score:    {best_value:.4f}\")\n",
    "        \n",
    "        if best_params:\n",
    "            print(f\"\\n  Best Parameters:\")\n",
    "            # Get default params for comparison\n",
    "            try:\n",
    "                from src.models import ModelRegistry\n",
    "                model_info = ModelRegistry.get_model_info(model_name)\n",
    "                default_config = model_info.get('default_config', {})\n",
    "            except:\n",
    "                default_config = {}\n",
    "            \n",
    "            # Create parameter comparison table\n",
    "            param_data = []\n",
    "            for param_name, tuned_value in best_params.items():\n",
    "                default_value = default_config.get(param_name, None)\n",
    "                # Calculate change\n",
    "                if default_value is not None:\n",
    "                    if isinstance(tuned_value, (int, float)) and isinstance(default_value, (int, float)):\n",
    "                        change_pct = ((tuned_value - default_value) / default_value * 100) if default_value != 0 else 0\n",
    "                        change_str = f\"{change_pct:+.1f}%\"\n",
    "                    else:\n",
    "                        change_str = \"changed\"\n",
    "                else:\n",
    "                    change_str = \"new\"\n",
    "                \n",
    "                param_data.append({\n",
    "                    'Parameter': param_name,\n",
    "                    'Default': str(default_value) if default_value is not None else 'N/A',\n",
    "                    'Tuned': str(tuned_value),\n",
    "                    'Change': change_str\n",
    "                })\n",
    "            \n",
    "            if param_data:\n",
    "                param_df = pd.DataFrame(param_data)\n",
    "                print(\"\\n\" + param_df.to_string(index=False))\n",
    "            else:\n",
    "                for param_name, value in best_params.items():\n",
    "                    print(f\"    {param_name}: {value}\")\n",
    "        \n",
    "        # Calculate improvement over default\n",
    "        if model_name in TRAINING_RESULTS:\n",
    "            default_f1 = TRAINING_RESULTS[model_name].get('metrics', {}).get('macro_f1', 0.0)\n",
    "            improvement = ((best_value - default_f1) / default_f1 * 100) if default_f1 > 0 else 0\n",
    "            print(f\"\\n  Improvement Analysis:\")\n",
    "            print(f\"    Default F1:     {default_f1:.4f}\")\n",
    "            print(f\"    Tuned F1:       {best_value:.4f}\")\n",
    "            print(f\"    Improvement:    {improvement:+.2f}%\")\n",
    "        print()\n",
    "    \n",
    "    # Show retrain recommendations\n",
    "    if show_retrain_recommendation:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\" RETRAIN RECOMMENDATIONS\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        recommendations = []\n",
    "        for model_name, results in TUNING_RESULTS.items():\n",
    "            best_value = results.get('best_value', 0.0)\n",
    "            # Calculate improvement\n",
    "            if model_name in TRAINING_RESULTS:\n",
    "                default_f1 = TRAINING_RESULTS[model_name].get('metrics', {}).get('macro_f1', 0.0)\n",
    "                improvement = ((best_value - default_f1) / default_f1 * 100) if default_f1 > 0 else 0\n",
    "                recommendations.append({\n",
    "                    'Model': model_name,\n",
    "                    'Default F1': default_f1,\n",
    "                    'Tuned F1': best_value,\n",
    "                    'Improvement': improvement,\n",
    "                    'Action': 'RETRAIN' if improvement > 2.0 else 'Optional'\n",
    "                })\n",
    "        \n",
    "        if recommendations:\n",
    "            rec_df = pd.DataFrame(recommendations)\n",
    "            rec_df = rec_df.sort_values('Improvement', ascending=False)\n",
    "            # Format for display\n",
    "            rec_df['Default F1'] = rec_df['Default F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "            rec_df['Tuned F1'] = rec_df['Tuned F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "            rec_df['Improvement'] = rec_df['Improvement'].apply(lambda x: f\"{x:+.2f}%\")\n",
    "            print(rec_df.to_string(index=False))\n",
    "            \n",
    "            # Highlight high-priority retrains\n",
    "            high_priority = [r for r in recommendations if r['Improvement'] > 2.0]\n",
    "            if high_priority:\n",
    "                print(f\"\\n[!] HIGH PRIORITY: {len(high_priority)} model(s) show >2% improvement:\")\n",
    "                for rec in high_priority:\n",
    "                    print(f\"  - {rec['Model']}: {rec['Improvement']:+.2f}% improvement\")\n",
    "                print(\"\\n  Recommendation: Retrain these models with tuned parameters\")\n",
    "            else:\n",
    "                print(\"\\n[OK] All models performing near-optimally with default parameters\")\n",
    "        else:\n",
    "            print(\"No comparison data available (models not trained with defaults)\")\n",
    "    \n",
    "    # Optimization plots (optional)\n",
    "    if show_optimization_plots:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\" OPTIMIZATION HISTORY\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        print(\"[Info] Optimization plots require Optuna visualization.\")\n",
    "        print(\"  In Colab, install: !pip install optuna plotly\")\n",
    "        print(\"  Then re-run this cell to see optimization history.\")\n",
    "        try:\n",
    "            import optuna\n",
    "            print(\"\\n[OK] Optuna available - plots can be generated\")\n",
    "            print(\"  (Full plot integration coming in next update)\")\n",
    "        except ImportError:\n",
    "            print(\"\\n[X] Optuna not installed - plots unavailable\")\n",
    "    \n",
    "    # Save tuning results\n",
    "    try:\n",
    "        if 'EXPERIMENTS_DIR' in dir():\n",
    "            tuning_results_path = EXPERIMENTS_DIR / 'tuning_results.json'\n",
    "            import json\n",
    "            with open(tuning_results_path, 'w') as f:\n",
    "                json.dump(TUNING_RESULTS, f, indent=2, default=str)\n",
    "            print(f\"\\n[Saved] Tuning results: {tuning_results_path}\")\n",
    "    except Exception as e:\n",
    "        pass  # Silently skip if can't save\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Hyperparameter tuning analysis complete\")\n",
    "    print(f\"{'='*70}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. PHASE 4: ENSEMBLE (Optional)\n",
    "\n",
    "Combine multiple models for improved predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Train Ensemble { display-mode: \"form\" }\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#@markdown ## Ensemble Training Options\n",
    "show_base_model_validation = True  #@param {type: \"boolean\"}\n",
    "filter_by_cv_stability = False  #@param {type: \"boolean\"}\n",
    "show_ensemble_comparison = True  #@param {type: \"boolean\"}\n",
    "min_diversity_threshold = 0.1  #@param {type: \"number\"}\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'USE_CLASS_WEIGHTS' not in dir():\n",
    "    USE_CLASS_WEIGHTS = True\n",
    "if 'USE_SAMPLE_WEIGHTS' not in dir():\n",
    "    USE_SAMPLE_WEIGHTS = True\n",
    "if 'GPU_AVAILABLE' not in dir():\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "# Define clear_memory if not available\n",
    "if 'clear_memory' not in dir():\n",
    "    def clear_memory():\n",
    "        gc.collect()\n",
    "        if GPU_AVAILABLE:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize ensemble results dict\n",
    "ENSEMBLE_RESULTS = {}\n",
    "\n",
    "# Filter out failed models from ensemble base models\n",
    "successful_models = [\n",
    "    model for model, data in TRAINING_RESULTS.items()\n",
    "    if data.get('run_id') != 'failed' and data.get('metrics')\n",
    "]\n",
    "\n",
    "# Check if any ensemble is enabled\n",
    "any_ensemble_enabled = TRAIN_VOTING or TRAIN_STACKING or TRAIN_BLENDING\n",
    "\n",
    "if not any_ensemble_enabled:\n",
    "    print(\"[Skipped] No ensemble training enabled.\")\n",
    "    print(\"Enable TRAIN_VOTING, TRAIN_STACKING, or TRAIN_BLENDING in Section 1.\")\n",
    "elif not TRAINING_RESULTS:\n",
    "    print(\"[WARNING] No trained models found in TRAINING_RESULTS.\")\n",
    "    print(\"Please run Section 4.1 (Model Training) first.\")\n",
    "elif len(successful_models) < 2:\n",
    "    print(\"[Error] Need at least 2 successfully trained models for ensemble.\")\n",
    "    print(f\"Successfully trained: {successful_models}\")\n",
    "    if len(TRAINING_RESULTS) > len(successful_models):\n",
    "        failed = [m for m in TRAINING_RESULTS if m not in successful_models]\n",
    "        print(f\"Failed models (excluded): {failed}\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" PHASE 4: ENSEMBLE TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Helper function to parse base models and validate\n",
    "    def parse_and_validate_base_models(base_models_str, ensemble_name):\n",
    "        \"\"\"Parse comma-separated base models and validate availability.\"\"\"\n",
    "        # Parse base models\n",
    "        base_model_names = [m.strip() for m in base_models_str.split(',') if m.strip()]\n",
    "        \n",
    "        if show_base_model_validation:\n",
    "            print(f\"\\n[{ensemble_name.upper()}] Base Model Validation:\")\n",
    "            print(f\"  Requested: {base_model_names}\")\n",
    "        \n",
    "        # Validate: only use successfully trained models\n",
    "        valid_base_models = [m for m in base_model_names if m in successful_models]\n",
    "        invalid_models = [m for m in base_model_names if m not in successful_models]\n",
    "        \n",
    "        if invalid_models:\n",
    "            print(f\"  [!] Skipped (not trained/failed): {invalid_models}\")\n",
    "        \n",
    "        # Optionally filter by CV stability\n",
    "        if filter_by_cv_stability and 'CV_RESULTS' in dir() and CV_RESULTS:\n",
    "            stable_models = [\n",
    "                m for m in valid_base_models\n",
    "                if m in CV_RESULTS and CV_RESULTS[m].get('stability') in ['Excellent', 'Good']\n",
    "            ]\n",
    "            if len(stable_models) < len(valid_base_models):\n",
    "                unstable = [m for m in valid_base_models if m not in stable_models]\n",
    "                print(f\"  [!] Filtered (low CV stability): {unstable}\")\n",
    "            valid_base_models = stable_models\n",
    "        \n",
    "        if show_base_model_validation:\n",
    "            print(f\"  [OK] Valid base models: {valid_base_models}\")\n",
    "        \n",
    "        return valid_base_models\n",
    "    \n",
    "    # Helper function to parse weights\n",
    "    def parse_weights(weights_str):\n",
    "        \"\"\"Parse comma-separated weights string into list of floats.\"\"\"\n",
    "        if not weights_str or not weights_str.strip():\n",
    "            return None\n",
    "        try:\n",
    "            weights = [float(w.strip()) for w in weights_str.split(',')]\n",
    "            return weights\n",
    "        except ValueError:\n",
    "            print(f\"  [!] Invalid weights format: {weights_str}\")\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        from src.models.trainer import Trainer\n",
    "        from src.models.config import TrainerConfig\n",
    "        \n",
    "        # Load data container\n",
    "        print(f\"\\n[Data Loading]\")\n",
    "        print(f\"  Splits directory: {SPLITS_DIR}\")\n",
    "        print(f\"  Horizon: {TRAINING_HORIZON}\")\n",
    "        \n",
    "        container = TimeSeriesDataContainer.load(SPLITS_DIR, TRAINING_HORIZON)\n",
    "        print(f\"  [OK] Loaded: {container.X_train.shape[0]:,} train samples\")\n",
    "        \n",
    "        # ===================================================================\n",
    "        # TRAIN VOTING ENSEMBLE\n",
    "        # ===================================================================\n",
    "        if TRAIN_VOTING:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" VOTING ENSEMBLE\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            valid_voting_models = parse_and_validate_base_models(VOTING_BASE_MODELS, \"voting\")\n",
    "            \n",
    "            if len(valid_voting_models) < 2:\n",
    "                print(f\"  [X] Need at least 2 valid base models (got {len(valid_voting_models)})\")\n",
    "                print(\"  Skipping Voting ensemble.\")\n",
    "            else:\n",
    "                # Parse weights if provided\n",
    "                weights = parse_weights(VOTING_WEIGHTS) if VOTING_WEIGHTS else None\n",
    "                if weights and len(weights) != len(valid_voting_models):\n",
    "                    print(f\"  [!] Weights count ({len(weights)}) != models count ({len(valid_voting_models)})\")\n",
    "                    print(\"  Using equal weights instead.\")\n",
    "                    weights = None\n",
    "                \n",
    "                # Create config\n",
    "                voting_config = TrainerConfig(\n",
    "                    model_name='voting',\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    model_config={\n",
    "                        'base_model_names': valid_voting_models,\n",
    "                        'voting_type': VOTING_TYPE,\n",
    "                        'weights': weights,\n",
    "                    },\n",
    "                    use_sample_weights=USE_SAMPLE_WEIGHTS,\n",
    "                    use_class_weights=USE_CLASS_WEIGHTS,\n",
    "                )\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(voting_config)\n",
    "                print(f\"\\n  Training voting ensemble with {len(valid_voting_models)} base models...\")\n",
    "                results = trainer.train(container)\n",
    "                \n",
    "                if results.get('status') == 'success':\n",
    "                    ENSEMBLE_RESULTS['voting'] = results\n",
    "                    metrics = results.get('metrics', {})\n",
    "                    print(f\"  [OK] Voting ensemble trained successfully!\")\n",
    "                    print(f\"    Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "                    print(f\"    Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n",
    "                else:\n",
    "                    print(f\"  [X] Voting ensemble failed: {results.get('error', 'Unknown error')}\")\n",
    "                \n",
    "                clear_memory()\n",
    "        \n",
    "        # ===================================================================\n",
    "        # TRAIN STACKING ENSEMBLE\n",
    "        # ===================================================================\n",
    "        if TRAIN_STACKING:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" STACKING ENSEMBLE\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            valid_stacking_models = parse_and_validate_base_models(STACKING_BASE_MODELS, \"stacking\")\n",
    "            \n",
    "            if len(valid_stacking_models) < 2:\n",
    "                print(f\"  [X] Need at least 2 valid base models (got {len(valid_stacking_models)})\")\n",
    "                print(\"  Skipping Stacking ensemble.\")\n",
    "            else:\n",
    "                # Create config\n",
    "                stacking_config = TrainerConfig(\n",
    "                    model_name='stacking',\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    model_config={\n",
    "                        'base_model_names': valid_stacking_models,\n",
    "                        'meta_learner': STACKING_META_LEARNER,\n",
    "                        'use_proba': STACKING_USE_PROBA,\n",
    "                    },\n",
    "                    use_sample_weights=USE_SAMPLE_WEIGHTS,\n",
    "                    use_class_weights=USE_CLASS_WEIGHTS,\n",
    "                )\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(stacking_config)\n",
    "                print(f\"\\n  Training stacking ensemble with {len(valid_stacking_models)} base models...\")\n",
    "                print(f\"    Meta-learner: {STACKING_META_LEARNER}\")\n",
    "                results = trainer.train(container)\n",
    "                \n",
    "                if results.get('status') == 'success':\n",
    "                    ENSEMBLE_RESULTS['stacking'] = results\n",
    "                    metrics = results.get('metrics', {})\n",
    "                    print(f\"  [OK] Stacking ensemble trained successfully!\")\n",
    "                    print(f\"    Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "                    print(f\"    Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n",
    "                else:\n",
    "                    print(f\"  [X] Stacking ensemble failed: {results.get('error', 'Unknown error')}\")\n",
    "                \n",
    "                clear_memory()\n",
    "        \n",
    "        # ===================================================================\n",
    "        # TRAIN BLENDING ENSEMBLE\n",
    "        # ===================================================================\n",
    "        if TRAIN_BLENDING:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" BLENDING ENSEMBLE\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            valid_blending_models = parse_and_validate_base_models(BLENDING_BASE_MODELS, \"blending\")\n",
    "            \n",
    "            if len(valid_blending_models) < 2:\n",
    "                print(f\"  [X] Need at least 2 valid base models (got {len(valid_blending_models)})\")\n",
    "                print(\"  Skipping Blending ensemble.\")\n",
    "            else:\n",
    "                # Create config\n",
    "                blending_config = TrainerConfig(\n",
    "                    model_name='blending',\n",
    "                    horizon=TRAINING_HORIZON,\n",
    "                    model_config={\n",
    "                        'base_model_names': valid_blending_models,\n",
    "                        'meta_learner': BLENDING_META_LEARNER,\n",
    "                        'holdout_ratio': BLENDING_HOLDOUT_RATIO,\n",
    "                    },\n",
    "                    use_sample_weights=USE_SAMPLE_WEIGHTS,\n",
    "                    use_class_weights=USE_CLASS_WEIGHTS,\n",
    "                )\n",
    "                \n",
    "                # Train\n",
    "                trainer = Trainer(blending_config)\n",
    "                print(f\"\\n  Training blending ensemble with {len(valid_blending_models)} base models...\")\n",
    "                print(f\"    Meta-learner: {BLENDING_META_LEARNER}\")\n",
    "                print(f\"    Holdout ratio: {BLENDING_HOLDOUT_RATIO}\")\n",
    "                results = trainer.train(container)\n",
    "                \n",
    "                if results.get('status') == 'success':\n",
    "                    ENSEMBLE_RESULTS['blending'] = results\n",
    "                    metrics = results.get('metrics', {})\n",
    "                    print(f\"  [OK] Blending ensemble trained successfully!\")\n",
    "                    print(f\"    Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "                    print(f\"    Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n",
    "                else:\n",
    "                    print(f\"  [X] Blending ensemble failed: {results.get('error', 'Unknown error')}\")\n",
    "                \n",
    "                clear_memory()\n",
    "        \n",
    "        # ===================================================================\n",
    "        # ENSEMBLE COMPARISON\n",
    "        # ===================================================================\n",
    "        if show_ensemble_comparison and ENSEMBLE_RESULTS:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" ENSEMBLE COMPARISON\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            comparison_data = []\n",
    "            for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "                metrics = results.get('metrics', {})\n",
    "                comparison_data.append({\n",
    "                    'Ensemble': ensemble_name,\n",
    "                    'Accuracy': metrics.get('accuracy', 0),\n",
    "                    'Macro F1': metrics.get('macro_f1', 0),\n",
    "                    'Weighted F1': metrics.get('weighted_f1', 0),\n",
    "                })\n",
    "            \n",
    "            # Add best base model for comparison\n",
    "            if TRAINING_RESULTS:\n",
    "                best_base_model = max(\n",
    "                    {m: TRAINING_RESULTS[m]['metrics']['macro_f1']\n",
    "                     for m in successful_models}.items(),\n",
    "                    key=lambda x: x[1]\n",
    "                )\n",
    "                best_base_acc = TRAINING_RESULTS[best_base_model[0]]['metrics']['accuracy']\n",
    "                comparison_data.append({\n",
    "                    'Ensemble': f\"(best base: {best_base_model[0]})\",\n",
    "                    'Accuracy': best_base_acc,\n",
    "                    'Macro F1': best_base_model[1],\n",
    "                    'Weighted F1': TRAINING_RESULTS[best_base_model[0]]['metrics'].get('weighted_f1', 0),\n",
    "                })\n",
    "            \n",
    "            comp_df = pd.DataFrame(comparison_data)\n",
    "            comp_df = comp_df.sort_values('Macro F1', ascending=False)\n",
    "            print(\"\\n\" + comp_df.to_string(index=False))\n",
    "        \n",
    "        # ===================================================================\n",
    "        # SUMMARY\n",
    "        # ===================================================================\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" ENSEMBLE TRAINING COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\n  Ensembles trained: {len(ENSEMBLE_RESULTS)}\")\n",
    "        if ENSEMBLE_RESULTS:\n",
    "            print(f\"  Available: {list(ENSEMBLE_RESULTS.keys())}\")\n",
    "        print(\"\\n  [OK] Results stored in ENSEMBLE_RESULTS dict\")\n",
    "        \n",
    "        del container\n",
    "        clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Ensemble training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        ENSEMBLE_RESULTS = {}\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.2 Ensemble Analysis & Diversity { display-mode: \"form\" }\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#@markdown ## Analysis Options\n",
    "show_diversity_metrics = True  #@param {type: \"boolean\"}\n",
    "show_base_contributions = True  #@param {type: \"boolean\"}\n",
    "show_disagreement_analysis = False  #@param {type: \"boolean\"}\n",
    "plot_contribution_charts = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Ensure environment variables\n",
    "if 'ENSEMBLE_RESULTS' not in dir():\n",
    "    ENSEMBLE_RESULTS = {}\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if os.path.exists('/content') else Path('.')\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "\n",
    "# Check if ensemble results are populated\n",
    "if not ENSEMBLE_RESULTS:\n",
    "    print(\"[Skipped] No ensemble models trained.\")\n",
    "    print(\"Enable TRAIN_VOTING, TRAIN_STACKING, or TRAIN_BLENDING in Section 1\")\n",
    "    print(\"and run Cell 6.1 to train ensembles.\")\n",
    "elif not TRAINING_RESULTS:\n",
    "    print(\"[WARNING] No base model training results found in TRAINING_RESULTS.\")\n",
    "    print(\"Please run Section 4.1 (Model Training) first.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" ENSEMBLE ANALYSIS & DIVERSITY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "        \n",
    "        # Load data for predictions\n",
    "        container = TimeSeriesDataContainer.load(SPLITS_DIR, TRAINING_HORIZON)\n",
    "        \n",
    "        # ===================================================================\n",
    "        # DIVERSITY ANALYSIS\n",
    "        # ===================================================================\n",
    "        if show_diversity_metrics:\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(\" DIVERSITY METRICS\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "                print(f\"\\n[{ensemble_name.upper()}]\")\n",
    "                base_model_names = results['config']['base_model_names']\n",
    "                print(f\"  Base models: {base_model_names}\")\n",
    "                \n",
    "                # Get predictions from each base model on validation set\n",
    "                base_predictions = {}\n",
    "                for model_name in base_model_names:\n",
    "                    if model_name in TRAINING_RESULTS:\n",
    "                        if 'val_predictions' in TRAINING_RESULTS[model_name]:\n",
    "                            base_predictions[model_name] = TRAINING_RESULTS[model_name]['val_predictions']\n",
    "                \n",
    "                if len(base_predictions) >= 2:\n",
    "                    # Calculate pairwise disagreement\n",
    "                    preds_list = list(base_predictions.values())\n",
    "                    model_names = list(base_predictions.keys())\n",
    "                    n_models = len(preds_list)\n",
    "                    \n",
    "                    disagreement_matrix = np.zeros((n_models, n_models))\n",
    "                    for i in range(n_models):\n",
    "                        for j in range(i + 1, n_models):\n",
    "                            disagreement = np.mean(np.array(preds_list[i]) != np.array(preds_list[j]))\n",
    "                            disagreement_matrix[i, j] = disagreement\n",
    "                            disagreement_matrix[j, i] = disagreement\n",
    "                    \n",
    "                    avg_disagreement = np.mean(disagreement_matrix[np.triu_indices(n_models, k=1)])\n",
    "                    print(f\"  Average pairwise disagreement: {avg_disagreement:.4f}\")\n",
    "                    \n",
    "                    # Q-statistic (lower is better diversity)\n",
    "                    # Simplified calculation\n",
    "                    q_stats = []\n",
    "                    for i in range(n_models):\n",
    "                        for j in range(i + 1, n_models):\n",
    "                            pi = np.array(preds_list[i])\n",
    "                            pj = np.array(preds_list[j])\n",
    "                            # Count agreements/disagreements\n",
    "                            a = np.sum((pi == 1) & (pj == 1))  # both correct proxy\n",
    "                            b = np.sum((pi == 1) & (pj != 1))\n",
    "                            c = np.sum((pi != 1) & (pj == 1))\n",
    "                            d = np.sum((pi != 1) & (pj != 1))\n",
    "                            if (a * d + b * c) > 0:\n",
    "                                q = (a * d - b * c) / (a * d + b * c)\n",
    "                                q_stats.append(q)\n",
    "                    \n",
    "                    if q_stats:\n",
    "                        avg_q = np.mean(q_stats)\n",
    "                        print(f\"  Average Q-statistic: {avg_q:.4f} (lower = more diverse)\")\n",
    "                else:\n",
    "                    print(\"  [!] Not enough base model predictions for diversity analysis\")\n",
    "        \n",
    "        # ===================================================================\n",
    "        # BASE MODEL CONTRIBUTIONS\n",
    "        # ===================================================================\n",
    "        if show_base_contributions:\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(\" BASE MODEL CONTRIBUTIONS\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "                print(f\"\\n[{ensemble_name.upper()}]\")\n",
    "                \n",
    "                # Show base model F1 scores\n",
    "                base_model_names = results['config']['base_model_names']\n",
    "                for model in base_model_names:\n",
    "                    if model in TRAINING_RESULTS:\n",
    "                        f1 = TRAINING_RESULTS[model]['metrics']['macro_f1']\n",
    "                        print(f\"  {model}: F1 = {f1:.4f}\")\n",
    "                \n",
    "                # Show ensemble improvement\n",
    "                ensemble_f1 = results['metrics']['macro_f1']\n",
    "                best_base_f1 = max(\n",
    "                    TRAINING_RESULTS[m]['metrics']['macro_f1']\n",
    "                    for m in base_model_names if m in TRAINING_RESULTS\n",
    "                )\n",
    "                improvement = (ensemble_f1 - best_base_f1) / best_base_f1 * 100 if best_base_f1 > 0 else 0\n",
    "                print(f\"  ----\")\n",
    "                print(f\"  Ensemble: F1 = {ensemble_f1:.4f} ({improvement:+.2f}% vs best base)\")\n",
    "        \n",
    "        # ===================================================================\n",
    "        # CONTRIBUTION CHARTS\n",
    "        # ===================================================================\n",
    "        if plot_contribution_charts and ENSEMBLE_RESULTS:\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(\" CONTRIBUTION VISUALIZATION\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, len(ENSEMBLE_RESULTS), figsize=(5 * len(ENSEMBLE_RESULTS), 4))\n",
    "            if len(ENSEMBLE_RESULTS) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for idx, (ensemble_name, results) in enumerate(ENSEMBLE_RESULTS.items()):\n",
    "                base_model_names = results['config']['base_model_names']\n",
    "                base_f1s = []\n",
    "                model_labels = []\n",
    "                \n",
    "                for model_name in base_model_names:\n",
    "                    if model_name in TRAINING_RESULTS:\n",
    "                        base_f1s.append(TRAINING_RESULTS[model_name]['metrics']['macro_f1'])\n",
    "                        model_labels.append(model_name)\n",
    "                \n",
    "                # Add ensemble\n",
    "                base_f1s.append(results['metrics']['macro_f1'])\n",
    "                model_labels.append(f\"{ensemble_name}\\n(ensemble)\")\n",
    "                \n",
    "                # Plot\n",
    "                colors = ['steelblue'] * (len(base_f1s) - 1) + ['coral']\n",
    "                axes[idx].bar(range(len(base_f1s)), base_f1s, color=colors)\n",
    "                axes[idx].set_xticks(range(len(base_f1s)))\n",
    "                axes[idx].set_xticklabels(model_labels, rotation=45, ha='right')\n",
    "                axes[idx].set_ylabel('Macro F1')\n",
    "                axes[idx].set_title(f'{ensemble_name.upper()}')\n",
    "                axes[idx].set_ylim(0, 1)\n",
    "                axes[idx].grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # ===================================================================\n",
    "        # DISAGREEMENT ANALYSIS\n",
    "        # ===================================================================\n",
    "        if show_disagreement_analysis and ENSEMBLE_RESULTS:\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(\" DISAGREEMENT ANALYSIS\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for ensemble_name, results in ENSEMBLE_RESULTS.items():\n",
    "                print(f\"\\n[{ensemble_name.upper()}]\")\n",
    "                base_model_names = results['config']['base_model_names']\n",
    "                \n",
    "                # Get predictions\n",
    "                base_predictions = {}\n",
    "                for model_name in base_model_names:\n",
    "                    if model_name in TRAINING_RESULTS and 'val_predictions' in TRAINING_RESULTS[model_name]:\n",
    "                        base_predictions[model_name] = TRAINING_RESULTS[model_name]['val_predictions']\n",
    "                \n",
    "                if len(base_predictions) >= 2:\n",
    "                    preds_array = np.array(list(base_predictions.values()))\n",
    "                    n_samples = preds_array.shape[1]\n",
    "                    \n",
    "                    # Count unanimous vs split predictions\n",
    "                    unanimous = np.sum(np.all(preds_array == preds_array[0], axis=0))\n",
    "                    split = n_samples - unanimous\n",
    "                    \n",
    "                    print(f\"  Total samples: {n_samples}\")\n",
    "                    print(f\"  Unanimous predictions: {unanimous} ({unanimous/n_samples*100:.1f}%)\")\n",
    "                    print(f\"  Split predictions: {split} ({split/n_samples*100:.1f}%)\")\n",
    "                else:\n",
    "                    print(\"  [!] Not enough predictions for disagreement analysis\")\n",
    "        \n",
    "        # ===================================================================\n",
    "        # BEST ENSEMBLE RECOMMENDATION\n",
    "        # ===================================================================\n",
    "        if ENSEMBLE_RESULTS:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\" RECOMMENDATION\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            best_ensemble_name = max(\n",
    "                ENSEMBLE_RESULTS,\n",
    "                key=lambda x: ENSEMBLE_RESULTS[x]['metrics']['macro_f1']\n",
    "            )\n",
    "            best_ensemble = ENSEMBLE_RESULTS[best_ensemble_name]\n",
    "            \n",
    "            print(f\"\\n  Best ensemble: {best_ensemble_name.upper()}\")\n",
    "            print(f\"  Macro F1: {best_ensemble['metrics']['macro_f1']:.4f}\")\n",
    "            print(f\"  Accuracy: {best_ensemble['metrics']['accuracy']:.4f}\")\n",
    "            print(f\"\\n  Reason: Highest F1 score among {len(ENSEMBLE_RESULTS)} ensembles\")\n",
    "        \n",
    "        del container\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Ensemble analysis failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. RESULTS & EXPORT\n",
    "\n",
    "Summary of all results and export options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.1 Final Summary { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'SPLITS_DIR' not in dir():\n",
    "    SPLITS_DIR = PROJECT_ROOT / 'data/splits/scaled'\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" PIPELINE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n Configuration:\")\n",
    "print(f\"   Symbol: {SYMBOL}\")\n",
    "\n",
    "# Show auto-detected date range (with safety checks)\n",
    "if 'DATA_START' in dir() and DATA_START is not None:\n",
    "    print(f\"   Date Range: {DATA_START.strftime('%Y-%m-%d')} to {DATA_END.strftime('%Y-%m-%d')}\")\n",
    "    if 'DATA_START_YEAR' in dir() and 'DATA_END_YEAR' in dir():\n",
    "        print(f\"   Years: {DATA_START_YEAR} - {DATA_END_YEAR}\")\n",
    "else:\n",
    "    print(f\"   Date Range: Not detected (run Section 3.1)\")\n",
    "\n",
    "print(f\"   Training Horizon: H{TRAINING_HORIZON}\")\n",
    "\n",
    "if 'TRAIN_LEN' in dir():\n",
    "    print(f\"\\n Data:\")\n",
    "    print(f\"   Train: {TRAIN_LEN:,} samples\")\n",
    "    if 'VAL_LEN' in dir():\n",
    "        print(f\"   Val: {VAL_LEN:,} samples\")\n",
    "    if 'TEST_LEN' in dir():\n",
    "        print(f\"   Test: {TEST_LEN:,} samples\")\n",
    "\n",
    "if 'TRAINING_RESULTS' in dir() and TRAINING_RESULTS:\n",
    "    print(f\"\\n Model Results:\")\n",
    "    for model, data in sorted(TRAINING_RESULTS.items(), \n",
    "                              key=lambda x: x[1]['metrics'].get('macro_f1', 0), \n",
    "                              reverse=True):\n",
    "        metrics = data['metrics']\n",
    "        print(f\"   {model}: Acc={metrics.get('accuracy', 0):.2%}, F1={metrics.get('macro_f1', 0):.4f}\")\n",
    "    \n",
    "    best = max(TRAINING_RESULTS, key=lambda x: TRAINING_RESULTS[x]['metrics'].get('macro_f1', 0))\n",
    "    print(f\"\\n Best Model: {best}\")\n",
    "\n",
    "print(f\"\\n Saved Artifacts:\")\n",
    "print(f\"   Data: {SPLITS_DIR}\")\n",
    "print(f\"   Models: {EXPERIMENTS_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" PIPELINE COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Export Model Package { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure environment variables are defined\n",
    "if 'IS_COLAB' not in dir():\n",
    "    IS_COLAB = os.path.exists('/content')\n",
    "if 'PROJECT_ROOT' not in dir():\n",
    "    PROJECT_ROOT = Path('/content/research') if IS_COLAB else Path.cwd()\n",
    "if 'EXPERIMENTS_DIR' not in dir():\n",
    "    EXPERIMENTS_DIR = PROJECT_ROOT / 'experiments/runs'\n",
    "if 'RESULTS_DIR' not in dir():\n",
    "    RESULTS_DIR = PROJECT_ROOT / 'experiments'\n",
    "if 'TRAINING_RESULTS' not in dir():\n",
    "    TRAINING_RESULTS = {}\n",
    "if 'TEST_RESULTS' not in dir():\n",
    "    TEST_RESULTS = {}\n",
    "if 'CV_RESULTS' not in dir():\n",
    "    CV_RESULTS = {}\n",
    "if 'ENSEMBLE_RESULTS' not in dir():\n",
    "    ENSEMBLE_RESULTS = {}\n",
    "\n",
    "# Early check for any results - will provide detailed warning during export\n",
    "_has_any_results = TRAINING_RESULTS or ENSEMBLE_RESULTS\n",
    "\n",
    "#@markdown ### Export Configuration\n",
    "\n",
    "export_model = False  #@param {type: \"boolean\"}\n",
    "#@markdown Enable to export model package\n",
    "\n",
    "export_selection = \"Best Model\"  #@param [\"Best Model\", \"All Models\", \"Ensembles Only\", \"Top 3 Models\", \"Custom Selection\"]\n",
    "#@markdown Select which models to export\n",
    "\n",
    "custom_models_to_export = \"\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated model names (only used if Custom Selection)\n",
    "\n",
    "export_format = \"Standard Package\"  #@param [\"Standard Package\", \"Production Bundle\", \"Research Archive\", \"Minimal (Model Only)\"]\n",
    "#@markdown Export package type\n",
    "\n",
    "#@markdown ### Export Options\n",
    "\n",
    "include_onnx = False  #@param {type: \"boolean\"}\n",
    "#@markdown Export to ONNX format for production (XGBoost, LightGBM, CatBoost only)\n",
    "\n",
    "include_predictions = True  #@param {type: \"boolean\"}\n",
    "#@markdown Include validation and test predictions\n",
    "\n",
    "include_visualizations = True  #@param {type: \"boolean\"}\n",
    "#@markdown Include generated plots and charts\n",
    "\n",
    "include_model_card = True  #@param {type: \"boolean\"}\n",
    "#@markdown Generate model cards with performance details\n",
    "\n",
    "create_zip_archive = True  #@param {type: \"boolean\"}\n",
    "#@markdown Create ZIP archive of export package\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_models_to_export():\n",
    "    \"\"\"Determine which models to export based on selection.\"\"\"\n",
    "    all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "\n",
    "    if not all_results:\n",
    "        return []\n",
    "\n",
    "    if export_selection == \"Best Model\":\n",
    "        best_model = max(all_results, key=lambda x: all_results[x]['metrics'].get('macro_f1', 0))\n",
    "        return [best_model]\n",
    "\n",
    "    elif export_selection == \"All Models\":\n",
    "        return list(all_results.keys())\n",
    "\n",
    "    elif export_selection == \"Ensembles Only\":\n",
    "        return [m for m in all_results.keys() if 'ensemble' in m or 'voting' in m or 'stacking' in m or 'blending' in m]\n",
    "\n",
    "    elif export_selection == \"Top 3 Models\":\n",
    "        sorted_models = sorted(all_results.items(), key=lambda x: x[1]['metrics'].get('macro_f1', 0), reverse=True)\n",
    "        return [m[0] for m in sorted_models[:3]]\n",
    "\n",
    "    elif export_selection == \"Custom Selection\":\n",
    "        if not custom_models_to_export:\n",
    "            print(\"\u26a0 Custom selection requires model names in 'custom_models_to_export'\")\n",
    "            return []\n",
    "        models = [m.strip() for m in custom_models_to_export.split(',')]\n",
    "        valid_models = [m for m in models if m in all_results]\n",
    "        if len(valid_models) < len(models):\n",
    "            invalid = set(models) - set(valid_models)\n",
    "            print(f\"\u26a0 Invalid models: {invalid}\")\n",
    "        return valid_models\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_model_card(model_name, model_info, test_info=None, cv_info=None):\n",
    "    \"\"\"Generate model card in Markdown format.\"\"\"\n",
    "    metrics = model_info.get('metrics', {})\n",
    "    config = model_info.get('config', {})\n",
    "\n",
    "    card = f\"\"\"# Model Card: {model_name.upper()}\n",
    "\n",
    "## Model Information\n",
    "- **Type:** {model_info.get('model_type', 'Unknown')}\n",
    "- **Symbol:** {SYMBOL if 'SYMBOL' in dir() else 'N/A'}\n",
    "- **Horizon:** {TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A'} bars\n",
    "- **Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Run ID:** {model_info.get('run_id', 'Unknown')}\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Validation Set\n",
    "- **Accuracy:** {metrics.get('accuracy', 0):.4f}\n",
    "- **Macro F1:** {metrics.get('macro_f1', 0):.4f}\n",
    "- **Precision:** {metrics.get('precision', 0):.4f}\n",
    "- **Recall:** {metrics.get('recall', 0):.4f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add test results if available\n",
    "    if test_info:\n",
    "        test_metrics = test_info.get('metrics', {})\n",
    "        val_f1 = metrics.get('macro_f1', 0)\n",
    "        test_f1 = test_metrics.get('macro_f1', 0)\n",
    "        gap = ((test_f1 - val_f1) / val_f1 * 100) if val_f1 > 0 else 0\n",
    "\n",
    "        card += f\"\"\"### Test Set\n",
    "- **Accuracy:** {test_metrics.get('accuracy', 0):.4f}\n",
    "- **Macro F1:** {test_metrics.get('macro_f1', 0):.4f}\n",
    "- **Precision:** {test_metrics.get('precision', 0):.4f}\n",
    "- **Recall:** {test_metrics.get('recall', 0):.4f}\n",
    "- **Generalization Gap:** {gap:+.2f}%\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add CV results if available\n",
    "    if cv_info:\n",
    "        cv_metrics = cv_info.get('cv_metrics', {})\n",
    "        card += f\"\"\"### Cross-Validation\n",
    "- **Mean F1:** {cv_metrics.get('mean_f1', 0):.4f} \u00b1 {cv_metrics.get('std_f1', 0):.4f}\n",
    "- **Mean Accuracy:** {cv_metrics.get('mean_accuracy', 0):.4f} \u00b1 {cv_metrics.get('std_accuracy', 0):.4f}\n",
    "- **Folds:** {cv_info.get('n_splits', 'N/A')}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add configuration\n",
    "    if config:\n",
    "        card += f\"\"\"## Configuration\n",
    "\n",
    "```json\n",
    "{json.dumps(config, indent=2)}\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add feature information\n",
    "    if 'feature_importance' in model_info:\n",
    "        importance = model_info['feature_importance']\n",
    "        top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        card += f\"\"\"## Top 10 Features\n",
    "\n",
    "\"\"\"\n",
    "        for i, (feature, score) in enumerate(top_features, 1):\n",
    "            card += f\"{i}. **{feature}**: {score:.4f}\\n\"\n",
    "        card += \"\\n\"\n",
    "\n",
    "    # Add training details\n",
    "    train_time = model_info.get('training_time_sec', 0)\n",
    "    card += f\"\"\"## Training Details\n",
    "- **Training Time:** {train_time:.2f}s\n",
    "- **Model Size:** {model_info.get('model_size_mb', 'N/A')} MB\n",
    "- **Framework:** {model_info.get('framework', 'Unknown')}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    return card\n",
    "\n",
    "\n",
    "def export_to_onnx(model, model_name, model_path, feature_names):\n",
    "    \"\"\"Export model to ONNX format (boosting models only).\"\"\"\n",
    "    try:\n",
    "        # Check if model type supports ONNX\n",
    "        onnx_compatible = ['xgboost', 'lightgbm', 'catboost']\n",
    "        if not any(m in model_name.lower() for m in onnx_compatible):\n",
    "            return False, \"Model type not compatible with ONNX\"\n",
    "\n",
    "        # Try to import ONNX libraries\n",
    "        try:\n",
    "            from skl2onnx import convert_sklearn\n",
    "            from skl2onnx.common.data_types import FloatTensorType\n",
    "            import onnx\n",
    "        except ImportError:\n",
    "            return False, \"ONNX libraries not installed (skl2onnx, onnx)\"\n",
    "\n",
    "        # Load the model\n",
    "        loaded_model = joblib.load(model_path)\n",
    "\n",
    "        # Determine number of features\n",
    "        n_features = len(feature_names) if feature_names else 150\n",
    "\n",
    "        # Define input type\n",
    "        initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "\n",
    "        # Convert to ONNX\n",
    "        onnx_model = convert_sklearn(loaded_model, initial_types=initial_type)\n",
    "\n",
    "        # Save ONNX model\n",
    "        onnx_path = model_path.parent / 'model.onnx'\n",
    "        with open(onnx_path, 'wb') as f:\n",
    "            f.write(onnx_model.SerializeToString())\n",
    "\n",
    "        # Get file size\n",
    "        size_mb = onnx_path.stat().st_size / 1e6\n",
    "\n",
    "        return True, f\"ONNX export successful ({size_mb:.2f} MB)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"ONNX export failed: {str(e)}\"\n",
    "\n",
    "\n",
    "def create_manifest(export_dir, models_exported, export_info):\n",
    "    \"\"\"Create manifest.json with export metadata.\"\"\"\n",
    "    all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "\n",
    "    # Find best model\n",
    "    best_model = max(all_results, key=lambda x: all_results[x]['metrics'].get('macro_f1', 0))\n",
    "    best_f1 = all_results[best_model]['metrics'].get('macro_f1', 0)\n",
    "\n",
    "    # Collect model formats\n",
    "    formats = {}\n",
    "    for model_name in models_exported:\n",
    "        model_formats = ['pkl']\n",
    "        onnx_path = export_dir / 'models' / model_name / 'model.onnx'\n",
    "        if onnx_path.exists():\n",
    "            model_formats.append('onnx')\n",
    "        formats[model_name] = model_formats\n",
    "\n",
    "    manifest = {\n",
    "        'export_timestamp': datetime.now().isoformat(),\n",
    "        'symbol': SYMBOL if 'SYMBOL' in dir() else 'N/A',\n",
    "        'horizon': TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A',\n",
    "        'models_exported': models_exported,\n",
    "        'best_model': best_model,\n",
    "        'best_test_f1': best_f1,\n",
    "        'export_format': export_format,\n",
    "        'formats': formats,\n",
    "        'data_stats': export_info.get('data_stats', {}),\n",
    "        'export_options': {\n",
    "            'include_onnx': include_onnx,\n",
    "            'include_predictions': include_predictions,\n",
    "            'include_visualizations': include_visualizations,\n",
    "            'include_model_card': include_model_card\n",
    "        }\n",
    "    }\n",
    "\n",
    "    manifest_path = export_dir / 'manifest.json'\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    return manifest_path\n",
    "\n",
    "\n",
    "def create_readme(export_dir, models_exported):\n",
    "    \"\"\"Create README.md with setup and usage instructions.\"\"\"\n",
    "    readme = f\"\"\"# ML Model Export Package\n",
    "\n",
    "**Export Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Symbol:** {SYMBOL if 'SYMBOL' in dir() else 'N/A'}\n",
    "**Horizon:** {TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'N/A'} bars\n",
    "\n",
    "## Package Contents\n",
    "\n",
    "This export package contains:\n",
    "\n",
    "- **Models:** {len(models_exported)} trained model(s)\n",
    "- **Predictions:** Validation and test set predictions\n",
    "- **Metrics:** Training, validation, and test performance metrics\n",
    "- **Visualizations:** Confusion matrices, feature importance, learning curves\n",
    "- **Model Cards:** Detailed model documentation and performance analysis\n",
    "- **Data Info:** Feature names, label mappings, data statistics\n",
    "\n",
    "## Models Included\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    for model_name in models_exported:\n",
    "        readme += f\"- `{model_name}`\\n\"\n",
    "\n",
    "    readme += \"\"\"\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "```\n",
    "\u251c\u2500\u2500 models/              # Trained models (PKL, ONNX)\n",
    "\u251c\u2500\u2500 predictions/         # Model predictions (CSV)\n",
    "\u251c\u2500\u2500 metrics/             # Performance metrics (JSON)\n",
    "\u251c\u2500\u2500 visualizations/      # Plots and charts (PNG)\n",
    "\u251c\u2500\u2500 model_cards/         # Model documentation (MD)\n",
    "\u251c\u2500\u2500 data/                # Feature info and stats\n",
    "\u251c\u2500\u2500 manifest.json        # Export metadata\n",
    "\u2514\u2500\u2500 README.md            # This file\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### Load a Model\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('models/xgboost/model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### Load ONNX Model (Production)\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Create inference session\n",
    "session = ort.InferenceSession('models/xgboost/model.onnx')\n",
    "\n",
    "# Run inference\n",
    "input_name = session.get_inputs()[0].name\n",
    "predictions = session.run(None, {input_name: X_test.astype('float32')})[0]\n",
    "```\n",
    "\n",
    "### Load Predictions\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load test predictions\n",
    "test_preds = pd.read_csv('predictions/test_predictions.csv')\n",
    "print(test_preds.head())\n",
    "```\n",
    "\n",
    "## Model Cards\n",
    "\n",
    "Each model has a detailed model card in `model_cards/` with:\n",
    "- Performance metrics (validation, test, CV)\n",
    "- Configuration parameters\n",
    "- Feature importance\n",
    "- Training details\n",
    "- Usage examples\n",
    "\n",
    "## Performance Summary\n",
    "\n",
    "See `metrics/test_metrics.json` for detailed performance metrics across all models.\n",
    "\n",
    "## Support\n",
    "\n",
    "For questions or issues:\n",
    "1. Review model cards for specific model details\n",
    "2. Check manifest.json for export metadata\n",
    "3. Consult feature documentation in data/\n",
    "\n",
    "---\n",
    "\n",
    "Generated by ML Model Factory\n",
    "\"\"\"\n",
    "\n",
    "    readme_path = export_dir / 'README.md'\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    return readme_path\n",
    "\n",
    "\n",
    "def export_predictions(model_name, model_info, export_dir):\n",
    "    \"\"\"Export validation and test predictions to CSV.\"\"\"\n",
    "    pred_dir = export_dir / 'predictions' / model_name\n",
    "    pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Export validation predictions if available\n",
    "    if 'val_predictions' in model_info:\n",
    "        val_preds = model_info['val_predictions']\n",
    "        val_df = pd.DataFrame({\n",
    "            'index': range(len(val_preds['actual'])),\n",
    "            'actual': val_preds['actual'],\n",
    "            'predicted': val_preds['predicted']\n",
    "        })\n",
    "        if 'confidence' in val_preds:\n",
    "            val_df['confidence'] = val_preds['confidence']\n",
    "        val_df['correct'] = val_df['actual'] == val_df['predicted']\n",
    "\n",
    "        val_path = pred_dir / 'val_predictions.csv'\n",
    "        val_df.to_csv(val_path, index=False)\n",
    "\n",
    "    # Export test predictions if available\n",
    "    test_info = TEST_RESULTS.get(model_name, {})\n",
    "    if 'predictions' in test_info:\n",
    "        test_preds = test_info['predictions']\n",
    "        test_df = pd.DataFrame({\n",
    "            'index': range(len(test_preds['actual'])),\n",
    "            'actual': test_preds['actual'],\n",
    "            'predicted': test_preds['predicted']\n",
    "        })\n",
    "        if 'confidence' in test_preds:\n",
    "            test_df['confidence'] = test_preds['confidence']\n",
    "        test_df['correct'] = test_df['actual'] == test_df['predicted']\n",
    "\n",
    "        test_path = pred_dir / 'test_predictions.csv'\n",
    "        test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    # Create predictions summary\n",
    "    summary = {\n",
    "        'model_name': model_name,\n",
    "        'val_samples': len(val_preds['actual']) if 'val_predictions' in model_info else 0,\n",
    "        'test_samples': len(test_preds['actual']) if 'predictions' in test_info else 0,\n",
    "        'val_accuracy': (val_df['correct'].sum() / len(val_df)) if 'val_predictions' in model_info else None,\n",
    "        'test_accuracy': (test_df['correct'].sum() / len(test_df)) if 'predictions' in test_info else None\n",
    "    }\n",
    "\n",
    "    summary_path = pred_dir / 'predictions_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPORT LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "if export_model:\n",
    "    # Check if any results are available\n",
    "    if not TRAINING_RESULTS and not ENSEMBLE_RESULTS:\n",
    "        print(\"[WARNING] No trained models found.\")\n",
    "        print(\"Please run Section 4.1 (Model Training) or Section 6.1 (Ensemble Training) first.\")\n",
    "    else:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"MODEL EXPORT PACKAGE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Get models to export\n",
    "    models_to_export = get_models_to_export()\n",
    "\n",
    "    if not models_to_export:\n",
    "        print(\"\\n\u26a0 No models to export. Check your selection criteria.\")\n",
    "    else:\n",
    "        print(f\"\\n\ud83d\udce6 Exporting {len(models_to_export)} model(s): {', '.join(models_to_export)}\")\n",
    "\n",
    "        # Create export directory\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        symbol = SYMBOL if 'SYMBOL' in dir() else 'UNKNOWN'\n",
    "        horizon = TRAINING_HORIZON if 'TRAINING_HORIZON' in dir() else 'XX'\n",
    "        export_name = f\"{timestamp}_{symbol}_H{horizon}\"\n",
    "\n",
    "        export_dir = RESULTS_DIR / 'exports' / export_name\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n\ud83d\udcc1 Export directory: {export_dir}\")\n",
    "\n",
    "        # Track export statistics\n",
    "        export_stats = {\n",
    "            'models_exported': 0,\n",
    "            'onnx_exports': 0,\n",
    "            'predictions_exported': 0,\n",
    "            'visualizations_exported': 0,\n",
    "            'model_cards_generated': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "\n",
    "        export_info = {\n",
    "            'data_stats': {\n",
    "                'train_samples': TRAINING_RESULTS.get(models_to_export[0], {}).get('train_samples', 0),\n",
    "                'val_samples': TRAINING_RESULTS.get(models_to_export[0], {}).get('val_samples', 0),\n",
    "                'test_samples': TEST_RESULTS.get(models_to_export[0], {}).get('test_samples', 0),\n",
    "                'n_features': TRAINING_RESULTS.get(models_to_export[0], {}).get('n_features', 0)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Export each model\n",
    "        for model_name in models_to_export:\n",
    "            print(f\"\\n\ud83d\udcca Processing: {model_name}\")\n",
    "\n",
    "            all_results = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}\n",
    "            model_info = all_results.get(model_name, {})\n",
    "\n",
    "            if not model_info:\n",
    "                print(f\"  \u26a0 No training results found for {model_name}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: No training results\")\n",
    "                continue\n",
    "\n",
    "            run_id = model_info.get('run_id')\n",
    "            if not run_id:\n",
    "                print(f\"  \u26a0 No run_id found for {model_name}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: No run_id\")\n",
    "                continue\n",
    "\n",
    "            # Create model directory\n",
    "            model_dir = export_dir / 'models' / model_name\n",
    "            model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Find and copy model file\n",
    "            source_dir = EXPERIMENTS_DIR / run_id\n",
    "            model_file = source_dir / 'model.pkl'\n",
    "\n",
    "            if not model_file.exists():\n",
    "                print(f\"  \u26a0 Model file not found: {model_file}\")\n",
    "                export_stats['errors'].append(f\"{model_name}: Model file not found\")\n",
    "                continue\n",
    "\n",
    "            # Copy model\n",
    "            dest_model = model_dir / 'model.pkl'\n",
    "            shutil.copy2(model_file, dest_model)\n",
    "            model_size = dest_model.stat().st_size / 1e6\n",
    "            print(f\"  \u2713 Model copied ({model_size:.2f} MB)\")\n",
    "            export_stats['models_exported'] += 1\n",
    "\n",
    "            # Export to ONNX if requested\n",
    "            if include_onnx:\n",
    "                feature_names = model_info.get('feature_names', [])\n",
    "                success, message = export_to_onnx(model_info, model_name, dest_model, feature_names)\n",
    "                if success:\n",
    "                    print(f\"  \u2713 ONNX: {message}\")\n",
    "                    export_stats['onnx_exports'] += 1\n",
    "                else:\n",
    "                    print(f\"  \u26a0 ONNX: {message}\")\n",
    "\n",
    "            # Save configuration\n",
    "            config = model_info.get('config', {})\n",
    "            if config:\n",
    "                config_path = model_dir / 'config.json'\n",
    "                with open(config_path, 'w') as f:\n",
    "                    json.dump(config, f, indent=2)\n",
    "                print(f\"  \u2713 Configuration saved\")\n",
    "\n",
    "            # Export predictions\n",
    "            if include_predictions:\n",
    "                try:\n",
    "                    export_predictions(model_name, model_info, export_dir)\n",
    "                    print(f\"  \u2713 Predictions exported\")\n",
    "                    export_stats['predictions_exported'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  \u26a0 Predictions export failed: {e}\")\n",
    "\n",
    "            # Generate model card\n",
    "            if include_model_card:\n",
    "                try:\n",
    "                    card_dir = export_dir / 'model_cards'\n",
    "                    card_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                    test_info = TEST_RESULTS.get(model_name, {})\n",
    "                    cv_info = CV_RESULTS.get(model_name, {})\n",
    "\n",
    "                    card_content = generate_model_card(model_name, model_info, test_info, cv_info)\n",
    "                    card_path = card_dir / f\"{model_name}_card.md\"\n",
    "\n",
    "                    with open(card_path, 'w') as f:\n",
    "                        f.write(card_content)\n",
    "\n",
    "                    print(f\"  \u2713 Model card generated\")\n",
    "                    export_stats['model_cards_generated'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  \u26a0 Model card generation failed: {e}\")\n",
    "\n",
    "        # Copy visualizations\n",
    "        if include_visualizations:\n",
    "            print(f\"\\n\ud83c\udfa8 Copying visualizations...\")\n",
    "            viz_dir = export_dir / 'visualizations'\n",
    "            viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Copy from experiments directory\n",
    "            for model_name in models_to_export:\n",
    "                model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(model_name, {})\n",
    "                run_id = model_info.get('run_id')\n",
    "                if run_id:\n",
    "                    source_viz = EXPERIMENTS_DIR / run_id / 'visualizations'\n",
    "                    if source_viz.exists():\n",
    "                        dest_viz = viz_dir / model_name\n",
    "                        shutil.copytree(source_viz, dest_viz, dirs_exist_ok=True)\n",
    "                        viz_count = len(list(dest_viz.rglob('*.png')))\n",
    "                        export_stats['visualizations_exported'] += viz_count\n",
    "\n",
    "            if export_stats['visualizations_exported'] > 0:\n",
    "                print(f\"  \u2713 {export_stats['visualizations_exported']} visualizations copied\")\n",
    "\n",
    "        # Export metrics\n",
    "        print(f\"\\n\ud83d\udcc8 Exporting metrics...\")\n",
    "        metrics_dir = export_dir / 'metrics'\n",
    "        metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Training metrics\n",
    "        training_metrics = {m: TRAINING_RESULTS[m]['metrics'] for m in models_to_export if m in TRAINING_RESULTS}\n",
    "        with open(metrics_dir / 'training_metrics.json', 'w') as f:\n",
    "            json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "        # Test metrics\n",
    "        test_metrics = {m: TEST_RESULTS[m]['metrics'] for m in models_to_export if m in TEST_RESULTS}\n",
    "        if test_metrics:\n",
    "            with open(metrics_dir / 'test_metrics.json', 'w') as f:\n",
    "                json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "        # CV results\n",
    "        cv_metrics = {m: CV_RESULTS[m] for m in models_to_export if m in CV_RESULTS}\n",
    "        if cv_metrics:\n",
    "            with open(metrics_dir / 'cv_results.json', 'w') as f:\n",
    "                json.dump(cv_metrics, f, indent=2)\n",
    "\n",
    "        print(f\"  \u2713 Metrics exported\")\n",
    "\n",
    "        # Export data info\n",
    "        print(f\"\\n\ud83d\udcca Exporting data information...\")\n",
    "        data_dir = export_dir / 'data'\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Feature names\n",
    "        if models_to_export:\n",
    "            first_model = models_to_export[0]\n",
    "            model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(first_model, {})\n",
    "            feature_names = model_info.get('feature_names', [])\n",
    "\n",
    "            if feature_names:\n",
    "                with open(data_dir / 'feature_names.txt', 'w') as f:\n",
    "                    f.write('\\n'.join(feature_names))\n",
    "\n",
    "        # Label mapping\n",
    "        label_mapping = {-1: 'SHORT', 0: 'NEUTRAL', 1: 'LONG'}\n",
    "        with open(data_dir / 'label_mapping.json', 'w') as f:\n",
    "            json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "        # Data stats\n",
    "        with open(data_dir / 'data_stats.json', 'w') as f:\n",
    "            json.dump(export_info['data_stats'], f, indent=2)\n",
    "\n",
    "        print(f\"  \u2713 Data info exported\")\n",
    "\n",
    "        # Create manifest\n",
    "        print(f\"\\n\ud83d\udccb Creating manifest...\")\n",
    "        manifest_path = create_manifest(export_dir, models_to_export, export_info)\n",
    "        print(f\"  \u2713 Manifest created: {manifest_path.name}\")\n",
    "\n",
    "        # Create README\n",
    "        print(f\"\\n\ud83d\udcdd Creating README...\")\n",
    "        readme_path = create_readme(export_dir, models_to_export)\n",
    "        print(f\"  \u2713 README created: {readme_path.name}\")\n",
    "\n",
    "        # Calculate total size\n",
    "        total_size = sum(f.stat().st_size for f in export_dir.rglob('*') if f.is_file())\n",
    "        total_size_mb = total_size / 1e6\n",
    "\n",
    "        # Create ZIP archive\n",
    "        zip_path = None\n",
    "        if create_zip_archive:\n",
    "            print(f\"\\n\ud83d\udce6 Creating ZIP archive...\")\n",
    "            zip_base = export_dir.parent / export_name\n",
    "            zip_path = Path(shutil.make_archive(str(zip_base), 'zip', export_dir))\n",
    "            zip_size_mb = zip_path.stat().st_size / 1e6\n",
    "            print(f\"  \u2713 Archive created: {zip_path.name} ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"EXPORT SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n\ud83d\udcc1 Export Path: {export_dir}\")\n",
    "        print(f\"\\n\ud83d\udcca Models Exported: {export_stats['models_exported']}\")\n",
    "        for model_name in models_to_export:\n",
    "            model_info = {**TRAINING_RESULTS, **ENSEMBLE_RESULTS}.get(model_name, {})\n",
    "            formats = ['PKL']\n",
    "            if (export_dir / 'models' / model_name / 'model.onnx').exists():\n",
    "                formats.append('ONNX')\n",
    "            print(f\"  \u2713 {model_name} ({', '.join(formats)})\")\n",
    "\n",
    "        print(f\"\\n\ud83d\udce6 Package Contents:\")\n",
    "        print(f\"  \u2713 Models: {export_stats['models_exported']}\")\n",
    "        if export_stats['onnx_exports'] > 0:\n",
    "            print(f\"  \u2713 ONNX exports: {export_stats['onnx_exports']}\")\n",
    "        if include_predictions:\n",
    "            print(f\"  \u2713 Predictions: Val + Test\")\n",
    "        print(f\"  \u2713 Metrics: Training, Test, CV\")\n",
    "        if export_stats['visualizations_exported'] > 0:\n",
    "            print(f\"  \u2713 Visualizations: {export_stats['visualizations_exported']} plots\")\n",
    "        if export_stats['model_cards_generated'] > 0:\n",
    "            print(f\"  \u2713 Model Cards: {export_stats['model_cards_generated']}\")\n",
    "        print(f\"  \u2713 Data Info: Features, labels, stats\")\n",
    "        print(f\"  \u2713 README: Setup and usage guide\")\n",
    "\n",
    "        print(f\"\\n\ud83d\udcbe Total Size: {total_size_mb:.1f} MB\", end='')\n",
    "        if zip_path:\n",
    "            zip_size_mb = zip_path.stat().st_size / 1e6\n",
    "            print(f\" (compressed: {zip_size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "        if export_stats['errors']:\n",
    "            print(f\"\\n\u26a0 Errors ({len(export_stats['errors'])}):\")\n",
    "            for error in export_stats['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "\n",
    "        print(f\"\\n\u2705 Next Steps:\")\n",
    "        print(f\"1. Extract ZIP to deployment environment\")\n",
    "        print(f\"2. Review model cards for performance details\")\n",
    "        if export_stats['onnx_exports'] > 0:\n",
    "            print(f\"3. Use ONNX models for production inference\")\n",
    "        print(f\"4. Check README for usage examples\")\n",
    "\n",
    "        # Colab download helper\n",
    "        if IS_COLAB and create_zip_archive and zip_path:\n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(\"DOWNLOAD TO LOCAL\")\n",
    "            print(\"=\" * 80)\n",
    "            download_export = False  #@param {type: \"boolean\"}\n",
    "\n",
    "            if download_export:\n",
    "                try:\n",
    "                    from google.colab import files\n",
    "                    files.download(str(zip_path))\n",
    "                    print(f\"\\n\u2713 Download started: {zip_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n\u26a0 Download failed: {e}\")\n",
    "                    print(f\"Manual download from: {zip_path}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\"Model export skipped. Enable 'export_model' checkbox above to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Quick Reference\n",
    "\n",
    "## Command Line Usage\n",
    "\n",
    "```bash\n",
    "# Train single model\n",
    "python scripts/train_model.py --model xgboost --horizon 20\n",
    "\n",
    "# Train neural model\n",
    "python scripts/train_model.py --model lstm --horizon 20 --seq-len 60\n",
    "\n",
    "# Run cross-validation\n",
    "python scripts/run_cv.py --models xgboost,lightgbm --horizons 20 --n-splits 5\n",
    "\n",
    "# Train ensemble\n",
    "python scripts/train_model.py --model voting --horizon 20\n",
    "\n",
    "# List all available models\n",
    "python scripts/train_model.py --list-models\n",
    "```\n",
    "\n",
    "## Model Families\n",
    "\n",
    "| Family | Models | Best For |\n",
    "|--------|--------|----------|\n",
    "| Boosting | XGBoost, LightGBM, CatBoost | Fast, accurate, tabular data |\n",
    "| Classical | Random Forest, Logistic, SVM | Baselines, interpretability |\n",
    "| Neural | LSTM, GRU, TCN | Sequential patterns, temporal dependencies |\n",
    "| Ensemble | Voting, Stacking, Blending | Combined predictions, robustness |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}