{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train All Models - Model Comparison\n",
    "\n",
    "This notebook trains and compares all available model types:\n",
    "\n",
    "**Boosting Models:**\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "\n",
    "**Neural Models:**\n",
    "- LSTM\n",
    "- GRU\n",
    "- TCN\n",
    "\n",
    "**Classical Models:**\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "\n",
    "**Ensemble Models:**\n",
    "- Voting Ensemble\n",
    "- Stacking Ensemble\n",
    "- Blending Ensemble\n",
    "\n",
    "At the end, we compare all models on the same test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install -q xgboost lightgbm catboost scikit-learn torch pandas numpy matplotlib tqdm pyarrow\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.utils.notebook import (\n",
    "    setup_notebook, display_metrics, download_sample_data,\n",
    "    plot_confusion_matrix, plot_model_comparison, get_sample_config\n",
    ")\n",
    "\n",
    "env = setup_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.models import ModelRegistry\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Show available models\n",
    "print(\"Available Models:\")\n",
    "all_models = ModelRegistry.list_models()\n",
    "for family, models in all_models.items():\n",
    "    print(f\"  {family}: {', '.join(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "sample_paths = download_sample_data(output_dir=\"../data/sample\", symbols=[\"SAMPLE\"])\n",
    "df = pd.read_parquet(sample_paths[\"SAMPLE\"])\n",
    "print(f\"Loaded {len(df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(df):\n",
    "    \"\"\"Compute technical features.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    for p in [5, 10, 20]:\n",
    "        df[f'return_{p}'] = df['close'].pct_change(p)\n",
    "    \n",
    "    # Moving averages\n",
    "    for p in [10, 20, 50]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'close_to_sma_{p}'] = df['close'] / df[f'sma_{p}'] - 1\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss.replace(0, np.inf)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # ATR\n",
    "    tr = pd.concat([\n",
    "        df['high'] - df['low'],\n",
    "        abs(df['high'] - df['close'].shift(1)),\n",
    "        abs(df['low'] - df['close'].shift(1))\n",
    "    ], axis=1).max(axis=1)\n",
    "    df['atr_14'] = tr.rolling(14).mean()\n",
    "    df['atr_pct'] = df['atr_14'] / df['close']\n",
    "    \n",
    "    # Bollinger\n",
    "    sma20 = df['close'].rolling(20).mean()\n",
    "    std20 = df['close'].rolling(20).std()\n",
    "    df['bb_position'] = (df['close'] - (sma20 - 2*std20)) / (4*std20)\n",
    "    \n",
    "    # Volume\n",
    "    df['volume_sma'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma']\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility_20'] = df['log_return'].rolling(20).std()\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def create_labels(df, horizon=20):\n",
    "    \"\"\"Create triple-barrier labels.\"\"\"\n",
    "    df = df.copy()\n",
    "    labels = np.zeros(len(df))\n",
    "    \n",
    "    for i in range(len(df) - horizon):\n",
    "        entry = df['close'].iloc[i]\n",
    "        atr = df['atr_14'].iloc[i]\n",
    "        upper = entry + 1.5 * atr\n",
    "        lower = entry - 1.0 * atr\n",
    "        \n",
    "        for j in range(1, horizon + 1):\n",
    "            if i + j >= len(df):\n",
    "                break\n",
    "            if df['high'].iloc[i + j] >= upper:\n",
    "                labels[i] = 1\n",
    "                break\n",
    "            if df['low'].iloc[i + j] <= lower:\n",
    "                labels[i] = -1\n",
    "                break\n",
    "    \n",
    "    df['label'] = labels\n",
    "    return df\n",
    "\n",
    "# Process data\n",
    "df_features = compute_features(df)\n",
    "df_labeled = create_labels(df_features)\n",
    "\n",
    "# Define features\n",
    "exclude_cols = ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'label']\n",
    "feature_cols = [c for c in df_labeled.columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "n = len(df_labeled)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "purge = 60\n",
    "\n",
    "train_df = df_labeled.iloc[:train_end - purge]\n",
    "val_df = df_labeled.iloc[train_end + purge:val_end - purge]\n",
    "test_df = df_labeled.iloc[val_end + purge:]\n",
    "\n",
    "# Tabular data (for boosting/classical)\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['label'].values.astype(int) + 1\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['label'].values.astype(int) + 1\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['label'].values.astype(int) + 1\n",
    "\n",
    "# Sequence data (for neural models)\n",
    "seq_len = 60\n",
    "\n",
    "def create_sequences(X, y, seq_len):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(seq_len, len(X)):\n",
    "        X_seq.append(X[i-seq_len:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_len)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, seq_len)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_len)\n",
    "\n",
    "print(f\"Tabular shapes: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}\")\n",
    "print(f\"Sequence shapes: Train {X_train_seq.shape}, Val {X_val_seq.shape}, Test {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for comparison\n",
    "all_results = {}\n",
    "\n",
    "def train_and_evaluate(model_name, X_train, y_train, X_val, y_val, X_test, y_test, config=None):\n",
    "    \"\"\"Train a model and return results.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        model = ModelRegistry.create(model_name, config=config)\n",
    "        print(f\"Model family: {model.model_family}\")\n",
    "        \n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        training_metrics = model.fit(X_train, y_train, X_val, y_val)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        predictions = model.predict(X_test)\n",
    "        y_pred = predictions.class_predictions\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Train Time: {train_time:.1f}s\")\n",
    "        print(f\"  Val F1: {training_metrics.val_f1:.4f}\")\n",
    "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Test F1 (macro): {f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'training_metrics': training_metrics.to_dict(),\n",
    "            'evaluation_metrics': {\n",
    "                'accuracy': accuracy,\n",
    "                'macro_f1': f1,\n",
    "            },\n",
    "            'predictions': y_pred,\n",
    "            'train_time': train_time,\n",
    "            'success': True,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return {'success': False, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "all_results['xgboost'] = train_and_evaluate(\n",
    "    'xgboost', X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    config={'n_estimators': 150, 'max_depth': 6, 'early_stopping_rounds': 20}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "all_results['lightgbm'] = train_and_evaluate(\n",
    "    'lightgbm', X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    config={'n_estimators': 150, 'max_depth': 6, 'early_stopping_rounds': 20}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "all_results['catboost'] = train_and_evaluate(\n",
    "    'catboost', X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    config={'iterations': 150, 'depth': 6, 'early_stopping_rounds': 20}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "all_results['random_forest'] = train_and_evaluate(\n",
    "    'random_forest', X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    config={'n_estimators': 100, 'max_depth': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (needs scaled data)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "all_results['logistic'] = train_and_evaluate(\n",
    "    'logistic', X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "    config={'max_iter': 500}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM (use subset for speed)\n",
    "subset_size = min(5000, len(X_train_scaled))\n",
    "all_results['svm'] = train_and_evaluate(\n",
    "    'svm', X_train_scaled[:subset_size], y_train[:subset_size], \n",
    "    X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "    config={'kernel': 'rbf', 'C': 1.0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Models (GPU required for best performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have GPU for neural models\n",
    "if env['gpu_available']:\n",
    "    print(f\"GPU available: {env['gpu_name']} - training neural models\")\n",
    "    train_neural = True\n",
    "else:\n",
    "    print(\"No GPU available - neural models will be slow. Training anyway...\")\n",
    "    train_neural = True  # Train anyway but with reduced settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_neural:\n",
    "    # LSTM\n",
    "    all_results['lstm'] = train_and_evaluate(\n",
    "        'lstm', X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq,\n",
    "        config={'hidden_size': 64, 'num_layers': 1, 'epochs': 10, 'batch_size': 64}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_neural:\n",
    "    # GRU\n",
    "    all_results['gru'] = train_and_evaluate(\n",
    "        'gru', X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq,\n",
    "        config={'hidden_size': 64, 'num_layers': 1, 'epochs': 10, 'batch_size': 64}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_neural:\n",
    "    # TCN\n",
    "    all_results['tcn'] = train_and_evaluate(\n",
    "        'tcn', X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq,\n",
    "        config={'num_channels': [32, 32], 'epochs': 10, 'batch_size': 64}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble (using boosting models)\n",
    "successful_models = {k: v['model'] for k, v in all_results.items() \n",
    "                     if v.get('success') and k in ['xgboost', 'lightgbm', 'catboost']}\n",
    "\n",
    "if len(successful_models) >= 2:\n",
    "    all_results['voting'] = train_and_evaluate(\n",
    "        'voting', X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        config={\n",
    "            'base_models': list(successful_models.values()),\n",
    "            'voting': 'soft'\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    print(\"Not enough successful models for voting ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Ensemble\n",
    "if len(successful_models) >= 2:\n",
    "    all_results['stacking'] = train_and_evaluate(\n",
    "        'stacking', X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        config={\n",
    "            'base_models': list(successful_models.values()),\n",
    "            'meta_learner': 'logistic'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in all_results.items():\n",
    "    if result.get('success'):\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Test Accuracy': result['evaluation_metrics']['accuracy'],\n",
    "            'Test F1 (macro)': result['evaluation_metrics']['macro_f1'],\n",
    "            'Val F1': result['training_metrics'].get('val_f1', 0),\n",
    "            'Train Time (s)': result['train_time'],\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test F1 (macro)', ascending=False)\n",
    "comparison_df = comparison_df.reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" MODEL COMPARISON - Sorted by Test F1\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plot_model_comparison(\n",
    "    {k: v for k, v in all_results.items() if v.get('success')},\n",
    "    metric='macro_f1',\n",
    "    title='Model Comparison - Test F1 Score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = comparison_df['Model'].tolist()\n",
    "times = comparison_df['Train Time (s)'].tolist()\n",
    "f1_scores = comparison_df['Test F1 (macro)'].tolist()\n",
    "\n",
    "colors = plt.cm.RdYlGn(np.array(f1_scores) / max(f1_scores))\n",
    "bars = ax.barh(models[::-1], times[::-1], color=colors[::-1])\n",
    "\n",
    "ax.set_xlabel('Training Time (seconds)')\n",
    "ax.set_title('Training Time by Model (color = F1 score)')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Add time labels\n",
    "for bar, t in zip(bars, times[::-1]):\n",
    "    ax.text(t + 0.5, bar.get_y() + bar.get_height()/2, f'{t:.1f}s', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_result = all_results[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name.upper()}\")\n",
    "print(f\"Test F1: {best_result['evaluation_metrics']['macro_f1']:.4f}\")\n",
    "print(f\"Test Accuracy: {best_result['evaluation_metrics']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "# Use appropriate test labels based on model type\n",
    "if best_model_name in ['lstm', 'gru', 'tcn']:\n",
    "    y_test_best = y_test_seq\n",
    "else:\n",
    "    y_test_best = y_test\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    y_test_best, best_result['predictions'],\n",
    "    labels=['Short', 'Neutral', 'Long'],\n",
    "    title=f'{best_model_name.upper()} - Confusion Matrix'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for best model\n",
    "print(f\"\\n{best_model_name.upper()} - Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test_best, best_result['predictions'],\n",
    "    target_names=['Short', 'Neutral', 'Long']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Save comparison results\n",
    "output_dir = Path('../outputs/model_comparison')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(output_dir / 'model_comparison.csv', index=False)\n",
    "print(f\"Saved comparison to {output_dir / 'model_comparison.csv'}\")\n",
    "\n",
    "# Save best model\n",
    "best_model_path = output_dir / f'best_model_{best_model_name}'\n",
    "best_result['model'].save(best_model_path)\n",
    "print(f\"Saved best model to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook compared all available model types:\n",
    "\n",
    "**Key Findings:**\n",
    "1. Boosting models (XGBoost, LightGBM, CatBoost) typically offer best accuracy/speed tradeoff\n",
    "2. Neural models (LSTM, GRU, TCN) can capture temporal patterns but require more training time\n",
    "3. Ensemble methods can improve robustness by combining multiple models\n",
    "\n",
    "**Next Steps:**\n",
    "- Use `03_cross_validation.ipynb` for proper cross-validation\n",
    "- Fine-tune the best model's hyperparameters\n",
    "- Build ensemble of top-performing models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
