{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Quickstart Guide\n",
    "\n",
    "This notebook provides a quick introduction to training ML models on OHLCV time series data.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Setting up the environment (Colab/local)\n",
    "2. Loading and preparing data\n",
    "3. Training an XGBoost model\n",
    "4. Evaluating results and visualizing predictions\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 or better) OR local machine with GPU\n",
    "- ~2GB RAM for sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Run this cell first to set up the environment. It will:\n",
    "- Install required packages\n",
    "- Detect GPU/CPU\n",
    "- Configure display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (uncomment if running in fresh Colab)\n",
    "# !git clone https://github.com/YOUR_USERNAME/Research.git\n",
    "# %cd Research\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q xgboost lightgbm catboost scikit-learn pandas numpy matplotlib tqdm pyarrow\n",
    "\n",
    "# Add project to path\n",
    "import sys\n",
    "sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup notebook environment\n",
    "from src.utils.notebook import setup_notebook, display_metrics, plot_confusion_matrix\n",
    "\n",
    "env = setup_notebook()\n",
    "print(f\"\\nGPU Available: {env['gpu_available']}\")\n",
    "if env['gpu_available']:\n",
    "    print(f\"GPU Name: {env['gpu_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Sample Data\n",
    "\n",
    "We'll generate synthetic OHLCV data for this quickstart. In production, you'd load your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.notebook import download_sample_data\n",
    "import pandas as pd\n",
    "\n",
    "# Generate sample data\n",
    "sample_paths = download_sample_data(output_dir=\"data/sample\", symbols=[\"SAMPLE\"])\n",
    "\n",
    "# Load and inspect\n",
    "df = pd.read_parquet(sample_paths[\"SAMPLE\"])\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "print(f\"Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "\n",
    "Generate technical indicators from the OHLCV data. This is a simplified version - the full pipeline has 150+ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_features(df):\n",
    "    \"\"\"Compute technical features for model training.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    for p in [5, 10, 20]:\n",
    "        df[f'return_{p}'] = df['close'].pct_change(p)\n",
    "    \n",
    "    # Moving averages\n",
    "    for p in [10, 20, 50]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'close_to_sma_{p}'] = df['close'] / df[f'sma_{p}'] - 1\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss.replace(0, np.inf)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # ATR\n",
    "    tr = pd.concat([\n",
    "        df['high'] - df['low'],\n",
    "        abs(df['high'] - df['close'].shift(1)),\n",
    "        abs(df['low'] - df['close'].shift(1))\n",
    "    ], axis=1).max(axis=1)\n",
    "    df['atr_14'] = tr.rolling(14).mean()\n",
    "    df['atr_pct'] = df['atr_14'] / df['close']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    sma20 = df['close'].rolling(20).mean()\n",
    "    std20 = df['close'].rolling(20).std()\n",
    "    df['bb_position'] = (df['close'] - (sma20 - 2*std20)) / (4*std20)\n",
    "    \n",
    "    # Volume features\n",
    "    df['volume_sma'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma']\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility_20'] = df['log_return'].rolling(20).std()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "df_features = compute_features(df)\n",
    "print(f\"Features computed: {len(df_features.columns) - 6} features\")\n",
    "print(f\"Samples after feature computation: {len(df_features):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Labels\n",
    "\n",
    "Create trading labels using a simplified triple-barrier approach:\n",
    "- **Long (+1)**: Price increases by more than 1 ATR\n",
    "- **Short (-1)**: Price decreases by more than 1 ATR\n",
    "- **Neutral (0)**: Neither barrier hit within horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df, horizon=20, k_up=1.5, k_down=1.0):\n",
    "    \"\"\"Create triple-barrier labels.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    labels = np.zeros(len(df))\n",
    "    \n",
    "    for i in range(len(df) - horizon):\n",
    "        entry = df['close'].iloc[i]\n",
    "        atr = df['atr_14'].iloc[i]\n",
    "        \n",
    "        upper = entry + k_up * atr\n",
    "        lower = entry - k_down * atr\n",
    "        \n",
    "        for j in range(1, horizon + 1):\n",
    "            if i + j >= len(df):\n",
    "                break\n",
    "            high = df['high'].iloc[i + j]\n",
    "            low = df['low'].iloc[i + j]\n",
    "            \n",
    "            if high >= upper:\n",
    "                labels[i] = 1  # Long\n",
    "                break\n",
    "            if low <= lower:\n",
    "                labels[i] = -1  # Short\n",
    "                break\n",
    "    \n",
    "    df['label'] = labels\n",
    "    return df\n",
    "\n",
    "df_labeled = create_labels(df_features, horizon=20)\n",
    "\n",
    "# Check distribution\n",
    "label_dist = df_labeled['label'].value_counts().sort_index()\n",
    "print(\"Label Distribution:\")\n",
    "print(f\"  Short (-1): {label_dist.get(-1, 0):,} ({label_dist.get(-1, 0)/len(df_labeled)*100:.1f}%)\")\n",
    "print(f\"  Neutral (0): {label_dist.get(0, 0):,} ({label_dist.get(0, 0)/len(df_labeled)*100:.1f}%)\")\n",
    "print(f\"  Long (+1): {label_dist.get(1, 0):,} ({label_dist.get(1, 0)/len(df_labeled)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train/Test Split\n",
    "\n",
    "Split data chronologically with proper purge/embargo to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "exclude_cols = ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'label']\n",
    "feature_cols = [c for c in df_labeled.columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Sample features: {feature_cols[:10]}\")\n",
    "\n",
    "# Split data (70/15/15 with purge)\n",
    "n = len(df_labeled)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "purge = 60  # Purge period\n",
    "\n",
    "train_df = df_labeled.iloc[:train_end - purge]\n",
    "val_df = df_labeled.iloc[train_end + purge:val_end - purge]\n",
    "test_df = df_labeled.iloc[val_end + purge:]\n",
    "\n",
    "# Prepare arrays\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['label'].values.astype(int) + 1  # Convert to 0,1,2\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['label'].values.astype(int) + 1\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['label'].values.astype(int) + 1\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(X_train):,}\")\n",
    "print(f\"  Val: {len(X_val):,}\")\n",
    "print(f\"  Test: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train XGBoost Model\n",
    "\n",
    "Train an XGBoost classifier using the Model Factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ModelRegistry\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# List available models\n",
    "print(\"Available Models:\")\n",
    "for family, models in ModelRegistry.list_models().items():\n",
    "    print(f\"  {family}: {', '.join(models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost model with custom config\n",
    "model_config = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'early_stopping_rounds': 20,\n",
    "    'eval_metric': 'mlogloss',\n",
    "}\n",
    "\n",
    "model = ModelRegistry.create('xgboost', config=model_config)\n",
    "print(f\"Created model: {model}\")\n",
    "print(f\"Model family: {model.model_family}\")\n",
    "print(f\"Requires scaling: {model.requires_scaling}\")\n",
    "print(f\"Requires sequences: {model.requires_sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training XGBoost model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "training_metrics = model.fit(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f}s\")\n",
    "print(f\"Best epoch: {training_metrics.best_epoch}\")\n",
    "print(f\"Train F1: {training_metrics.train_f1:.4f}\")\n",
    "print(f\"Val F1: {training_metrics.val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "y_pred = predictions.class_predictions\n",
    "y_proba = predictions.class_probabilities\n",
    "confidence = predictions.confidence\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Mean Confidence: {confidence.mean():.4f}\")\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Short', 'Neutral', 'Long']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred,\n",
    "    labels=['Short', 'Neutral', 'Long'],\n",
    "    title='XGBoost - Test Set Confusion Matrix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance\n",
    "importance = model.get_feature_importance()\n",
    "\n",
    "if importance:\n",
    "    # Sort by importance\n",
    "    sorted_importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15])\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    features = list(sorted_importance.keys())\n",
    "    values = list(sorted_importance.values())\n",
    "    \n",
    "    ax.barh(features[::-1], values[::-1], color='steelblue')\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title('Top 15 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Save model\n",
    "save_path = Path('models/xgboost_quickstart')\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save(save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# List saved files\n",
    "for f in save_path.iterdir():\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Load and Verify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model instance and load weights\n",
    "loaded_model = ModelRegistry.create('xgboost')\n",
    "loaded_model.load(save_path)\n",
    "\n",
    "# Verify predictions match\n",
    "loaded_predictions = loaded_model.predict(X_test[:100])\n",
    "original_predictions = model.predict(X_test[:100])\n",
    "\n",
    "predictions_match = np.allclose(\n",
    "    loaded_predictions.class_probabilities,\n",
    "    original_predictions.class_probabilities\n",
    ")\n",
    "\n",
    "print(f\"Predictions match after load: {predictions_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this quickstart, you learned:\n",
    "\n",
    "1. **Environment Setup**: How to configure the notebook environment for GPU training\n",
    "2. **Data Preparation**: Loading OHLCV data and computing technical features\n",
    "3. **Labeling**: Creating trading labels using the triple-barrier method\n",
    "4. **Model Training**: Training an XGBoost model using the Model Factory\n",
    "5. **Evaluation**: Computing metrics and visualizing results\n",
    "6. **Persistence**: Saving and loading trained models\n",
    "\n",
    "**Next Steps:**\n",
    "- Try `02_train_all_models.ipynb` to compare different model types\n",
    "- Use `03_cross_validation.ipynb` for proper cross-validation\n",
    "- Load your own OHLCV data for real trading predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
