{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ML Model Factory - Complete Pipeline & Training\n\nThis notebook runs the **complete ML pipeline** from raw data to trained models.\n\n## What This Notebook Does\n1. **Setup** - Mount Drive (for results), clone GitHub repo (for code & data)\n2. **Phase 1** - Data pipeline (clean -> features -> labels -> splits)\n3. **Phase 2** - Model training (single or multiple models)\n4. **Phase 3** - Cross-validation (optional)\n5. **Phase 4** - Ensemble training (optional)\n\n## Data Flow\n- **Data Source:** `/content/research/` (cloned from GitHub)\n- **Results Saved:** `/content/drive/MyDrive/research/` (Google Drive for persistence)\n\n## Quick Start\n1. Run cells in order (or use Runtime -> Run all)\n2. Data is loaded from the GitHub clone\n3. Results are saved to Google Drive for persistence\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 1.1 Mount Google Drive & Clone Repository { display-mode: \"form\" }\n#@markdown Run this cell to mount your Google Drive and set up the project.\n\nimport os\nimport sys\nfrom pathlib import Path\n\n# Mount Google Drive (for saving results only)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Project paths\nDRIVE_PROJECT = '/content/drive/MyDrive/research'  # For saving results\nLOCAL_PROJECT = '/content/research'  # Data comes from GitHub clone\n\n# Clone or pull repository\nif not Path(LOCAL_PROJECT).exists():\n    print(\"Cloning repository...\")\n    !git clone https://github.com/Snehpatel101/research.git {LOCAL_PROJECT}\nelse:\n    print(\"Pulling latest changes...\")\n    !cd {LOCAL_PROJECT} && git pull\n\n# Change to project directory\nos.chdir(LOCAL_PROJECT)\nsys.path.insert(0, LOCAL_PROJECT)\n\n# Create Drive directories for saving results\nfor d in [\"experiments/runs\", \"results\"]:\n    Path(DRIVE_PROJECT, d).mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nProject directory: {os.getcwd()}\")\nprint(f\"Data source: {LOCAL_PROJECT} (from GitHub)\")\nprint(f\"Results saved to: {DRIVE_PROJECT} (Google Drive)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 1.2 Install Dependencies { display-mode: \"form\" }\n#@markdown Installs all required packages for the ML pipeline.\n\nprint(\"Installing dependencies...\")\n\n# Add project to Python path (no pip install -e needed)\nimport sys\nif LOCAL_PROJECT not in sys.path:\n    sys.path.insert(0, LOCAL_PROJECT)\n\n# Install required packages\n!pip install xgboost lightgbm catboost optuna ta pywavelets scikit-learn pandas numpy -q\n\n# Verify PyTorch with CUDA\nimport torch\nif torch.cuda.is_available():\n    print(f\"PyTorch: {torch.__version__} with CUDA {torch.version.cuda}\")\nelse:\n    print(f\"PyTorch: {torch.__version__} (CPU only)\")\n\nprint(f\"\\nProject path added: {LOCAL_PROJECT}\")\nprint(\"Dependencies installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.3 Detect Hardware & Configure { display-mode: \"form\" }\n",
    "#@markdown Detects GPU and configures optimal settings.\n",
    "\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" HARDWARE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# System info\n",
    "print(f\"\\nSystem: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "\n",
    "# GPU detection\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_NAME = None\n",
    "GPU_MEMORY = 0\n",
    "RECOMMENDED_BATCH_SIZE = 256\n",
    "MIXED_PRECISION = False\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    GPU_NAME = props.name\n",
    "    GPU_MEMORY = props.total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"\\nGPU: {GPU_NAME}\")\n",
    "    print(f\"Memory: {GPU_MEMORY:.1f} GB\")\n",
    "    print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
    "    \n",
    "    # Set batch size based on memory\n",
    "    if GPU_MEMORY >= 40:  # A100\n",
    "        RECOMMENDED_BATCH_SIZE = 1024\n",
    "        MIXED_PRECISION = True\n",
    "    elif GPU_MEMORY >= 15:  # T4/V100\n",
    "        RECOMMENDED_BATCH_SIZE = 512\n",
    "        MIXED_PRECISION = True\n",
    "    else:\n",
    "        RECOMMENDED_BATCH_SIZE = 256\n",
    "        MIXED_PRECISION = props.major >= 7\n",
    "    \n",
    "    print(f\"\\nRecommended batch size: {RECOMMENDED_BATCH_SIZE}\")\n",
    "    print(f\"Mixed precision: {'Enabled' if MIXED_PRECISION else 'Disabled'}\")\n",
    "else:\n",
    "    print(\"\\nNo GPU detected - will use CPU\")\n",
    "    print(\"Tip: Runtime -> Change runtime type -> GPU\")\n",
    "\n",
    "# Verify model registry\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" AVAILABLE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from src.models import ModelRegistry\n",
    "    models = ModelRegistry.list_models()\n",
    "    for family, model_list in models.items():\n",
    "        print(f\"\\n{family.upper()}:\")\n",
    "        for m in model_list:\n",
    "            gpu_req = \"GPU\" if m in ['lstm', 'gru', 'tcn'] else \"CPU\"\n",
    "            print(f\"  - {m} ({gpu_req})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2.1 Check Raw Data Files { display-mode: \"form\" }\n#@markdown Verifies your OHLCV data files exist in the GitHub clone.\n\nimport pandas as pd\nfrom pathlib import Path\n\n# Data comes from the cloned GitHub repo\nRAW_DATA_DIR = Path(LOCAL_PROJECT) / \"data\" / \"raw\"\n\nprint(f\"Looking for data in: {RAW_DATA_DIR}\")\nprint(\"=\" * 60)\n\n# Find all data files\nparquet_files = list(RAW_DATA_DIR.glob(\"*.parquet\")) if RAW_DATA_DIR.exists() else []\ncsv_files = list(RAW_DATA_DIR.glob(\"*.csv\")) if RAW_DATA_DIR.exists() else []\nall_files = parquet_files + csv_files\n\nAVAILABLE_SYMBOLS = []\n\nif all_files:\n    print(\"\\nFound data files:\")\n    for f in all_files:\n        try:\n            if f.suffix == '.parquet':\n                df = pd.read_parquet(f)\n            else:\n                df = pd.read_csv(f)\n            \n            # Extract symbol from filename\n            symbol = f.stem.split('_')[0].upper()\n            AVAILABLE_SYMBOLS.append(symbol)\n            \n            size_mb = f.stat().st_size / 1e6\n            print(f\"\\n  {f.name}\")\n            print(f\"    Symbol: {symbol}\")\n            print(f\"    Rows: {len(df):,}\")\n            print(f\"    Size: {size_mb:.1f} MB\")\n            print(f\"    Columns: {list(df.columns)}\")\n            if 'datetime' in df.columns or df.index.name == 'datetime':\n                if 'datetime' in df.columns:\n                    df['datetime'] = pd.to_datetime(df['datetime'])\n                    print(f\"    Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n        except Exception as e:\n            print(f\"  {f.name}: Error - {e}\")\n    \n    AVAILABLE_SYMBOLS = list(set(AVAILABLE_SYMBOLS))\n    print(f\"\\nAvailable symbols: {AVAILABLE_SYMBOLS}\")\nelse:\n    print(\"\\nNo raw data files found in GitHub clone.\")\n    print(\"\\nIf you have processed data, skip to Section 3.2.\")\n    print(\"Otherwise, add data to the repo before running pipeline.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2.2 Create Directory Structure { display-mode: \"form\" }\n#@markdown Creates all required directories for the pipeline.\n\nfrom pathlib import Path\n\n# Data directories in LOCAL_PROJECT (GitHub clone)\ndata_directories = [\n    \"data/raw\",\n    \"data/clean\",\n    \"data/features\",\n    \"data/labels\",\n    \"data/final\",\n    \"data/splits/scaled\",\n    \"data/stacking\",\n    \"config/ga_results\",\n    \"runs\",\n]\n\n# Results directories in DRIVE_PROJECT (Google Drive for persistence)\nresult_directories = [\n    \"results\",\n    \"experiments/runs\",\n]\n\nprint(\"Creating data directories in GitHub clone...\")\nfor d in data_directories:\n    path = Path(LOCAL_PROJECT) / d\n    path.mkdir(parents=True, exist_ok=True)\n    print(f\"  {LOCAL_PROJECT}/{d}/\")\n\nprint(\"\\nCreating result directories in Google Drive...\")\nfor d in result_directories:\n    path = Path(DRIVE_PROJECT) / d\n    path.mkdir(parents=True, exist_ok=True)\n    print(f\"  {DRIVE_PROJECT}/{d}/\")\n\nprint(\"\\nDirectory structure ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Phase 1: Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Configure Pipeline { display-mode: \"form\" }\n",
    "#@markdown Configure the data processing pipeline.\n",
    "\n",
    "#@markdown ### Symbol Selection\n",
    "symbols = \"MES\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated symbols (e.g., \"MES,MGC\")\n",
    "\n",
    "#@markdown ### Timeframe\n",
    "target_timeframe = \"5min\"  #@param [\"5min\", \"10min\", \"15min\", \"30min\", \"1H\"]\n",
    "\n",
    "#@markdown ### Label Horizons\n",
    "horizons = \"5,10,15,20\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated horizons (bars ahead)\n",
    "\n",
    "#@markdown ### Train/Val/Test Split\n",
    "train_ratio = 0.70  #@param {type: \"slider\", min: 0.5, max: 0.8, step: 0.05}\n",
    "val_ratio = 0.15  #@param {type: \"slider\", min: 0.1, max: 0.25, step: 0.05}\n",
    "\n",
    "# Parse inputs\n",
    "SYMBOLS = [s.strip().upper() for s in symbols.split(',')]\n",
    "HORIZONS = [int(h.strip()) for h in horizons.split(',')]\n",
    "TRAIN_RATIO = train_ratio\n",
    "VAL_RATIO = val_ratio\n",
    "TEST_RATIO = round(1.0 - train_ratio - val_ratio, 2)\n",
    "\n",
    "print(\"Pipeline Configuration:\")\n",
    "print(f\"  Symbols: {SYMBOLS}\")\n",
    "print(f\"  Timeframe: {target_timeframe}\")\n",
    "print(f\"  Horizons: {HORIZONS}\")\n",
    "print(f\"  Train/Val/Test: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3.2 Run Data Pipeline OR Use Existing Data { display-mode: \"form\" }\n#@markdown Choose whether to run the full pipeline or use existing processed data.\n\ndata_source = \"Use existing processed data\"  #@param [\"Run full pipeline (requires raw data)\", \"Use existing processed data\"]\n\nfrom pathlib import Path\nimport time\n\n# Data comes from LOCAL_PROJECT (GitHub clone)\nsplits_dir = Path(LOCAL_PROJECT) / \"data\" / \"splits\" / \"scaled\"\ntrain_file = splits_dir / \"train_scaled.parquet\"\n\nif data_source == \"Use existing processed data\":\n    # Check if processed data exists\n    if train_file.exists():\n        import pandas as pd\n        train_df = pd.read_parquet(train_file)\n        val_df = pd.read_parquet(splits_dir / \"val_scaled.parquet\")\n        test_df = pd.read_parquet(splits_dir / \"test_scaled.parquet\")\n        \n        print(\"Found existing processed data!\")\n        print(f\"  Location: {splits_dir}\")\n        print(f\"  Train: {len(train_df):,} samples\")\n        print(f\"  Val: {len(val_df):,} samples\")\n        print(f\"  Test: {len(test_df):,} samples\")\n        print(\"\\nSkipping pipeline - proceeding to model training!\")\n    else:\n        print(\"ERROR: Processed data not found!\")\n        print(f\"  Expected: {splits_dir}/\")\n        print(\"\\nMake sure the GitHub repo contains processed data files:\")\n        print(\"  - train_scaled.parquet\")\n        print(\"  - val_scaled.parquet\")\n        print(\"  - test_scaled.parquet\")\n        print(\"\\nOr select 'Run full pipeline' and ensure raw data exists.\")\n\nelse:\n    # Run full pipeline\n    raw_dir = Path(LOCAL_PROJECT) / \"data\" / \"raw\"\n    raw_files = list(raw_dir.glob(\"*.parquet\")) + list(raw_dir.glob(\"*.csv\")) if raw_dir.exists() else []\n    \n    if not raw_files:\n        print(\"ERROR: No raw data files found!\")\n        print(f\"  Expected: {raw_dir}/MES_1m.parquet or .csv\")\n        print(\"\\nPlease ensure raw data is in the GitHub repo.\")\n    else:\n        print(\"Running Phase 1 Data Pipeline...\")\n        print(\"=\" * 60)\n        \n        start_time = time.time()\n        \n        try:\n            from src.phase1.pipeline_config import PipelineConfig\n            from src.pipeline.runner import PipelineRunner\n            \n            config = PipelineConfig(\n                symbols=SYMBOLS,\n                project_root=Path(LOCAL_PROJECT),\n                target_timeframe=target_timeframe,\n                label_horizons=HORIZONS,\n                train_ratio=TRAIN_RATIO,\n                val_ratio=VAL_RATIO,\n                test_ratio=TEST_RATIO,\n            )\n            \n            runner = PipelineRunner(config)\n            success = runner.run()\n            \n            elapsed = time.time() - start_time\n            \n            print(\"\\n\" + \"=\" * 60)\n            if success:\n                print(f\"Pipeline completed in {elapsed/60:.1f} minutes!\")\n            else:\n                print(\"Pipeline failed. Check errors above.\")\n                \n        except Exception as e:\n            print(f\"\\nError: {e}\")\n            import traceback\n            traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3.3 Verify Processed Data { display-mode: \"form\" }\n#@markdown Loads and displays the processed datasets.\n\nimport pandas as pd\nfrom pathlib import Path\n\n# Data comes from LOCAL_PROJECT (GitHub clone)\nsplits_dir = Path(LOCAL_PROJECT) / \"data\" / \"splits\" / \"scaled\"\n\nprint(\"Loading processed datasets...\")\nprint(\"=\" * 60)\n\ntry:\n    train_df = pd.read_parquet(splits_dir / \"train_scaled.parquet\")\n    val_df = pd.read_parquet(splits_dir / \"val_scaled.parquet\")\n    test_df = pd.read_parquet(splits_dir / \"test_scaled.parquet\")\n    \n    print(f\"\\nDataset sizes:\")\n    print(f\"  Train: {len(train_df):,} samples\")\n    print(f\"  Val:   {len(val_df):,} samples\")\n    print(f\"  Test:  {len(test_df):,} samples\")\n    print(f\"  Total: {len(train_df) + len(val_df) + len(test_df):,} samples\")\n    \n    # Count features\n    feature_cols = [c for c in train_df.columns if not c.startswith(('label_', 'sample_weight', 'quality_score', 'datetime', 'symbol'))]\n    label_cols = [c for c in train_df.columns if c.startswith('label_')]\n    \n    print(f\"\\nFeatures: {len(feature_cols)}\")\n    print(f\"Labels: {label_cols}\")\n    \n    # Label distribution\n    print(f\"\\nLabel distribution (train):\")\n    for col in label_cols:\n        dist = train_df[col].value_counts().sort_index()\n        print(f\"  {col}: Long={dist.get(1, 0):,} | Neutral={dist.get(0, 0):,} | Short={dist.get(-1, 0):,}\")\n    \n    # Store for later use\n    TRAIN_DF = train_df\n    VAL_DF = val_df\n    TEST_DF = test_df\n    FEATURE_COLS = feature_cols\n    \n    print(\"\\nData ready for model training!\")\n    \nexcept FileNotFoundError:\n    print(\"Processed data not found. Run Section 3.2 first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Phase 2: Model Training\n",
    "\n",
    "Choose between **Single Model** or **Multi-Model** training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Training Mode Selection { display-mode: \"form\" }\n",
    "#@markdown Choose your training mode and models.\n",
    "\n",
    "training_mode = \"Single Model\"  #@param [\"Single Model\", \"Multi-Model (Sequential)\", \"Multi-Model (Compare All)\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Single Model Options\n",
    "single_model = \"xgboost\"  #@param [\"xgboost\", \"lightgbm\", \"catboost\", \"random_forest\", \"logistic\", \"svm\", \"lstm\", \"gru\", \"tcn\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Multi-Model Options\n",
    "train_boosting = True  #@param {type: \"boolean\"}\n",
    "#@markdown XGBoost, LightGBM, CatBoost\n",
    "train_classical = False  #@param {type: \"boolean\"}\n",
    "#@markdown Random Forest, Logistic, SVM\n",
    "train_neural = False  #@param {type: \"boolean\"}\n",
    "#@markdown LSTM, GRU, TCN (requires GPU)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Training Parameters\n",
    "horizon = 20  #@param [5, 10, 15, 20]\n",
    "sequence_length = 60  #@param {type: \"slider\", min: 30, max: 120, step: 10}\n",
    "#@markdown Sequence length for neural models\n",
    "\n",
    "# Build model list\n",
    "if training_mode == \"Single Model\":\n",
    "    MODELS_TO_TRAIN = [single_model]\n",
    "elif training_mode == \"Multi-Model (Compare All)\":\n",
    "    MODELS_TO_TRAIN = []\n",
    "    if train_boosting:\n",
    "        MODELS_TO_TRAIN.extend(['xgboost', 'lightgbm', 'catboost'])\n",
    "    if train_classical:\n",
    "        MODELS_TO_TRAIN.extend(['random_forest', 'logistic', 'svm'])\n",
    "    if train_neural and GPU_AVAILABLE:\n",
    "        MODELS_TO_TRAIN.extend(['lstm', 'gru', 'tcn'])\n",
    "    elif train_neural and not GPU_AVAILABLE:\n",
    "        print(\"WARNING: Neural models skipped (no GPU)\")\n",
    "else:\n",
    "    MODELS_TO_TRAIN = []\n",
    "    if train_boosting:\n",
    "        MODELS_TO_TRAIN.extend(['xgboost', 'lightgbm', 'catboost'])\n",
    "    if train_classical:\n",
    "        MODELS_TO_TRAIN.extend(['random_forest', 'logistic', 'svm'])\n",
    "    if train_neural and GPU_AVAILABLE:\n",
    "        MODELS_TO_TRAIN.extend(['lstm', 'gru', 'tcn'])\n",
    "\n",
    "HORIZON = horizon\n",
    "SEQ_LEN = sequence_length\n",
    "\n",
    "print(f\"Training Mode: {training_mode}\")\n",
    "print(f\"Models to train: {MODELS_TO_TRAIN}\")\n",
    "print(f\"Horizon: H{HORIZON}\")\n",
    "if any(m in ['lstm', 'gru', 'tcn'] for m in MODELS_TO_TRAIN):\n",
    "    print(f\"Sequence length: {SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 4.2 Train Models { display-mode: \"form\" }\n#@markdown Execute model training based on your selections.\n\nimport time\nfrom pathlib import Path\n\nprint(\"=\" * 60)\nprint(\" MODEL TRAINING\")\nprint(\"=\" * 60)\n\n# Results storage\nTRAINING_RESULTS = {}\n\ntry:\n    from src.models import ModelRegistry, Trainer, TrainerConfig\n    from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n    \n    # Load data container from LOCAL_PROJECT (GitHub clone)\n    print(f\"\\nLoading data for horizon H{HORIZON}...\")\n    container = TimeSeriesDataContainer.from_parquet_dir(\n        path=Path(LOCAL_PROJECT) / \"data\" / \"splits\" / \"scaled\",\n        horizon=HORIZON\n    )\n    print(f\"  Train samples: {container.splits['train'].n_samples:,}\")\n    print(f\"  Val samples: {container.splits['val'].n_samples:,}\")\n    print(f\"  Features: {container.n_features}\")\n    \n    # Train each model\n    for i, model_name in enumerate(MODELS_TO_TRAIN, 1):\n        print(f\"\\n{'='*60}\")\n        print(f\" [{i}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n        print(\"=\" * 60)\n        \n        start_time = time.time()\n        \n        # Configure based on model type - save results to DRIVE_PROJECT\n        model_config = {}\n        if model_name in ['lstm', 'gru', 'tcn']:\n            config = TrainerConfig(\n                model_name=model_name,\n                horizon=HORIZON,\n                sequence_length=SEQ_LEN,\n                batch_size=RECOMMENDED_BATCH_SIZE,\n                max_epochs=50,\n                early_stopping_patience=10,\n                output_dir=Path(DRIVE_PROJECT) / \"experiments\" / \"runs\",\n                device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n                mixed_precision=MIXED_PRECISION,\n            )\n        else:\n            config = TrainerConfig(\n                model_name=model_name,\n                horizon=HORIZON,\n                output_dir=Path(DRIVE_PROJECT) / \"experiments\" / \"runs\",\n            )\n        \n        # Train\n        trainer = Trainer(config)\n        results = trainer.run(container)\n        \n        elapsed = time.time() - start_time\n        \n        # Store results\n        TRAINING_RESULTS[model_name] = {\n            'metrics': results.get('evaluation_metrics', {}),\n            'time': elapsed,\n            'run_id': results.get('run_id', 'unknown'),\n        }\n        \n        # Display results\n        metrics = results.get('evaluation_metrics', {})\n        print(f\"\\n  Results:\")\n        print(f\"    Accuracy: {metrics.get('accuracy', 0):.2%}\")\n        print(f\"    Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n        print(f\"    Time: {elapsed:.1f}s\")\n        \nexcept Exception as e:\n    print(f\"\\nError during training: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\" TRAINING COMPLETE\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.3 Compare Results { display-mode: \"form\" }\n",
    "#@markdown Display comparison of all trained models.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if TRAINING_RESULTS:\n",
    "    print(\"Model Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Build comparison table\n",
    "    rows = []\n",
    "    for model, data in TRAINING_RESULTS.items():\n",
    "        metrics = data['metrics']\n",
    "        rows.append({\n",
    "            'Model': model,\n",
    "            'Accuracy': metrics.get('accuracy', 0),\n",
    "            'Macro F1': metrics.get('macro_f1', 0),\n",
    "            'Weighted F1': metrics.get('weighted_f1', 0),\n",
    "            'Time (s)': data['time'],\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(rows)\n",
    "    comparison_df = comparison_df.sort_values('Macro F1', ascending=False)\n",
    "    \n",
    "    # Display table\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Plot if multiple models\n",
    "    if len(TRAINING_RESULTS) > 1:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Accuracy bar chart\n",
    "        comparison_df_sorted = comparison_df.sort_values('Accuracy', ascending=True)\n",
    "        axes[0].barh(comparison_df_sorted['Model'], comparison_df_sorted['Accuracy'])\n",
    "        axes[0].set_xlabel('Accuracy')\n",
    "        axes[0].set_title('Model Accuracy Comparison')\n",
    "        axes[0].set_xlim(0, 1)\n",
    "        \n",
    "        # Training time bar chart\n",
    "        comparison_df_sorted = comparison_df.sort_values('Time (s)', ascending=True)\n",
    "        axes[1].barh(comparison_df_sorted['Model'], comparison_df_sorted['Time (s)'])\n",
    "        axes[1].set_xlabel('Training Time (seconds)')\n",
    "        axes[1].set_title('Training Time Comparison')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Best model\n",
    "    best_model = comparison_df.iloc[0]['Model']\n",
    "    print(f\"\\nBest model: {best_model}\")\n",
    "else:\n",
    "    print(\"No training results yet. Run Section 4.2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Phase 3: Cross-Validation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 5.1 Run Cross-Validation { display-mode: \"form\" }\n#@markdown Run purged k-fold cross-validation for robust evaluation.\n\nrun_cv = False  #@param {type: \"boolean\"}\ncv_model = \"xgboost\"  #@param [\"xgboost\", \"lightgbm\", \"catboost\", \"random_forest\"]\nn_splits = 5  #@param {type: \"slider\", min: 3, max: 10, step: 1}\n\nif run_cv:\n    print(f\"Running {n_splits}-fold cross-validation for {cv_model}...\")\n    print(\"=\" * 60)\n    \n    # Data from LOCAL_PROJECT, results to DRIVE_PROJECT\n    !python scripts/run_cv.py \\\n        --models {cv_model} \\\n        --horizons {HORIZON} \\\n        --n-splits {n_splits} \\\n        --data-dir {LOCAL_PROJECT}/data/splits/scaled \\\n        --output-dir {DRIVE_PROJECT}/results/cv\nelse:\n    print(\"Cross-validation skipped. Enable 'run_cv' to run.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Phase 4: Ensemble Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 6.1 Train Ensemble { display-mode: \"form\" }\n#@markdown Combine multiple models into an ensemble.\n\ntrain_ensemble = False  #@param {type: \"boolean\"}\nensemble_type = \"voting\"  #@param [\"voting\", \"stacking\", \"blending\"]\nbase_models = \"xgboost,lightgbm,catboost\"  #@param {type: \"string\"}\n\nif train_ensemble:\n    print(f\"Training {ensemble_type} ensemble...\")\n    print(f\"Base models: {base_models}\")\n    print(\"=\" * 60)\n    \n    try:\n        from src.models import ModelRegistry, Trainer, TrainerConfig\n        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n        \n        # Load data from LOCAL_PROJECT (GitHub clone)\n        container = TimeSeriesDataContainer.from_parquet_dir(\n            path=Path(LOCAL_PROJECT) / \"data\" / \"splits\" / \"scaled\",\n            horizon=HORIZON\n        )\n        \n        # Save results to DRIVE_PROJECT (Google Drive)\n        config = TrainerConfig(\n            model_name=ensemble_type,\n            horizon=HORIZON,\n            output_dir=Path(DRIVE_PROJECT) / \"experiments\" / \"runs\",\n            model_config={\n                \"base_model_names\": [m.strip() for m in base_models.split(',')],\n            }\n        )\n        \n        trainer = Trainer(config)\n        results = trainer.run(container)\n        \n        metrics = results.get('evaluation_metrics', {})\n        print(f\"\\nEnsemble Results:\")\n        print(f\"  Accuracy: {metrics.get('accuracy', 0):.2%}\")\n        print(f\"  Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"Ensemble training skipped. Enable 'train_ensemble' to run.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Save Results & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 7.1 Summary & Saved Artifacts { display-mode: \"form\" }\n#@markdown Display summary and location of all saved files.\n\nfrom pathlib import Path\nimport json\n\nprint(\"=\" * 60)\nprint(\" SESSION SUMMARY\")\nprint(\"=\" * 60)\n\n# Data summary (from LOCAL_PROJECT)\nprint(\"\\n DATA (from GitHub clone):\")\nsplits_dir = Path(LOCAL_PROJECT) / \"data\" / \"splits\" / \"scaled\"\nif splits_dir.exists():\n    for f in splits_dir.glob(\"*.parquet\"):\n        size_mb = f.stat().st_size / 1e6\n        print(f\"  {f.name}: {size_mb:.1f} MB\")\n\n# Training results (saved to DRIVE_PROJECT)\nprint(\"\\n TRAINED MODELS (saved to Google Drive):\")\nexperiments_dir = Path(DRIVE_PROJECT) / \"experiments\" / \"runs\"\nif experiments_dir.exists():\n    runs = list(experiments_dir.iterdir())\n    for run_dir in sorted(runs)[-5:]:  # Last 5 runs\n        if run_dir.is_dir():\n            print(f\"  {run_dir.name}\")\n\n# Next steps\nprint(\"\\n NEXT STEPS:\")\nprint(\"  1. Review model metrics in Google Drive: experiments/runs/\")\nprint(\"  2. Try different model configurations\")\nprint(\"  3. Run cross-validation for robust evaluation\")\nprint(\"  4. Train ensemble for best performance\")\nprint(\"  5. Export best model for production\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\" Data loaded from: {LOCAL_PROJECT}\")\nprint(f\" Results saved to: {DRIVE_PROJECT}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Quick Test: Load Trained Model { display-mode: \"form\" }\n",
    "#@markdown Load a trained model and make predictions.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "experiments_dir = Path(DRIVE_PROJECT) / \"experiments\" / \"runs\"\n",
    "\n",
    "if experiments_dir.exists():\n",
    "    runs = sorted([d for d in experiments_dir.iterdir() if d.is_dir()])\n",
    "    \n",
    "    if runs:\n",
    "        latest_run = runs[-1]\n",
    "        print(f\"Latest run: {latest_run.name}\")\n",
    "        \n",
    "        # List contents\n",
    "        for item in latest_run.rglob(\"*\"):\n",
    "            if item.is_file():\n",
    "                rel_path = item.relative_to(latest_run)\n",
    "                size = item.stat().st_size / 1024\n",
    "                print(f\"  {rel_path}: {size:.1f} KB\")\n",
    "    else:\n",
    "        print(\"No training runs found.\")\n",
    "else:\n",
    "    print(\"Experiments directory not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Quick Commands\n",
    "\n",
    "```bash\n",
    "# Train single model\n",
    "!python scripts/train_model.py --model xgboost --horizon 20\n",
    "\n",
    "# Train neural model\n",
    "!python scripts/train_model.py --model lstm --horizon 20 --seq-len 60\n",
    "\n",
    "# Run cross-validation\n",
    "!python scripts/run_cv.py --models xgboost,lightgbm --horizons 20 --n-splits 5\n",
    "\n",
    "# Train ensemble\n",
    "!python scripts/train_model.py --model voting --horizon 20\n",
    "\n",
    "# List all models\n",
    "!python scripts/train_model.py --list-models\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}