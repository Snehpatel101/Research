{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Complete Pipeline & Training\n",
    "\n",
    "This notebook runs the **complete ML pipeline** from raw data to trained models.\n",
    "\n",
    "## What This Notebook Does\n",
    "1. **Setup** - Mount Drive (for results), clone GitHub repo (for code & data)\n",
    "2. **Phase 1** - Data pipeline (clean -> features -> labels -> splits)\n",
    "3. **Phase 2** - Model training (single or multiple models)\n",
    "4. **Phase 3** - Cross-validation (optional)\n",
    "5. **Phase 4** - Ensemble training (optional)\n",
    "\n",
    "## Data Flow\n",
    "- **Data Source:** `/content/research/` (cloned from GitHub)\n",
    "- **Results Saved:** `/content/drive/MyDrive/research/` (Google Drive for persistence)\n",
    "\n",
    "## Quick Start\n",
    "1. Run cells in order (or use Runtime -> Run all)\n",
    "2. Data is loaded from the GitHub clone\n",
    "3. Results are saved to Google Drive for persistence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 1.1 Mount Google Drive & Clone Repository { display-mode: \"form\" }\n",
    "#@markdown Run this cell to mount your Google Drive and set up the project.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive (for saving results only)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone or pull repository\n",
    "if not Path('/content/research').exists():\n",
    "    print(\"Cloning repository...\")\n",
    "    !git clone https://github.com/Snehpatel101/research.git /content/research\n",
    "else:\n",
    "    print(\"Pulling latest changes...\")\n",
    "    !cd /content/research && git pull\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/content/research')\n",
    "\n",
    "# Create Drive directories for saving results\n",
    "for d in [\"experiments/runs\", \"results\"]:\n",
    "    Path('/content/drive/MyDrive/research', d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" PATH CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nProject directory: {os.getcwd()}\")\n",
    "print(f\"Data source: /content/research (from GitHub)\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/research (Google Drive)\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 1.2 Install Dependencies { display-mode: \"form\" }\n",
    "#@markdown Installs all required packages for the ML pipeline.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Add project to Python path\n",
    "sys.path.insert(0, '/content/research')\n",
    "\n",
    "# Install required packages\n",
    "!pip install xgboost lightgbm catboost optuna ta pywavelets scikit-learn pandas numpy -q\n",
    "\n",
    "# Verify PyTorch with CUDA\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch: {torch.__version__} with CUDA {torch.version.cuda}\")\n",
    "else:\n",
    "    print(f\"PyTorch: {torch.__version__} (CPU only)\")\n",
    "\n",
    "print(f\"\\nProject path added: /content/research\")\n",
    "print(\"Dependencies installed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 1.3 Detect Hardware & Configure { display-mode: \"form\" }\n",
    "#@markdown Detects GPU and configures optimal settings.\n",
    "\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" HARDWARE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# System info\n",
    "print(f\"\\nSystem: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "\n",
    "# GPU detection\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_NAME = None\n",
    "GPU_MEMORY = 0\n",
    "RECOMMENDED_BATCH_SIZE = 256\n",
    "MIXED_PRECISION = False\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    GPU_NAME = props.name\n",
    "    GPU_MEMORY = props.total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"\\nGPU: {GPU_NAME}\")\n",
    "    print(f\"Memory: {GPU_MEMORY:.1f} GB\")\n",
    "    print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
    "    \n",
    "    if GPU_MEMORY >= 40:  # A100\n",
    "        RECOMMENDED_BATCH_SIZE = 1024\n",
    "        MIXED_PRECISION = True\n",
    "    elif GPU_MEMORY >= 15:  # T4/V100\n",
    "        RECOMMENDED_BATCH_SIZE = 512\n",
    "        MIXED_PRECISION = True\n",
    "    else:\n",
    "        RECOMMENDED_BATCH_SIZE = 256\n",
    "        MIXED_PRECISION = props.major >= 7\n",
    "    \n",
    "    print(f\"\\nRecommended batch size: {RECOMMENDED_BATCH_SIZE}\")\n",
    "    print(f\"Mixed precision: {'Enabled' if MIXED_PRECISION else 'Disabled'}\")\n",
    "else:\n",
    "    print(\"\\nNo GPU detected - will use CPU\")\n",
    "    print(\"Tip: Runtime -> Change runtime type -> GPU\")\n",
    "\n",
    "# Verify model registry\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" AVAILABLE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from src.models import ModelRegistry\n",
    "    models = ModelRegistry.list_models()\n",
    "    for family, model_list in models.items():\n",
    "        print(f\"\\n{family.upper()}:\")\n",
    "        for m in model_list:\n",
    "            gpu_req = \"GPU\" if m in ['lstm', 'gru', 'tcn'] else \"CPU\"\n",
    "            print(f\"  - {m} ({gpu_req})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Phase 1: Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 3.1 Configure Pipeline { display-mode: \"form\" }\n",
    "#@markdown Configure the data processing pipeline.\n",
    "\n",
    "#@markdown ### Symbol Selection\n",
    "symbols = \"MES\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated symbols (e.g., \"MES,MGC\")\n",
    "\n",
    "#@markdown ### Label Horizons\n",
    "horizons = \"5,10,15,20\"  #@param {type: \"string\"}\n",
    "#@markdown Comma-separated horizons (bars ahead)\n",
    "\n",
    "#@markdown ### Train/Val/Test Split\n",
    "train_ratio = 0.70  #@param {type: \"slider\", min: 0.5, max: 0.8, step: 0.05}\n",
    "val_ratio = 0.15  #@param {type: \"slider\", min: 0.1, max: 0.25, step: 0.05}\n",
    "\n",
    "# Parse inputs\n",
    "SYMBOLS = [s.strip().upper() for s in symbols.split(',')]\n",
    "HORIZONS = [int(h.strip()) for h in horizons.split(',')]\n",
    "TRAIN_RATIO = train_ratio\n",
    "VAL_RATIO = val_ratio\n",
    "TEST_RATIO = round(1.0 - train_ratio - val_ratio, 2)\n",
    "\n",
    "print(\"Pipeline Configuration:\")\n",
    "print(f\"  Symbols: {SYMBOLS}\")\n",
    "print(f\"  Horizons: {HORIZONS}\")\n",
    "print(f\"  Train/Val/Test: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 3.2 Run Data Pipeline OR Use Existing Data { display-mode: \"form\" }\n",
    "#@markdown Choose whether to run the full pipeline or use existing processed data.\n",
    "\n",
    "data_source = \"Use existing processed data\"  #@param [\"Run full pipeline (requires raw data)\", \"Use existing processed data\"]\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# CORRECT: Data from GitHub clone (not Google Drive)\n",
    "splits_dir = Path('/content/research/data/splits/scaled')\n",
    "train_file = splits_dir / \"train_scaled.parquet\"\n",
    "\n",
    "if data_source == \"Use existing processed data\":\n",
    "    if train_file.exists():\n",
    "        import pandas as pd\n",
    "        train_df = pd.read_parquet(train_file)\n",
    "        val_df = pd.read_parquet(splits_dir / \"val_scaled.parquet\")\n",
    "        test_df = pd.read_parquet(splits_dir / \"test_scaled.parquet\")\n",
    "        \n",
    "        print(\"Found existing processed data!\")\n",
    "        print(f\"  Location: {splits_dir}\")\n",
    "        print(f\"  Train: {len(train_df):,} samples\")\n",
    "        print(f\"  Val: {len(val_df):,} samples\")\n",
    "        print(f\"  Test: {len(test_df):,} samples\")\n",
    "        print(\"\\nSkipping pipeline - proceeding to model training!\")\n",
    "    else:\n",
    "        print(\"ERROR: Processed data not found!\")\n",
    "        print(f\"  Expected: {splits_dir}/\")\n",
    "        print(\"\\nMake sure the GitHub repo contains processed data files:\")\n",
    "        print(\"  - train_scaled.parquet\")\n",
    "        print(\"  - val_scaled.parquet\")\n",
    "        print(\"  - test_scaled.parquet\")\n",
    "else:\n",
    "    raw_dir = Path('/content/research/data/raw')\n",
    "    raw_files = list(raw_dir.glob(\"*.parquet\")) + list(raw_dir.glob(\"*.csv\")) if raw_dir.exists() else []\n",
    "    \n",
    "    if not raw_files:\n",
    "        print(\"ERROR: No raw data files found!\")\n",
    "        print(f\"  Expected: {raw_dir}/MES_1m.parquet or .csv\")\n",
    "    else:\n",
    "        print(\"Running Phase 1 Data Pipeline...\")\n",
    "        print(\"=\" * 60)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            from src.phase1.pipeline_config import PipelineConfig\n",
    "            from src.pipeline.runner import PipelineRunner\n",
    "            \n",
    "            config = PipelineConfig(\n",
    "                symbols=SYMBOLS,\n",
    "                project_root=Path('/content/research'),\n",
    "                label_horizons=HORIZONS,\n",
    "                train_ratio=TRAIN_RATIO,\n",
    "                val_ratio=VAL_RATIO,\n",
    "                test_ratio=TEST_RATIO,\n",
    "            )\n",
    "            \n",
    "            runner = PipelineRunner(config)\n",
    "            success = runner.run()\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            if success:\n",
    "                print(f\"Pipeline completed in {elapsed/60:.1f} minutes!\")\n",
    "            else:\n",
    "                print(\"Pipeline failed. Check errors above.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 3.3 Verify Processed Data { display-mode: \"form\" }\n",
    "#@markdown Loads and displays the processed datasets.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# CORRECT: Data from GitHub clone (not Google Drive)\n",
    "splits_dir = Path('/content/research/data/splits/scaled')\n",
    "\n",
    "print(\"Loading processed datasets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_parquet(splits_dir / \"train_scaled.parquet\")\n",
    "    val_df = pd.read_parquet(splits_dir / \"val_scaled.parquet\")\n",
    "    test_df = pd.read_parquet(splits_dir / \"test_scaled.parquet\")\n",
    "    \n",
    "    print(f\"\\nDataset sizes:\")\n",
    "    print(f\"  Train: {len(train_df):,} samples\")\n",
    "    print(f\"  Val:   {len(val_df):,} samples\")\n",
    "    print(f\"  Test:  {len(test_df):,} samples\")\n",
    "    print(f\"  Total: {len(train_df) + len(val_df) + len(test_df):,} samples\")\n",
    "    \n",
    "    feature_cols = [c for c in train_df.columns if not c.startswith(('label_', 'sample_weight', 'quality_score', 'datetime', 'symbol'))]\n",
    "    label_cols = [c for c in train_df.columns if c.startswith('label_')]\n",
    "    \n",
    "    print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "    print(f\"Labels: {label_cols}\")\n",
    "    \n",
    "    print(f\"\\nLabel distribution (train):\")\n",
    "    for col in label_cols:\n",
    "        dist = train_df[col].value_counts().sort_index()\n",
    "        print(f\"  {col}: Long={dist.get(1, 0):,} | Neutral={dist.get(0, 0):,} | Short={dist.get(-1, 0):,}\")\n",
    "    \n",
    "    TRAIN_DF = train_df\n",
    "    VAL_DF = val_df\n",
    "    TEST_DF = test_df\n",
    "    FEATURE_COLS = feature_cols\n",
    "    \n",
    "    print(\"\\nData ready for model training!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found. Run Section 3.2 first.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#@title 3.4 Analyze Label Balance & Recommend Horizon { display-mode: \"form\" }\n#@markdown Analyzes class distribution across all horizons and recommends the best horizon for training.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"=\" * 60)\nprint(\" LABEL BALANCE ANALYSIS\")\nprint(\"=\" * 60)\n\n# Analyze each horizon\nhorizon_stats = {}\nfor h in [5, 10, 15, 20]:\n    col = f'label_h{h}'\n    if col in TRAIN_DF.columns:\n        counts = TRAIN_DF[col].value_counts().sort_index()\n        short = counts.get(-1, 0)\n        neutral = counts.get(0, 0)\n        long = counts.get(1, 0)\n        total = short + neutral + long\n        \n        # Imbalance ratio (max/min)\n        min_class = min(short, neutral, long)\n        max_class = max(short, neutral, long)\n        imbalance = max_class / min_class if min_class > 0 else float('inf')\n        \n        horizon_stats[h] = {\n            'short': short, 'neutral': neutral, 'long': long,\n            'imbalance': imbalance\n        }\n        \n        print(f\"\\nH{h}:\")\n        print(f\"  Short: {short:,} ({short/total*100:.1f}%)\")\n        print(f\"  Neutral: {neutral:,} ({neutral/total*100:.1f}%)\")\n        print(f\"  Long: {long:,} ({long/total*100:.1f}%)\")\n        print(f\"  Imbalance Ratio: {imbalance:.1f}x\")\n\n# Find best horizon (lowest imbalance)\nbest_h = min(horizon_stats, key=lambda h: horizon_stats[h]['imbalance'])\nprint(f\"\\n{'='*60}\")\nprint(f\" RECOMMENDATION: Use H{best_h} (lowest imbalance: {horizon_stats[best_h]['imbalance']:.1f}x)\")\nprint(f\"{'='*60}\")\n\n# Warning for H20\nif 20 in horizon_stats and horizon_stats[20]['imbalance'] > 100:\n    print(\"\\n[WARNING] H20 has severe class imbalance!\")\n    print(\"  The Short class is extremely underrepresented.\")\n    print(\"  This will cause poor Macro F1 scores.\")\n    print(\"  Consider using a shorter horizon or applying class balancing.\")\n\n# Plot distributions\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor i, h in enumerate([5, 10, 15, 20]):\n    if h in horizon_stats:\n        stats = horizon_stats[h]\n        colors = ['#d62728', '#7f7f7f', '#2ca02c']  # red, gray, green\n        bars = axes[i].bar(['Short', 'Neutral', 'Long'], \n                    [stats['short'], stats['neutral'], stats['long']], \n                    color=colors)\n        axes[i].set_title(f'H{h} (imbalance: {stats[\"imbalance\"]:.1f}x)')\n        axes[i].set_ylabel('Count')\n        \n        # Add count labels on bars\n        for bar, count in zip(bars, [stats['short'], stats['neutral'], stats['long']]):\n            height = bar.get_height()\n            axes[i].annotate(f'{count:,}',\n                           xy=(bar.get_x() + bar.get_width() / 2, height),\n                           xytext=(0, 3),\n                           textcoords=\"offset points\",\n                           ha='center', va='bottom', fontsize=8)\n\nplt.suptitle('Label Distribution by Horizon', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Store recommendation for downstream cells\nRECOMMENDED_HORIZON = best_h\nprint(f\"\\nRecommended horizon stored in: RECOMMENDED_HORIZON = {RECOMMENDED_HORIZON}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Phase 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "source": "#@title 4.0 Check Previous Runs & Recovery { display-mode: \"form\" }\n#@markdown Check for previous training runs and recover if needed.\n\nfrom pathlib import Path\nimport json\n\nexperiments_dir = Path('/content/drive/MyDrive/research/experiments/runs')\nresults_cache = Path('/content/drive/MyDrive/research/experiments/.training_results_cache.json')\n\nprint(\"=\" * 60)\nprint(\" PREVIOUS TRAINING RUNS\")\nprint(\"=\" * 60)\n\n# Try to recover TRAINING_RESULTS from cache\nTRAINING_RESULTS = {}\nif results_cache.exists():\n    try:\n        with open(results_cache) as f:\n            TRAINING_RESULTS = json.load(f)\n        print(f\"\\n[RECOVERED] Loaded {len(TRAINING_RESULTS)} model(s) from cache:\")\n        for model, data in TRAINING_RESULTS.items():\n            metrics = data.get('metrics', {})\n            acc = metrics.get('accuracy', 0)\n            f1 = metrics.get('macro_f1', 0)\n            print(f\"  - {model}: Accuracy={acc:.2%}, Macro F1={f1:.4f}\")\n        print(\"\\nYou can skip to section 4.3 to compare results!\")\n    except Exception as e:\n        print(f\"\\n[ERROR] Could not load cache: {e}\")\n        TRAINING_RESULTS = {}\n\n# List all training runs\nif experiments_dir.exists():\n    runs = sorted([d for d in experiments_dir.iterdir() if d.is_dir()], \n                  key=lambda x: x.name, reverse=True)\n    \n    if runs:\n        print(f\"\\nFound {len(runs)} previous run(s) on disk:\\n\")\n        for i, run in enumerate(runs[:10]):  # Show last 10\n            # Try to load metrics\n            metrics_file = run / 'metrics' / 'evaluation_metrics.json'\n            if metrics_file.exists():\n                with open(metrics_file) as f:\n                    metrics = json.load(f)\n                acc = metrics.get('accuracy', 0)\n                f1 = metrics.get('macro_f1', 0)\n                print(f\"  {i+1}. {run.name}\")\n                print(f\"      Accuracy: {acc:.2%}, Macro F1: {f1:.4f}\")\n            else:\n                print(f\"  {i+1}. {run.name} (no metrics)\")\n        \n        # Recovery option\n        print(f\"\\nTo manually load a previous model:\")\n        print(f\"  from src.models import ModelRegistry\")\n        print(f\"  model = ModelRegistry.create('xgboost')\")\n        print(f\"  model.load('{runs[0]}/checkpoints/best_model')\")\n    else:\n        print(\"\\nNo previous runs found.\")\nelse:\n    print(\"\\nExperiments directory not found.\")\n    print(\"Train your first model in section 4.2!\")\n\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 4.1 Training Mode Selection { display-mode: \"form\" }\n",
    "#@markdown Choose your training mode and models.\n",
    "\n",
    "training_mode = \"Single Model\"  #@param [\"Single Model\", \"Multi-Model (Sequential)\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Single Model Options\n",
    "single_model = \"xgboost\"  #@param [\"xgboost\", \"lightgbm\", \"catboost\", \"random_forest\", \"logistic\", \"svm\", \"lstm\", \"gru\", \"tcn\"]\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Multi-Model Options\n",
    "train_boosting = True  #@param {type: \"boolean\"}\n",
    "#@markdown XGBoost, LightGBM, CatBoost\n",
    "train_classical = False  #@param {type: \"boolean\"}\n",
    "#@markdown Random Forest, Logistic, SVM\n",
    "train_neural = False  #@param {type: \"boolean\"}\n",
    "#@markdown LSTM, GRU, TCN (requires GPU)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Training Parameters\n",
    "horizon = 20  #@param [5, 10, 15, 20]\n",
    "sequence_length = 60  #@param {type: \"slider\", min: 30, max: 120, step: 10}\n",
    "\n",
    "# Build model list\n",
    "if training_mode == \"Single Model\":\n",
    "    MODELS_TO_TRAIN = [single_model]\n",
    "else:\n",
    "    MODELS_TO_TRAIN = []\n",
    "    if train_boosting:\n",
    "        MODELS_TO_TRAIN.extend(['xgboost', 'lightgbm', 'catboost'])\n",
    "    if train_classical:\n",
    "        MODELS_TO_TRAIN.extend(['random_forest', 'logistic', 'svm'])\n",
    "    if train_neural and GPU_AVAILABLE:\n",
    "        MODELS_TO_TRAIN.extend(['lstm', 'gru', 'tcn'])\n",
    "    elif train_neural and not GPU_AVAILABLE:\n",
    "        print(\"WARNING: Neural models skipped (no GPU)\")\n",
    "\n",
    "HORIZON = horizon\n",
    "SEQ_LEN = sequence_length\n",
    "\n",
    "print(f\"Training Mode: {training_mode}\")\n",
    "print(f\"Models to train: {MODELS_TO_TRAIN}\")\n",
    "print(f\"Horizon: H{HORIZON}\")\n",
    "if any(m in ['lstm', 'gru', 'tcn'] for m in MODELS_TO_TRAIN):\n",
    "    print(f\"Sequence length: {SEQ_LEN}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title 4.2 Train Models { display-mode: \"form\" }\n#@markdown Execute model training based on your selections.\n\nimport time\nfrom pathlib import Path\nimport json\n\nprint(\"=\" * 60)\nprint(\" MODEL TRAINING\")\nprint(\"=\" * 60)\n\nTRAINING_RESULTS = {}\n\ntry:\n    from src.models import ModelRegistry, Trainer, TrainerConfig\n    from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n    \n    # CORRECT: Load data from GitHub clone (not Google Drive)\n    print(f\"\\nLoading data for horizon H{HORIZON}...\")\n    container = TimeSeriesDataContainer.from_parquet_dir(\n        path=Path('/content/research/data/splits/scaled'),\n        horizon=HORIZON\n    )\n    print(f\"  Train samples: {container.splits['train'].n_samples:,}\")\n    print(f\"  Val samples: {container.splits['val'].n_samples:,}\")\n    print(f\"  Features: {container.n_features}\")\n    \n    for i, model_name in enumerate(MODELS_TO_TRAIN, 1):\n        print(f\"\\n{'='*60}\")\n        print(f\" [{i}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n        print(\"=\" * 60)\n        \n        start_time = time.time()\n        \n        # Configure - save results to Google Drive\n        if model_name in ['lstm', 'gru', 'tcn']:\n            config = TrainerConfig(\n                model_name=model_name,\n                horizon=HORIZON,\n                sequence_length=SEQ_LEN,\n                batch_size=RECOMMENDED_BATCH_SIZE,\n                max_epochs=50,\n                early_stopping_patience=10,\n                output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n                device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n                mixed_precision=MIXED_PRECISION,\n            )\n        else:\n            config = TrainerConfig(\n                model_name=model_name,\n                horizon=HORIZON,\n                output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n            )\n        \n        trainer = Trainer(config)\n        results = trainer.run(container)\n        \n        elapsed = time.time() - start_time\n        \n        TRAINING_RESULTS[model_name] = {\n            'metrics': results.get('evaluation_metrics', {}),\n            'time': elapsed,\n            'run_id': results.get('run_id', 'unknown'),\n        }\n        \n        metrics = results.get('evaluation_metrics', {})\n        print(f\"\\n  Results:\")\n        print(f\"    Accuracy: {metrics.get('accuracy', 0):.2%}\")\n        print(f\"    Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n        print(f\"    Time: {elapsed:.1f}s\")\n        \n        # Save cache after each model (in case of kernel restart)\n        results_cache = Path('/content/drive/MyDrive/research/experiments/.training_results_cache.json')\n        with open(results_cache, 'w') as f:\n            json.dump(TRAINING_RESULTS, f, indent=2)\n        \nexcept Exception as e:\n    print(f\"\\nError during training: {e}\")\n    import traceback\n    traceback.print_exc()\n\n# Final save\nif TRAINING_RESULTS:\n    results_cache = Path('/content/drive/MyDrive/research/experiments/.training_results_cache.json')\n    with open(results_cache, 'w') as f:\n        json.dump(TRAINING_RESULTS, f, indent=2)\n    print(f\"\\nResults cached to: {results_cache}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\" TRAINING COMPLETE\")\nprint(\"=\" * 60)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#@title 4.3 Compare Results { display-mode: \"form\" }\n#@markdown Display comparison of all trained models.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nif TRAINING_RESULTS:\n    print(\"Model Comparison\")\n    print(\"=\" * 60)\n    \n    rows = []\n    for model, data in TRAINING_RESULTS.items():\n        metrics = data['metrics']\n        rows.append({\n            'Model': model,\n            'Accuracy': metrics.get('accuracy', 0),\n            'Macro F1': metrics.get('macro_f1', 0),\n            'Weighted F1': metrics.get('weighted_f1', 0),\n            'Time (s)': data['time'],\n        })\n    \n    comparison_df = pd.DataFrame(rows)\n    comparison_df = comparison_df.sort_values('Macro F1', ascending=False)\n    print(comparison_df.to_string(index=False))\n    \n    if len(TRAINING_RESULTS) > 1:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        \n        comparison_df_sorted = comparison_df.sort_values('Accuracy', ascending=True)\n        axes[0].barh(comparison_df_sorted['Model'], comparison_df_sorted['Accuracy'])\n        axes[0].set_xlabel('Accuracy')\n        axes[0].set_title('Model Accuracy Comparison')\n        axes[0].set_xlim(0, 1)\n        \n        comparison_df_sorted = comparison_df.sort_values('Time (s)', ascending=True)\n        axes[1].barh(comparison_df_sorted['Model'], comparison_df_sorted['Time (s)'])\n        axes[1].set_xlabel('Training Time (seconds)')\n        axes[1].set_title('Training Time Comparison')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    best_model = comparison_df.iloc[0]['Model']\n    print(f\"\\nBest model: {best_model}\")\nelse:\n    print(\"No training results yet. Run Section 4.2 first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title 4.4 Evaluate on Test Set { display-mode: \"form\" }\n#@markdown Evaluate the best model on the held-out test set.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\nfrom pathlib import Path\n\nif not TRAINING_RESULTS:\n    print(\"No trained models. Run section 4.2 first.\")\nelse:\n    # Find best model\n    best_model_name = max(TRAINING_RESULTS, key=lambda m: TRAINING_RESULTS[m]['metrics'].get('macro_f1', 0))\n    best_run_id = TRAINING_RESULTS[best_model_name].get('run_id', 'unknown')\n    \n    print(\"=\" * 60)\n    print(f\" TEST SET EVALUATION: {best_model_name.upper()}\")\n    print(\"=\" * 60)\n    \n    # Load test data\n    from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n    container = TimeSeriesDataContainer.from_parquet_dir(\n        path=Path('/content/research/data/splits/scaled'),\n        horizon=HORIZON\n    )\n    X_test, y_test, _ = container.get_sklearn_arrays('test')\n    \n    # Load model and predict\n    from src.models import ModelRegistry\n    model = ModelRegistry.create(best_model_name)\n    model_path = Path(f'/content/drive/MyDrive/research/experiments/runs/{best_run_id}/checkpoints/best_model')\n    model.load(model_path)\n    \n    predictions = model.predict(X_test)\n    y_pred = predictions.class_predictions\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    macro_f1 = f1_score(y_test, y_pred, average='macro')\n    \n    print(f\"\\nTest Set Results:\")\n    print(f\"  Samples: {len(y_test):,}\")\n    print(f\"  Accuracy: {accuracy:.2%}\")\n    print(f\"  Macro F1: {macro_f1:.4f}\")\n    \n    # Classification report\n    print(f\"\\nClassification Report:\")\n    class_names = ['Short', 'Neutral', 'Long']\n    print(classification_report(y_test, y_pred, target_names=class_names))\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names, ax=ax)\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    ax.set_title(f'Test Set Confusion Matrix - {best_model_name.upper()}')\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nModel loaded from: {model_path}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#@title 4.5 Trading Performance Metrics { display-mode: \"form\" }\n#@markdown Calculate trading-relevant performance metrics.\n\nimport numpy as np\nimport pandas as pd\n\nprint(\"=\" * 60)\nprint(\" TRADING PERFORMANCE ANALYSIS\")\nprint(\"=\" * 60)\n\nif 'y_pred' in dir() and 'y_test' in dir():\n    # Win rate per class\n    print(\"\\n1. WIN RATE BY PREDICTED CLASS:\")\n    for pred_class, name in [(-1, 'Short'), (0, 'Neutral'), (1, 'Long')]:\n        mask = y_pred == pred_class\n        if mask.sum() > 0:\n            correct = (y_test[mask] == pred_class).sum()\n            total = mask.sum()\n            win_rate = correct / total * 100\n            print(f\"   {name}: {correct}/{total} = {win_rate:.1f}%\")\n    \n    # Directional accuracy (ignoring neutral)\n    print(\"\\n2. DIRECTIONAL ACCURACY (Long/Short only):\")\n    directional_mask = (y_pred != 0) & (y_test != 0)\n    if directional_mask.sum() > 0:\n        # Correct direction: both positive or both negative\n        correct_direction = ((y_pred[directional_mask] > 0) == (y_test[directional_mask] > 0)).sum()\n        total_directional = directional_mask.sum()\n        dir_acc = correct_direction / total_directional * 100\n        print(f\"   Correct: {correct_direction}/{total_directional} = {dir_acc:.1f}%\")\n    \n    # Profit factor (simplified: correct predictions = wins)\n    print(\"\\n3. TRADING STATISTICS:\")\n    wins = (y_pred == y_test).sum()\n    losses = (y_pred != y_test).sum()\n    print(f\"   Wins: {wins:,}\")\n    print(f\"   Losses: {losses:,}\")\n    print(f\"   Win Rate: {wins/(wins+losses)*100:.1f}%\")\n    \n    # Consecutive losses\n    results = (y_pred == y_test).astype(int)\n    max_consecutive_losses = 0\n    current_losses = 0\n    for r in results:\n        if r == 0:\n            current_losses += 1\n            max_consecutive_losses = max(max_consecutive_losses, current_losses)\n        else:\n            current_losses = 0\n    print(f\"   Max Consecutive Losses: {max_consecutive_losses}\")\n    \n    # Expected value (simplified)\n    print(\"\\n4. EXPECTED VALUE ANALYSIS:\")\n    print(\"   (Assuming +1 unit for correct, -1 for incorrect)\")\n    ev = (wins - losses) / len(y_test)\n    print(f\"   Expected Value per Trade: {ev:+.4f} units\")\n    print(f\"   After 100 trades: {ev * 100:+.2f} units\")\n    \n    # Profit factor\n    if losses > 0:\n        profit_factor = wins / losses\n        print(f\"   Profit Factor: {profit_factor:.2f}\")\n    else:\n        print(\"   Profit Factor: Infinite (no losses)\")\n    \nelse:\n    print(\"Run section 4.4 (Test Set Evaluation) first.\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Phase 3: Cross-Validation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Phase 4: Ensemble Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "source": "#@title 6.1 Train Ensemble { display-mode: \"form\" }\n#@markdown Combine multiple models into an ensemble for improved predictions.\n\ntrain_ensemble = False  #@param {type: \"boolean\"}\nensemble_type = \"blending\"  #@param [\"voting\", \"stacking\", \"blending\"]\nbase_models = \"xgboost,lightgbm,catboost\"  #@param {type: \"string\"}\nmeta_learner = \"logistic\"  #@param [\"logistic\", \"random_forest\", \"xgboost\"]\n\nif train_ensemble:\n    import time\n    from pathlib import Path\n    \n    # Parse base models\n    base_model_list = [m.strip() for m in base_models.split(',')]\n    \n    print(\"=\" * 60)\n    print(f\" {ensemble_type.upper()} ENSEMBLE TRAINING\")\n    print(\"=\" * 60)\n    print(f\"Base models: {', '.join(base_model_list)}\")\n    print(f\"Meta-learner: {meta_learner}\")\n    print()\n    \n    # Store results for comparison\n    base_model_results = {}\n    ensemble_start_time = time.time()\n    \n    try:\n        from src.models import ModelRegistry, Trainer, TrainerConfig\n        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n        \n        # Load data from GitHub clone\n        print(\"Loading data...\")\n        container = TimeSeriesDataContainer.from_parquet_dir(\n            path=Path('/content/research/data/splits/scaled'),\n            horizon=HORIZON\n        )\n        print(f\"  Samples: train={container.splits['train'].n_samples:,}, \"\n              f\"val={container.splits['val'].n_samples:,}\")\n        print()\n        \n        # Train each base model with progress\n        print(\"-\" * 60)\n        print(\" Training Base Models\")\n        print(\"-\" * 60)\n        \n        for i, model_name in enumerate(base_model_list, 1):\n            print(f\"[{i}/{len(base_model_list)}] Training {model_name}...\", end=\" \", flush=True)\n            model_start = time.time()\n            \n            try:\n                # Configure base model\n                if model_name in ['lstm', 'gru', 'tcn']:\n                    config = TrainerConfig(\n                        model_name=model_name,\n                        horizon=HORIZON,\n                        sequence_length=SEQ_LEN if 'SEQ_LEN' in dir() else 60,\n                        batch_size=RECOMMENDED_BATCH_SIZE if 'RECOMMENDED_BATCH_SIZE' in dir() else 256,\n                        max_epochs=50,\n                        early_stopping_patience=10,\n                        output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n                        device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n                        mixed_precision=MIXED_PRECISION if 'MIXED_PRECISION' in dir() else False,\n                    )\n                else:\n                    config = TrainerConfig(\n                        model_name=model_name,\n                        horizon=HORIZON,\n                        output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n                    )\n                \n                trainer = Trainer(config)\n                results = trainer.run(container)\n                \n                model_elapsed = time.time() - model_start\n                metrics = results.get('evaluation_metrics', {})\n                \n                base_model_results[model_name] = {\n                    'accuracy': metrics.get('accuracy', 0),\n                    'macro_f1': metrics.get('macro_f1', 0),\n                    'weighted_f1': metrics.get('weighted_f1', 0),\n                    'time': model_elapsed,\n                    'run_id': results.get('run_id', 'unknown'),\n                }\n                \n                print(f\"done ({model_elapsed:.1f}s) - Acc: {metrics.get('accuracy', 0):.1%}\")\n                \n            except Exception as e:\n                print(f\"FAILED: {e}\")\n                base_model_results[model_name] = {\n                    'accuracy': 0, 'macro_f1': 0, 'weighted_f1': 0,\n                    'time': time.time() - model_start, 'error': str(e)\n                }\n        \n        print()\n        \n        # Train ensemble (meta-learner)\n        print(\"-\" * 60)\n        print(\" Training Ensemble Meta-Learner\")\n        print(\"-\" * 60)\n        print(f\"Training {ensemble_type} with {meta_learner} meta-learner...\", end=\" \", flush=True)\n        \n        meta_start = time.time()\n        \n        # Configure ensemble\n        ensemble_config = TrainerConfig(\n            model_name=ensemble_type,\n            horizon=HORIZON,\n            output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n            model_config={\n                \"base_model_names\": base_model_list,\n                \"meta_learner\": meta_learner,\n            }\n        )\n        \n        ensemble_trainer = Trainer(ensemble_config)\n        ensemble_results = ensemble_trainer.run(container)\n        \n        meta_elapsed = time.time() - meta_start\n        ensemble_metrics = ensemble_results.get('evaluation_metrics', {})\n        \n        print(f\"done ({meta_elapsed:.1f}s)\")\n        \n        # Store ensemble results\n        ensemble_accuracy = ensemble_metrics.get('accuracy', 0)\n        ensemble_macro_f1 = ensemble_metrics.get('macro_f1', 0)\n        ensemble_weighted_f1 = ensemble_metrics.get('weighted_f1', 0)\n        \n        total_elapsed = time.time() - ensemble_start_time\n        \n        # Display results\n        print()\n        print(\"=\" * 60)\n        print(\" ENSEMBLE RESULTS\")\n        print(\"=\" * 60)\n        print(f\"Accuracy:     {ensemble_accuracy:.2%}\")\n        print(f\"Macro F1:     {ensemble_macro_f1:.4f}\")\n        print(f\"Weighted F1:  {ensemble_weighted_f1:.4f}\")\n        print()\n        \n        # Comparison table\n        print(\"-\" * 60)\n        print(\" Comparison: Ensemble vs Base Models\")\n        print(\"-\" * 60)\n        print(f\"{'Model':<15} {'Accuracy':>10} {'Macro F1':>10} {'Time':>10}\")\n        print(\"-\" * 45)\n        \n        # Find best accuracy\n        all_accuracies = {k: v['accuracy'] for k, v in base_model_results.items()}\n        all_accuracies[ensemble_type.upper()] = ensemble_accuracy\n        best_model = max(all_accuracies, key=all_accuracies.get)\n        \n        # Print base model results\n        for model_name, data in base_model_results.items():\n            acc_str = f\"{data['accuracy']:.2%}\" if data['accuracy'] > 0 else \"ERROR\"\n            f1_str = f\"{data['macro_f1']:.4f}\" if data['macro_f1'] > 0 else \"N/A\"\n            time_str = f\"{data['time']:.1f}s\"\n            marker = \" <-- Best!\" if model_name == best_model else \"\"\n            print(f\"{model_name:<15} {acc_str:>10} {f1_str:>10} {time_str:>10}{marker}\")\n        \n        # Print ensemble result (highlighted)\n        marker = \" <-- Best!\" if ensemble_type.upper() == best_model else \"\"\n        print(f\"{ensemble_type.upper():<15} {ensemble_accuracy:>9.2%} {ensemble_macro_f1:>10.4f} {meta_elapsed:>9.1f}s{marker}\")\n        print(\"-\" * 45)\n        \n        # Calculate improvement\n        best_base_acc = max(v['accuracy'] for v in base_model_results.values() if v['accuracy'] > 0)\n        improvement = ensemble_accuracy - best_base_acc\n        \n        print()\n        if improvement > 0:\n            print(f\"Ensemble improvement: +{improvement:.2%} over best base model\")\n        elif improvement < 0:\n            print(f\"Ensemble underperformed by: {abs(improvement):.2%}\")\n        else:\n            print(\"Ensemble matched best base model performance\")\n        \n        print(f\"Total training time: {total_elapsed:.1f}s\")\n        print()\n        print(\"=\" * 60)\n        \n        # Store in TRAINING_RESULTS for later comparison\n        if 'TRAINING_RESULTS' not in dir():\n            TRAINING_RESULTS = {}\n        \n        # Add base models to training results\n        for model_name, data in base_model_results.items():\n            if 'error' not in data:\n                TRAINING_RESULTS[model_name] = {\n                    'metrics': {\n                        'accuracy': data['accuracy'],\n                        'macro_f1': data['macro_f1'],\n                        'weighted_f1': data['weighted_f1'],\n                    },\n                    'time': data['time'],\n                    'run_id': data.get('run_id', 'unknown'),\n                }\n        \n        # Add ensemble to training results\n        TRAINING_RESULTS[f\"{ensemble_type}_ensemble\"] = {\n            'metrics': {\n                'accuracy': ensemble_accuracy,\n                'macro_f1': ensemble_macro_f1,\n                'weighted_f1': ensemble_weighted_f1,\n            },\n            'time': total_elapsed,\n            'run_id': ensemble_results.get('run_id', 'unknown'),\n            'base_models': base_model_list,\n            'meta_learner': meta_learner,\n        }\n        \n        print(f\"Results stored in TRAINING_RESULTS['{ensemble_type}_ensemble']\")\n        \n    except ImportError as e:\n        print(f\"\\nImport Error: {e}\")\n        print(\"Make sure all required modules are available.\")\n        print(\"Try running: !pip install xgboost lightgbm catboost scikit-learn\")\n        \n    except FileNotFoundError as e:\n        print(f\"\\nData Error: {e}\")\n        print(\"Processed data not found. Run Section 3.2 first to prepare data.\")\n        \n    except Exception as e:\n        print(f\"\\nUnexpected Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        print()\n        print(\"Troubleshooting tips:\")\n        print(\"  1. Verify data exists: !ls /content/research/data/splits/scaled/\")\n        print(\"  2. Check model registry: from src.models import ModelRegistry; print(ModelRegistry.list_all())\")\n        print(\"  3. Try training base models individually first (Section 4.2)\")\n        \nelse:\n    print(\"Ensemble training skipped.\")\n    print(\"Enable 'train_ensemble' checkbox above to run.\")\n    print()\n    print(\"Available ensemble types:\")\n    print(\"  - voting: Weighted average of base model predictions\")\n    print(\"  - stacking: Train meta-learner on out-of-fold predictions\")\n    print(\"  - blending: Train meta-learner on holdout set predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Save Results & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 7.1 Summary & Saved Artifacts { display-mode: \"form\" }\n",
    "#@markdown Display summary and location of all saved files.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" SESSION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Data summary\n",
    "print(\"\\n DATA (from GitHub clone):\")\n",
    "splits_dir = Path('/content/research/data/splits/scaled')\n",
    "if splits_dir.exists():\n",
    "    for f in splits_dir.glob(\"*.parquet\"):\n",
    "        size_mb = f.stat().st_size / 1e6\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "# Training results\n",
    "print(\"\\n TRAINED MODELS (saved to Google Drive):\")\n",
    "experiments_dir = Path('/content/drive/MyDrive/research/experiments/runs')\n",
    "if experiments_dir.exists():\n",
    "    runs = list(experiments_dir.iterdir())\n",
    "    for run_dir in sorted(runs)[-5:]:\n",
    "        if run_dir.is_dir():\n",
    "            print(f\"  {run_dir.name}\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\n NEXT STEPS:\")\n",
    "print(\"  1. Review model metrics in Google Drive: experiments/runs/\")\n",
    "print(\"  2. Try different model configurations\")\n",
    "print(\"  3. Run cross-validation for robust evaluation\")\n",
    "print(\"  4. Train ensemble for best performance\")\n",
    "print(\"  5. Export best model for production\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" Data loaded from: /content/research\")\n",
    "print(\" Results saved to: /content/drive/MyDrive/research\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Quick Commands\n",
    "\n",
    "```bash\n",
    "# Train single model\n",
    "!python scripts/train_model.py --model xgboost --horizon 20\n",
    "\n",
    "# Train neural model\n",
    "!python scripts/train_model.py --model lstm --horizon 20 --seq-len 60\n",
    "\n",
    "# Run cross-validation\n",
    "!python scripts/run_cv.py --models xgboost,lightgbm --horizons 20 --n-splits 5\n",
    "\n",
    "# Train ensemble\n",
    "!python scripts/train_model.py --model voting --horizon 20\n",
    "\n",
    "# List all models\n",
    "!python scripts/train_model.py --list-models\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}