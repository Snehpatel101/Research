{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Complete Pipeline & Training",
    "",
    "This notebook runs the **complete ML pipeline** from raw data to trained models.",
    "",
    "## What This Notebook Does",
    "1. **Setup** - Mount Drive (for results), clone GitHub repo (for code & data)",
    "2. **Phase 1** - Data pipeline (clean -> features -> labels -> splits)",
    "3. **Phase 2** - Model training (single or multiple models)",
    "4. **Phase 3** - Cross-validation (optional)",
    "5. **Phase 4** - Ensemble training (optional)",
    "",
    "## Data Flow",
    "- **Data Source:** `/content/research/` (cloned from GitHub)",
    "- **Results Saved:** `/content/drive/MyDrive/research/` (Google Drive for persistence)",
    "",
    "## Memory Monitoring",
    "This notebook includes memory monitoring utilities to help diagnose kernel crashes:",
    "- **Cell 1.4**: Memory monitoring functions (`print_memory_status`, `clear_gpu_memory`)",
    "- **Cell 1.5**: Clear memory utility (run when RAM/GPU usage is high)",
    "- **Data/Training cells**: Include before/after memory checks",
    "",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 1.1 Mount Google Drive & Clone Repository { display-mode: \"form\" }\n",
    "#@markdown Run this cell to mount your Google Drive and set up the project.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive (for saving results only)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone or pull repository\n",
    "if not Path('/content/research').exists():\n",
    "    print(\"Cloning repository...\")\n",
    "    !git clone https://github.com/Snehpatel101/research.git /content/research\n",
    "else:\n",
    "    print(\"Pulling latest changes...\")\n",
    "    !cd /content/research && git pull\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/content/research')\n",
    "\n",
    "# Create Drive directories for saving results\n",
    "for d in [\"experiments/runs\", \"results\"]:\n",
    "    Path('/content/drive/MyDrive/research', d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" PATH CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nProject directory: {os.getcwd()}\")\n",
    "print(f\"Data source: /content/research (from GitHub)\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/research (Google Drive)\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title 1.2.1 Install Dependencies { display-mode: \"form\" }\n#@markdown Installs all required packages for the ML pipeline.\n\nimport sys\n\n# Add project to Python path\nsys.path.insert(0, '/content/research')\n\n# Install required packages\n!pip install xgboost lightgbm catboost optuna ta pywavelets scikit-learn pandas numpy -q\n\n# Verify PyTorch with CUDA\nimport torch\nif torch.cuda.is_available():\n    print(f\"PyTorch: {torch.__version__} with CUDA {torch.version.cuda}\")\nelse:\n    print(f\"PyTorch: {torch.__version__} (CPU only)\")\n\nprint(f\"\\nProject path added: /content/research\")\nprint(\"Dependencies installed!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#@title 1.2.2 Crash Recovery Helper { display-mode: \"form\" }\n#@markdown **Run this cell after kernel restart to recover session state.**\n#@markdown \n#@markdown This cell:\n#@markdown - Detects if this is a fresh start or recovery\n#@markdown - Loads cached results from disk (survives kernel crashes)\n#@markdown - Restores essential variables (HORIZON, MODELS_TO_TRAIN, etc.)\n#@markdown - Prints recovery status\n\nimport sys\nimport os\nfrom pathlib import Path\nimport json\n\n# Ensure project path is set\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\nprint(\"=\" * 60)\nprint(\" CRASH RECOVERY HELPER\")\nprint(\"=\" * 60)\n\n# Define cache paths\nCACHE_DIR = Path('/content/drive/MyDrive/research/experiments')\nTRAINING_CACHE = CACHE_DIR / '.training_results_cache.json'\nSESSION_CACHE = CACHE_DIR / '.session_state_cache.json'\nCHECKPOINT_CACHE = CACHE_DIR / '.training_checkpoint.json'\n\n# Detection: Is this a fresh start or recovery?\nis_recovery = False\nrecovery_sources = []\n\n# Check for training results cache\nif TRAINING_CACHE.exists():\n    is_recovery = True\n    recovery_sources.append(\"training_results\")\n    \n# Check for session state cache\nif SESSION_CACHE.exists():\n    is_recovery = True\n    recovery_sources.append(\"session_state\")\n\n# Check for training checkpoint\nif CHECKPOINT_CACHE.exists():\n    is_recovery = True\n    recovery_sources.append(\"training_checkpoint\")\n\nif is_recovery:\n    print(f\"\\n[RECOVERY MODE] Found cached data from previous session\")\n    print(f\"  Sources: {', '.join(recovery_sources)}\")\nelse:\n    print(\"\\n[FRESH START] No previous session data found\")\n\n# Initialize core variables with defaults\nHORIZON = 20\nMODELS_TO_TRAIN = ['xgboost']\nSEQ_LEN = 60\nTRAINING_RESULTS = {}\n\n# Load session state if available\nif SESSION_CACHE.exists():\n    try:\n        with open(SESSION_CACHE) as f:\n            session_state = json.load(f)\n        \n        HORIZON = session_state.get('horizon', 20)\n        MODELS_TO_TRAIN = session_state.get('models_to_train', ['xgboost'])\n        SEQ_LEN = session_state.get('seq_len', 60)\n        \n        print(f\"\\n[RESTORED] Session variables:\")\n        print(f\"  HORIZON = {HORIZON}\")\n        print(f\"  MODELS_TO_TRAIN = {MODELS_TO_TRAIN}\")\n        print(f\"  SEQ_LEN = {SEQ_LEN}\")\n    except Exception as e:\n        print(f\"\\n[WARNING] Could not load session state: {e}\")\n\n# Load training results if available\nif TRAINING_CACHE.exists():\n    try:\n        with open(TRAINING_CACHE) as f:\n            TRAINING_RESULTS = json.load(f)\n        \n        print(f\"\\n[RESTORED] Training results ({len(TRAINING_RESULTS)} models):\")\n        for model, data in TRAINING_RESULTS.items():\n            metrics = data.get('metrics', {})\n            acc = metrics.get('accuracy', 0)\n            f1 = metrics.get('macro_f1', 0)\n            print(f\"  - {model}: Acc={acc:.2%}, F1={f1:.4f}\")\n    except Exception as e:\n        print(f\"\\n[WARNING] Could not load training results: {e}\")\n\n# Check for incomplete training (checkpoint)\nif CHECKPOINT_CACHE.exists():\n    try:\n        with open(CHECKPOINT_CACHE) as f:\n            checkpoint = json.load(f)\n        \n        completed = checkpoint.get('completed_models', [])\n        pending = checkpoint.get('pending_models', [])\n        \n        if pending:\n            print(f\"\\n[CHECKPOINT] Incomplete training detected:\")\n            print(f\"  Completed: {completed}\")\n            print(f\"  Pending: {pending}\")\n            print(f\"\\n  Resume training in Section 4.2 - it will skip completed models.\")\n    except Exception as e:\n        print(f\"\\n[WARNING] Could not load checkpoint: {e}\")\n\n# Detect GPU\ntry:\n    import torch\n    GPU_AVAILABLE = torch.cuda.is_available()\n    if GPU_AVAILABLE:\n        props = torch.cuda.get_device_properties(0)\n        GPU_NAME = props.name\n        GPU_MEMORY = props.total_memory / (1024**3)\n        \n        if GPU_MEMORY >= 40:\n            RECOMMENDED_BATCH_SIZE = 1024\n            MIXED_PRECISION = True\n        elif GPU_MEMORY >= 15:\n            RECOMMENDED_BATCH_SIZE = 512\n            MIXED_PRECISION = True\n        else:\n            RECOMMENDED_BATCH_SIZE = 256\n            MIXED_PRECISION = props.major >= 7\n        \n        print(f\"\\n[GPU] {GPU_NAME} ({GPU_MEMORY:.1f} GB)\")\n    else:\n        RECOMMENDED_BATCH_SIZE = 256\n        MIXED_PRECISION = False\n        print(f\"\\n[GPU] Not available - using CPU\")\nexcept:\n    GPU_AVAILABLE = False\n    RECOMMENDED_BATCH_SIZE = 256\n    MIXED_PRECISION = False\n    print(f\"\\n[GPU] Detection failed - using CPU\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\" RECOVERY STATUS\")\nprint(\"=\" * 60)\nif TRAINING_RESULTS:\n    print(f\"\\n  Ready to continue! You can:\")\n    print(f\"    - Skip to Section 4.3 to compare existing results\")\n    print(f\"    - Run Section 4.2 to train remaining models (completed will be skipped)\")\n    print(f\"    - Run Section 4.4 to evaluate on test set\")\nelse:\n    print(f\"\\n  No previous results. Start fresh from Section 3 or 4.\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 1.3 Detect Hardware & Configure { display-mode: \"form\" }\n#@markdown Detects GPU and configures optimal settings.\n\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\nimport torch\nimport platform\n\nprint(\"=\" * 60)\nprint(\" HARDWARE DETECTION\")\nprint(\"=\" * 60)\n\n# System info\nprint(f\"\\nSystem: {platform.system()} {platform.release()}\")\nprint(f\"Python: {sys.version.split()[0]}\")\n\n# GPU detection\nGPU_AVAILABLE = torch.cuda.is_available()\nGPU_NAME = None\nGPU_MEMORY = 0\nRECOMMENDED_BATCH_SIZE = 256\nMIXED_PRECISION = False\n\nif GPU_AVAILABLE:\n    props = torch.cuda.get_device_properties(0)\n    GPU_NAME = props.name\n    GPU_MEMORY = props.total_memory / (1024**3)\n    \n    print(f\"\\nGPU: {GPU_NAME}\")\n    print(f\"Memory: {GPU_MEMORY:.1f} GB\")\n    print(f\"Compute Capability: {props.major}.{props.minor}\")\n    \n    if GPU_MEMORY >= 40:  # A100\n        RECOMMENDED_BATCH_SIZE = 1024\n        MIXED_PRECISION = True\n    elif GPU_MEMORY >= 15:  # T4/V100\n        RECOMMENDED_BATCH_SIZE = 512\n        MIXED_PRECISION = True\n    else:\n        RECOMMENDED_BATCH_SIZE = 256\n        MIXED_PRECISION = props.major >= 7\n    \n    print(f\"\\nRecommended batch size: {RECOMMENDED_BATCH_SIZE}\")\n    print(f\"Mixed precision: {'Enabled' if MIXED_PRECISION else 'Disabled'}\")\nelse:\n    print(\"\\nNo GPU detected - will use CPU\")\n    print(\"Tip: Runtime -> Change runtime type -> GPU\")\n\n# Verify model registry\nprint(\"\\n\" + \"=\" * 60)\nprint(\" AVAILABLE MODELS\")\nprint(\"=\" * 60)\n\ntry:\n    from src.models import ModelRegistry\n    models = ModelRegistry.list_models()\n    for family, model_list in models.items():\n        print(f\"\\n{family.upper()}:\")\n        for m in model_list:\n            gpu_req = \"GPU\" if m in ['lstm', 'gru', 'tcn'] else \"CPU\"\n            print(f\"  - {m} ({gpu_req})\")\nexcept Exception as e:\n    print(f\"Error loading models: {e}\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 1.4 Memory Monitoring Utilities { display-mode: \"form\" }\n#@markdown Defines functions for monitoring RAM and GPU memory usage.\n#@markdown Run this cell to enable memory diagnostics throughout the notebook.\n\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\ntry:\n    import psutil\n    import torch\n    \n    def print_memory_status(label: str = \"Current\"):\n        \"\"\"Print current memory usage for RAM and GPU.\n        \n        Args:\n            label: A descriptive label for the memory snapshot (e.g., \"Before loading\")\n        \"\"\"\n        print(f\"\\n--- Memory Status: {label} ---\")\n        \n        # RAM usage\n        try:\n            ram = psutil.virtual_memory()\n            ram_used_gb = ram.used / 1e9\n            ram_total_gb = ram.total / 1e9\n            ram_available_gb = ram.available / 1e9\n            print(f\"RAM: {ram_used_gb:.1f}GB used / {ram_total_gb:.1f}GB total ({ram.percent}%)\")\n            print(f\"     {ram_available_gb:.1f}GB available\")\n            \n            # Warning threshold\n            if ram.percent > 85:\n                print(\"     [WARNING] RAM usage above 85% - risk of OOM!\")\n        except Exception as e:\n            print(f\"RAM: Error reading - {e}\")\n        \n        # GPU usage\n        try:\n            if torch.cuda.is_available():\n                allocated = torch.cuda.memory_allocated() / 1e9\n                reserved = torch.cuda.memory_reserved() / 1e9\n                total = torch.cuda.get_device_properties(0).total_memory / 1e9\n                free = total - reserved\n                print(f\"GPU: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved / {total:.1f}GB total\")\n                print(f\"     {free:.2f}GB free\")\n                \n                # Warning threshold\n                if reserved / total > 0.85:\n                    print(\"     [WARNING] GPU memory above 85% - risk of OOM!\")\n            else:\n                print(\"GPU: Not available\")\n        except Exception as e:\n            print(f\"GPU: Error reading - {e}\")\n        \n        print(\"-\" * 40)\n\n    def clear_gpu_memory():\n        \"\"\"Clear GPU memory cache and run garbage collection.\"\"\"\n        import gc\n        \n        print(\"\\nClearing memory...\")\n        \n        # Python garbage collection\n        gc_collected = gc.collect()\n        print(f\"  Python GC: collected {gc_collected} objects\")\n        \n        # GPU cache\n        if torch.cuda.is_available():\n            before_reserved = torch.cuda.memory_reserved() / 1e9\n            torch.cuda.empty_cache()\n            after_reserved = torch.cuda.memory_reserved() / 1e9\n            freed = before_reserved - after_reserved\n            print(f\"  GPU cache: cleared {freed:.2f}GB\")\n            torch.cuda.synchronize()\n        else:\n            print(\"  GPU: Not available\")\n        \n        print(\"  Done!\")\n\n    def get_memory_dict():\n        \"\"\"Return memory stats as a dictionary for logging.\"\"\"\n        stats = {}\n        \n        try:\n            ram = psutil.virtual_memory()\n            stats['ram_used_gb'] = ram.used / 1e9\n            stats['ram_total_gb'] = ram.total / 1e9\n            stats['ram_percent'] = ram.percent\n        except:\n            pass\n        \n        try:\n            if torch.cuda.is_available():\n                stats['gpu_allocated_gb'] = torch.cuda.memory_allocated() / 1e9\n                stats['gpu_reserved_gb'] = torch.cuda.memory_reserved() / 1e9\n                stats['gpu_total_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        except:\n            pass\n        \n        return stats\n\n    # Test the functions\n    print(\"Memory monitoring utilities loaded successfully!\")\n    print_memory_status(\"Initial State\")\n    \n    MEMORY_UTILS_LOADED = True\n\nexcept ImportError as e:\n    print(f\"Warning: Could not load memory utilities - {e}\")\n    print(\"Install with: !pip install psutil\")\n    MEMORY_UTILS_LOADED = False\nexcept Exception as e:\n    print(f\"Error loading memory utilities: {e}\")\n    MEMORY_UTILS_LOADED = False",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 1.5 Clear Memory (Run When Needed) { display-mode: \"form\" }\n#@markdown **Run this cell to free up RAM and GPU memory.**\n#@markdown \n#@markdown Use this cell when:\n#@markdown - Memory usage is high (above 80%)\n#@markdown - Before training a large model\n#@markdown - After encountering OOM errors (then restart kernel)\n#@markdown - Between training different model types\n\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\nimport gc\n\nprint(\"=\" * 60)\nprint(\" CLEARING MEMORY\")\nprint(\"=\" * 60)\n\n# Show before state\ntry:\n    if 'print_memory_status' in dir():\n        print_memory_status(\"Before Cleanup\")\nexcept:\n    pass\n\n# 1. Delete large DataFrames if they exist\nlarge_vars_deleted = []\nfor var_name in ['TRAIN_DF', 'VAL_DF', 'TEST_DF', 'train_df', 'val_df', 'test_df', \n                 'container', 'trainer', 'model', 'X_train', 'X_val', 'X_test',\n                 'y_train', 'y_val', 'y_test', 'predictions']:\n    if var_name in dir():\n        try:\n            exec(f\"del {var_name}\")\n            large_vars_deleted.append(var_name)\n        except:\n            pass\n\nif large_vars_deleted:\n    print(f\"\\nDeleted variables: {', '.join(large_vars_deleted)}\")\nelse:\n    print(\"\\nNo large variables to delete\")\n\n# 2. Run Python garbage collection\ngc_collected = gc.collect()\nprint(f\"Python GC: collected {gc_collected} objects\")\n\n# 3. Clear GPU memory\ntry:\n    import torch\n    if torch.cuda.is_available():\n        before_reserved = torch.cuda.memory_reserved() / 1e9\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        after_reserved = torch.cuda.memory_reserved() / 1e9\n        freed = before_reserved - after_reserved\n        print(f\"GPU cache: cleared {freed:.2f}GB\")\n    else:\n        print(\"GPU: Not available\")\nexcept ImportError:\n    print(\"GPU: PyTorch not installed\")\nexcept Exception as e:\n    print(f\"GPU: Error - {e}\")\n\n# 4. Second pass GC (catches circular references)\ngc_collected_2 = gc.collect()\nif gc_collected_2 > 0:\n    print(f\"Python GC (2nd pass): collected {gc_collected_2} more objects\")\n\n# Show after state\ntry:\n    if 'print_memory_status' in dir():\n        print_memory_status(\"After Cleanup\")\nexcept:\n    pass\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\" CLEANUP COMPLETE\")\nprint(\"=\" * 60)\nprint(\"\\nNote: If memory is still high, consider:\")\nprint(\"  1. Runtime -> Restart runtime (preserves Drive data)\")\nprint(\"  2. After restart, run cells 1.1-1.4 to restore environment\")\nprint(\"  3. Run cell 4.0 to recover training results from cache\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 3. Phase 1: Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title 3.2 Run Data Pipeline OR Use Existing Data { display-mode: \"form\" }\n#@markdown Choose whether to run the full pipeline or use existing processed data.\n\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\ndata_source = \"Use existing processed data\"  #@param [\"Run full pipeline (requires raw data)\", \"Use existing processed data\"]\n\nfrom pathlib import Path\nimport time\nimport gc\n\n# Memory check before loading (if utilities available)\ntry:\n    if 'print_memory_status' in dir():\n        print_memory_status(\"Before Data Loading\")\nexcept:\n    pass\n\n# CORRECT: Data from GitHub clone (not Google Drive)\nsplits_dir = Path('/content/research/data/splits/scaled')\ntrain_file = splits_dir / \"train_scaled.parquet\"\n\nif data_source == \"Use existing processed data\":\n    if train_file.exists():\n        import pandas as pd\n        \n        # Load one at a time and print info to save memory\n        print(\"Found existing processed data!\")\n        print(f\"  Location: {splits_dir}\")\n        \n        # Only load metadata (row counts) without keeping full DataFrames in memory\n        for split_name in [\"train\", \"val\", \"test\"]:\n            split_file = splits_dir / f\"{split_name}_scaled.parquet\"\n            if split_file.exists():\n                # Use read_parquet with columns=[] to get row count without loading data\n                temp_df = pd.read_parquet(split_file)\n                print(f\"  {split_name.capitalize()}: {len(temp_df):,} samples\")\n                del temp_df\n        \n        # Force garbage collection\n        gc.collect()\n        \n        print(\"\\nData verified - proceeding to model training!\")\n        print(\"(DataFrames not held in memory - will be loaded by TimeSeriesDataContainer)\")\n    else:\n        print(\"ERROR: Processed data not found!\")\n        print(f\"  Expected: {splits_dir}/\")\n        print(\"\\nMake sure the GitHub repo contains processed data files:\")\n        print(\"  - train_scaled.parquet\")\n        print(\"  - val_scaled.parquet\")\n        print(\"  - test_scaled.parquet\")\nelse:\n    raw_dir = Path('/content/research/data/raw')\n    raw_files = list(raw_dir.glob(\"*.parquet\")) + list(raw_dir.glob(\"*.csv\")) if raw_dir.exists() else []\n    \n    if not raw_files:\n        print(\"ERROR: No raw data files found!\")\n        print(f\"  Expected: {raw_dir}/MES_1m.parquet or .csv\")\n    else:\n        print(\"Running Phase 1 Data Pipeline...\")\n        print(\"=\" * 60)\n        start_time = time.time()\n        \n        try:\n            from src.phase1.pipeline_config import PipelineConfig\n            from src.pipeline.runner import PipelineRunner\n            \n            config = PipelineConfig(\n                symbols=SYMBOLS,\n                project_root=Path('/content/research'),\n                label_horizons=HORIZONS,\n                train_ratio=TRAIN_RATIO,\n                val_ratio=VAL_RATIO,\n                test_ratio=TEST_RATIO,\n            )\n            \n            runner = PipelineRunner(config)\n            success = runner.run()\n            \n            # Clean up pipeline artifacts\n            del runner, config\n            gc.collect()\n            \n            elapsed = time.time() - start_time\n            print(\"\\n\" + \"=\" * 60)\n            if success:\n                print(f\"Pipeline completed in {elapsed/60:.1f} minutes!\")\n            else:\n                print(\"Pipeline failed. Check errors above.\")\n        except Exception as e:\n            print(f\"\\nError: {e}\")\n            import traceback\n            traceback.print_exc()\n\n# Memory check after loading\ntry:\n    if 'print_memory_status' in dir():\n        print_memory_status(\"After Data Loading\")\nexcept:\n    pass",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title 3.3 Verify Processed Data { display-mode: \"form\" }\n#@markdown Loads and displays dataset info WITHOUT keeping DataFrames in memory.\n\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\nimport pandas as pd\nfrom pathlib import Path\nimport gc\n\n# Memory check before loading\ntry:\n    if 'print_memory_status' in dir():\n        print_memory_status(\"Before Loading DataFrames\")\nexcept:\n    pass\n\n# CORRECT: Data from GitHub clone (not Google Drive)\nsplits_dir = Path('/content/research/data/splits/scaled')\n\nprint(\"Verifying processed datasets...\")\nprint(\"=\" * 60)\n\ntry:\n    # Load train to get column info (then delete)\n    train_df = pd.read_parquet(splits_dir / \"train_scaled.parquet\")\n    \n    # Extract column info before deleting\n    feature_cols = [c for c in train_df.columns if not c.startswith(('label_', 'sample_weight', 'quality_score', 'datetime', 'symbol'))]\n    label_cols = [c for c in train_df.columns if c.startswith('label_')]\n    train_len = len(train_df)\n    \n    # Get label distribution before deleting\n    label_distributions = {}\n    for col in label_cols:\n        label_distributions[col] = train_df[col].value_counts().sort_index().to_dict()\n    \n    # Delete train_df to free memory\n    del train_df\n    gc.collect()\n    \n    # Get row counts from val/test without holding in memory\n    val_df = pd.read_parquet(splits_dir / \"val_scaled.parquet\")\n    val_len = len(val_df)\n    del val_df\n    gc.collect()\n    \n    test_df = pd.read_parquet(splits_dir / \"test_scaled.parquet\")\n    test_len = len(test_df)\n    del test_df\n    gc.collect()\n    \n    # Print summary\n    print(f\"\\nDataset sizes:\")\n    print(f\"  Train: {train_len:,} samples\")\n    print(f\"  Val:   {val_len:,} samples\")\n    print(f\"  Test:  {test_len:,} samples\")\n    print(f\"  Total: {train_len + val_len + test_len:,} samples\")\n    \n    print(f\"\\nFeatures: {len(feature_cols)}\")\n    print(f\"Labels: {label_cols}\")\n    \n    print(f\"\\nLabel distribution (train):\")\n    for col, dist in label_distributions.items():\n        print(f\"  {col}: Long={dist.get(1, 0):,} | Neutral={dist.get(0, 0):,} | Short={dist.get(-1, 0):,}\")\n    \n    # Store metadata for downstream cells (NOT the DataFrames)\n    FEATURE_COLS = feature_cols\n    TRAIN_LEN = train_len\n    VAL_LEN = val_len\n    TEST_LEN = test_len\n    \n    print(\"\\nData verified! (DataFrames freed from memory)\")\n    print(\"TimeSeriesDataContainer will load data on-demand during training.\")\n    \n    # Memory check after cleanup\n    try:\n        if 'print_memory_status' in dir():\n            print_memory_status(\"After Cleanup\")\n    except:\n        pass\n    \nexcept FileNotFoundError:\n    print(\"Processed data not found. Run Section 3.2 first.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 4. Phase 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "source": "#@title 4.1 Training Mode Selection { display-mode: \"form\" }\n#@markdown Choose your training mode and models.\n\n# FIRST: Set defaults for variables that may not exist (handles kernel restart)\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\nimport gc\n\n# Check GPU availability (in case cell 1.3 wasn't run)\nif 'GPU_AVAILABLE' not in dir():\n    try:\n        import torch\n        GPU_AVAILABLE = torch.cuda.is_available()\n        if GPU_AVAILABLE:\n            props = torch.cuda.get_device_properties(0)\n            GPU_NAME = props.name\n            GPU_MEMORY = props.total_memory / (1024**3)\n            print(f\"[Auto-detected] GPU: {GPU_NAME} ({GPU_MEMORY:.1f} GB)\")\n        else:\n            GPU_MEMORY = 0\n            print(\"[Auto-detected] No GPU - will use CPU\")\n    except:\n        GPU_AVAILABLE = False\n        GPU_MEMORY = 0\n        print(\"[Auto-detected] PyTorch not available - will use CPU\")\n\n# GPU Memory estimation utilities\ndef estimate_gpu_memory_needed(model_name, n_samples, n_features, seq_len, batch_size):\n    \"\"\"Estimate GPU memory needed for training in GB.\n    \n    Args:\n        model_name: Name of the model (lstm, gru, tcn)\n        n_samples: Number of training samples\n        n_features: Number of input features\n        seq_len: Sequence length for temporal models\n        batch_size: Training batch size\n        \n    Returns:\n        Estimated GPU memory needed in GB\n    \"\"\"\n    if model_name in ['lstm', 'gru']:\n        # LSTM/GRU: hidden_size=128, 2 layers, bidirectional possible\n        hidden_size = 128\n        num_layers = 2\n        # Parameters: 4 * hidden_size * (input_size + hidden_size + 1) * num_layers\n        params_mb = (4 * hidden_size * (n_features + hidden_size + 1) * num_layers * 4) / 1e6\n        # Activations per batch: batch * seq_len * hidden_size * 4 (float32)\n        activations_mb = batch_size * seq_len * hidden_size * 4 / 1e6\n        # Total: params + activations * 3 (forward + backward + optimizer states)\n        return (params_mb + activations_mb * 3) / 1024  # Convert to GB\n    elif model_name == 'tcn':\n        # TCN: more parameters due to dilated convolutions\n        hidden_size = 256\n        num_channels = [64, 128, 256]\n        params_mb = 5  # Rough estimate: ~5M parameters\n        activations_mb = batch_size * seq_len * hidden_size * 4 / 1e6\n        return (params_mb + activations_mb * 3) / 1024  # Convert to GB\n    return 0\n\ndef get_safe_batch_size(model_name, n_samples, n_features, seq_len, available_gb, base_batch_size=256):\n    \"\"\"Calculate safe batch size to avoid OOM.\n    \n    Args:\n        model_name: Name of the model\n        n_samples: Number of training samples\n        n_features: Number of input features\n        seq_len: Sequence length\n        available_gb: Available GPU memory in GB\n        base_batch_size: Starting batch size to try\n        \n    Returns:\n        Safe batch size that should fit in GPU memory\n    \"\"\"\n    if model_name not in ['lstm', 'gru', 'tcn']:\n        return base_batch_size\n    \n    # Reserve 2GB for PyTorch overhead and other allocations\n    usable_gb = max(1, available_gb - 2)\n    \n    # Start with base batch size and reduce if needed\n    batch_size = base_batch_size\n    while batch_size >= 16:\n        estimated = estimate_gpu_memory_needed(model_name, n_samples, n_features, seq_len, batch_size)\n        if estimated < usable_gb:\n            return batch_size\n        batch_size = batch_size // 2\n    \n    return 16  # Minimum batch size\n\n# Store utilities for other cells\nGPU_MEMORY_UTILS = {\n    'estimate_gpu_memory_needed': estimate_gpu_memory_needed,\n    'get_safe_batch_size': get_safe_batch_size,\n}\n\ntraining_mode = \"Single Model\"  #@param [\"Single Model\", \"Multi-Model (Sequential)\"]\n\n#@markdown ---\n#@markdown ### Single Model Options\nsingle_model = \"xgboost\"  #@param [\"xgboost\", \"lightgbm\", \"catboost\", \"random_forest\", \"logistic\", \"svm\", \"lstm\", \"gru\", \"tcn\"]\n\n#@markdown ---\n#@markdown ### Multi-Model Options\ntrain_boosting = True  #@param {type: \"boolean\"}\n#@markdown XGBoost, LightGBM, CatBoost\ntrain_classical = False  #@param {type: \"boolean\"}\n#@markdown Random Forest, Logistic, SVM\ntrain_neural = False  #@param {type: \"boolean\"}\n#@markdown LSTM, GRU, TCN (requires GPU)\n\n#@markdown ---\n#@markdown ### Training Parameters\nhorizon = 20  #@param [5, 10, 15, 20]\nsequence_length = 60  #@param {type: \"slider\", min: 30, max: 120, step: 10}\n\n# Build model list\nif training_mode == \"Single Model\":\n    MODELS_TO_TRAIN = [single_model]\nelse:\n    MODELS_TO_TRAIN = []\n    if train_boosting:\n        MODELS_TO_TRAIN.extend(['xgboost', 'lightgbm', 'catboost'])\n    if train_classical:\n        MODELS_TO_TRAIN.extend(['random_forest', 'logistic', 'svm'])\n    if train_neural and GPU_AVAILABLE:\n        MODELS_TO_TRAIN.extend(['lstm', 'gru', 'tcn'])\n    elif train_neural and not GPU_AVAILABLE:\n        print(\"WARNING: Neural models skipped (no GPU)\")\n\nHORIZON = horizon\nSEQ_LEN = sequence_length\n\nprint(f\"Training Mode: {training_mode}\")\nprint(f\"Models to train: {MODELS_TO_TRAIN}\")\nprint(f\"Horizon: H{HORIZON}\")\nif any(m in ['lstm', 'gru', 'tcn'] for m in MODELS_TO_TRAIN):\n    print(f\"Sequence length: {SEQ_LEN}\")\n    if GPU_AVAILABLE:\n        print(f\"GPU Memory: {GPU_MEMORY:.1f} GB\")\n        if GPU_MEMORY < 10:\n            print(\"  [NOTE] Low GPU memory - batch size will be auto-reduced for neural models\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 4.0.1 Quick Recover (After Kernel Crash) { display-mode: \"form\" }\n#@markdown **One-click recovery after kernel crash or restart.**\n#@markdown \n#@markdown This cell restores:\n#@markdown - All training results from previous session\n#@markdown - Session variables (HORIZON, MODELS_TO_TRAIN, etc.)\n#@markdown - GPU configuration\n#@markdown - Checkpoint state for incomplete training\n\nimport sys\nimport os\nfrom pathlib import Path\nimport json\n\n# Ensure project path\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\nos.chdir('/content/research')\n\nprint(\"=\" * 60)\nprint(\" QUICK RECOVERY\")\nprint(\"=\" * 60)\n\n# Define cache paths\nCACHE_DIR = Path('/content/drive/MyDrive/research/experiments')\nTRAINING_CACHE = CACHE_DIR / '.training_results_cache.json'\nSESSION_CACHE = CACHE_DIR / '.session_state_cache.json'\nCHECKPOINT_CACHE = CACHE_DIR / '.training_checkpoint.json'\n\nrecovery_success = True\nrecovered_items = []\n\n# 1. Recover TRAINING_RESULTS\nTRAINING_RESULTS = {}\nif TRAINING_CACHE.exists():\n    try:\n        with open(TRAINING_CACHE) as f:\n            TRAINING_RESULTS = json.load(f)\n        recovered_items.append(f\"TRAINING_RESULTS ({len(TRAINING_RESULTS)} models)\")\n    except Exception as e:\n        print(f\"[WARNING] Could not load training results: {e}\")\n        recovery_success = False\nelse:\n    print(\"[INFO] No training results cache found\")\n\n# 2. Recover session state\nHORIZON = 20\nMODELS_TO_TRAIN = ['xgboost']\nSEQ_LEN = 60\n\nif SESSION_CACHE.exists():\n    try:\n        with open(SESSION_CACHE) as f:\n            session = json.load(f)\n        HORIZON = session.get('horizon', 20)\n        MODELS_TO_TRAIN = session.get('models_to_train', ['xgboost'])\n        SEQ_LEN = session.get('seq_len', 60)\n        recovered_items.append(f\"Session (H{HORIZON}, {len(MODELS_TO_TRAIN)} models)\")\n    except Exception as e:\n        print(f\"[WARNING] Could not load session state: {e}\")\n        recovery_success = False\nelse:\n    print(\"[INFO] No session cache found - using defaults\")\n\n# 3. Detect GPU\ntry:\n    import torch\n    GPU_AVAILABLE = torch.cuda.is_available()\n    if GPU_AVAILABLE:\n        props = torch.cuda.get_device_properties(0)\n        GPU_NAME = props.name\n        GPU_MEMORY = props.total_memory / (1024**3)\n        \n        if GPU_MEMORY >= 40:\n            RECOMMENDED_BATCH_SIZE = 1024\n            MIXED_PRECISION = True\n        elif GPU_MEMORY >= 15:\n            RECOMMENDED_BATCH_SIZE = 512\n            MIXED_PRECISION = True\n        else:\n            RECOMMENDED_BATCH_SIZE = 256\n            MIXED_PRECISION = props.major >= 7\n        \n        recovered_items.append(f\"GPU ({GPU_NAME})\")\n    else:\n        RECOMMENDED_BATCH_SIZE = 256\n        MIXED_PRECISION = False\n        recovered_items.append(\"CPU mode\")\nexcept Exception as e:\n    GPU_AVAILABLE = False\n    RECOMMENDED_BATCH_SIZE = 256\n    MIXED_PRECISION = False\n    print(f\"[WARNING] GPU detection failed: {e}\")\n\n# 4. Check for incomplete training\nincomplete_training = None\nif CHECKPOINT_CACHE.exists():\n    try:\n        with open(CHECKPOINT_CACHE) as f:\n            checkpoint = json.load(f)\n        pending = checkpoint.get('pending_models', [])\n        if pending:\n            incomplete_training = checkpoint\n            recovered_items.append(f\"Checkpoint ({len(pending)} pending)\")\n    except:\n        pass\n\n# Summary\nprint(f\"\\n[RECOVERED] {len(recovered_items)} items:\")\nfor item in recovered_items:\n    print(f\"  - {item}\")\n\nprint(f\"\\n\" + \"-\" * 60)\nprint(\"RESTORED VARIABLES:\")\nprint(\"-\" * 60)\nprint(f\"  HORIZON = {HORIZON}\")\nprint(f\"  MODELS_TO_TRAIN = {MODELS_TO_TRAIN}\")\nprint(f\"  SEQ_LEN = {SEQ_LEN}\")\nprint(f\"  GPU_AVAILABLE = {GPU_AVAILABLE}\")\nprint(f\"  RECOMMENDED_BATCH_SIZE = {RECOMMENDED_BATCH_SIZE}\")\nprint(f\"  MIXED_PRECISION = {MIXED_PRECISION}\")\n\nif TRAINING_RESULTS:\n    print(f\"\\n\" + \"-\" * 60)\n    print(\"TRAINING RESULTS:\")\n    print(\"-\" * 60)\n    for model, data in TRAINING_RESULTS.items():\n        if not model.endswith('_FAILED'):\n            metrics = data.get('metrics', {})\n            acc = metrics.get('accuracy', 0)\n            f1 = metrics.get('macro_f1', 0)\n            print(f\"  {model}: Accuracy={acc:.2%}, Macro F1={f1:.4f}\")\n\nif incomplete_training:\n    print(f\"\\n\" + \"-\" * 60)\n    print(\"INCOMPLETE TRAINING DETECTED:\")\n    print(\"-\" * 60)\n    print(f\"  Completed: {incomplete_training.get('completed_models', [])}\")\n    print(f\"  Pending: {incomplete_training.get('pending_models', [])}\")\n    print(f\"\\n  Run Section 4.2 to resume training!\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\" NEXT STEPS\")\nprint(\"=\" * 60)\nif TRAINING_RESULTS:\n    print(\"  - Section 4.3: Compare results\")\n    print(\"  - Section 4.4: Evaluate on test set\")\n    print(\"  - Section 4.2: Train more models (completed will be skipped)\")\nelse:\n    print(\"  - Section 4.1: Configure training\")\n    print(\"  - Section 4.2: Start training\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title 4.0.2 Clear Cache (Force Retrain) { display-mode: \"form\" }\n#@markdown **Clear cached results to retrain models from scratch.**\n#@markdown \n#@markdown Use this when you want to:\n#@markdown - Retrain models with different parameters\n#@markdown - Start fresh after changing data\n#@markdown - Clear failed training attempts\n\nfrom pathlib import Path\nimport json\n\nclear_training_results = False  #@param {type: \"boolean\"}\n#@markdown Clear all trained model results\n\nclear_session_state = False  #@param {type: \"boolean\"}\n#@markdown Clear session variables (HORIZON, MODELS_TO_TRAIN, etc.)\n\nclear_checkpoints = True  #@param {type: \"boolean\"}\n#@markdown Clear incomplete training checkpoints\n\nprint(\"=\" * 60)\nprint(\" CACHE MANAGEMENT\")\nprint(\"=\" * 60)\n\nCACHE_DIR = Path('/content/drive/MyDrive/research/experiments')\nTRAINING_CACHE = CACHE_DIR / '.training_results_cache.json'\nSESSION_CACHE = CACHE_DIR / '.session_state_cache.json'\nCHECKPOINT_CACHE = CACHE_DIR / '.training_checkpoint.json'\n\ncleared = []\n\nif clear_training_results:\n    if TRAINING_CACHE.exists():\n        # Backup before clearing\n        backup_path = CACHE_DIR / '.training_results_cache.backup.json'\n        TRAINING_CACHE.rename(backup_path)\n        cleared.append(f\"Training results (backed up to {backup_path.name})\")\n        \n        # Also clear in-memory variable\n        if 'TRAINING_RESULTS' in dir():\n            TRAINING_RESULTS = {}\n    else:\n        print(\"  No training results cache to clear\")\n\nif clear_session_state:\n    if SESSION_CACHE.exists():\n        SESSION_CACHE.unlink()\n        cleared.append(\"Session state\")\n    else:\n        print(\"  No session state cache to clear\")\n\nif clear_checkpoints:\n    if CHECKPOINT_CACHE.exists():\n        CHECKPOINT_CACHE.unlink()\n        cleared.append(\"Training checkpoint\")\n    else:\n        print(\"  No checkpoint to clear\")\n\nif cleared:\n    print(f\"\\n[CLEARED] {len(cleared)} cache(s):\")\n    for item in cleared:\n        print(f\"  - {item}\")\n    print(\"\\nYou can now run Section 4.2 to retrain all models.\")\nelse:\n    print(\"\\nNo caches were cleared.\")\n    print(\"Enable the checkboxes above to clear specific caches.\")\n\n# Show current cache status\nprint(f\"\\n\" + \"-\" * 60)\nprint(\"CURRENT CACHE STATUS:\")\nprint(\"-\" * 60)\n\nif TRAINING_CACHE.exists():\n    try:\n        with open(TRAINING_CACHE) as f:\n            results = json.load(f)\n        print(f\"  Training results: {len(results)} model(s)\")\n    except:\n        print(f\"  Training results: exists (unreadable)\")\nelse:\n    print(f\"  Training results: empty\")\n\nif SESSION_CACHE.exists():\n    print(f\"  Session state: saved\")\nelse:\n    print(f\"  Session state: empty\")\n\nif CHECKPOINT_CACHE.exists():\n    try:\n        with open(CHECKPOINT_CACHE) as f:\n            cp = json.load(f)\n        pending = len(cp.get('pending_models', []))\n        print(f\"  Checkpoint: {pending} model(s) pending\")\n    except:\n        print(f\"  Checkpoint: exists (unreadable)\")\nelse:\n    print(f\"  Checkpoint: none\")\n\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title 4.2 Train Models { display-mode: \"form\" }\n#@markdown Execute model training based on your selections.\n\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\nimport time\nimport gc\nfrom pathlib import Path\nimport json\n\nprint(\"=\" * 60)\nprint(\" MODEL TRAINING\")\nprint(\"=\" * 60)\n\n# Use defaults if not configured (handles kernel restart or skipped cells)\nHORIZON = HORIZON if 'HORIZON' in dir() else 20\nMODELS_TO_TRAIN = MODELS_TO_TRAIN if 'MODELS_TO_TRAIN' in dir() else ['xgboost']\nSEQ_LEN = SEQ_LEN if 'SEQ_LEN' in dir() else 60\nRECOMMENDED_BATCH_SIZE = RECOMMENDED_BATCH_SIZE if 'RECOMMENDED_BATCH_SIZE' in dir() else 256\nGPU_AVAILABLE = GPU_AVAILABLE if 'GPU_AVAILABLE' in dir() else False\nMIXED_PRECISION = MIXED_PRECISION if 'MIXED_PRECISION' in dir() else False\nGPU_MEMORY = GPU_MEMORY if 'GPU_MEMORY' in dir() else 0\n\n# Initialize TRAINING_RESULTS if not exists\nif 'TRAINING_RESULTS' not in dir():\n    TRAINING_RESULTS = {}\n\n# GPU memory utilities (define locally if not available from 4.1)\nif 'GPU_MEMORY_UTILS' not in dir():\n    def estimate_gpu_memory_needed(model_name, n_samples, n_features, seq_len, batch_size):\n        \"\"\"Estimate GPU memory needed for training in GB.\"\"\"\n        if model_name in ['lstm', 'gru']:\n            hidden_size = 128\n            num_layers = 2\n            params_mb = (4 * hidden_size * (n_features + hidden_size + 1) * num_layers * 4) / 1e6\n            activations_mb = batch_size * seq_len * hidden_size * 4 / 1e6\n            return (params_mb + activations_mb * 3) / 1024\n        elif model_name == 'tcn':\n            hidden_size = 256\n            params_mb = 5\n            activations_mb = batch_size * seq_len * hidden_size * 4 / 1e6\n            return (params_mb + activations_mb * 3) / 1024\n        return 0\n    \n    def get_safe_batch_size(model_name, n_samples, n_features, seq_len, available_gb, base_batch_size=256):\n        \"\"\"Calculate safe batch size to avoid OOM.\"\"\"\n        if model_name not in ['lstm', 'gru', 'tcn']:\n            return base_batch_size\n        usable_gb = max(1, available_gb - 2)\n        batch_size = base_batch_size\n        while batch_size >= 16:\n            estimated = estimate_gpu_memory_needed(model_name, n_samples, n_features, seq_len, batch_size)\n            if estimated < usable_gb:\n                return batch_size\n            batch_size = batch_size // 2\n        return 16\nelse:\n    estimate_gpu_memory_needed = GPU_MEMORY_UTILS['estimate_gpu_memory_needed']\n    get_safe_batch_size = GPU_MEMORY_UTILS['get_safe_batch_size']\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory cache and run garbage collection.\"\"\"\n    gc.collect()\n    try:\n        import torch\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n    except:\n        pass\n\nprint(f\"\\nConfiguration (using defaults if not set):\")\nprint(f\"  HORIZON: {HORIZON}\")\nprint(f\"  MODELS_TO_TRAIN: {MODELS_TO_TRAIN}\")\nprint(f\"  SEQ_LEN: {SEQ_LEN}\")\nprint(f\"  GPU_AVAILABLE: {GPU_AVAILABLE}\")\nif GPU_AVAILABLE:\n    print(f\"  GPU_MEMORY: {GPU_MEMORY:.1f} GB\")\n\n# Memory check before training\ntry:\n    if 'print_memory_status' in dir():\n        print_memory_status(\"Before Training\")\nexcept:\n    pass\n\ntry:\n    from src.models import ModelRegistry, Trainer, TrainerConfig\n    from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n    \n    # CORRECT: Load data from GitHub clone (not Google Drive)\n    print(f\"\\nLoading data for horizon H{HORIZON}...\")\n    container = TimeSeriesDataContainer.from_parquet_dir(\n        path=Path('/content/research/data/splits/scaled'),\n        horizon=HORIZON\n    )\n    n_samples = container.splits['train'].n_samples\n    n_features = container.n_features\n    print(f\"  Train samples: {n_samples:,}\")\n    print(f\"  Val samples: {container.splits['val'].n_samples:,}\")\n    print(f\"  Features: {n_features}\")\n    \n    # Memory check after loading container\n    try:\n        if 'print_memory_status' in dir():\n            print_memory_status(\"After Loading Container\")\n    except:\n        pass\n    \n    for i, model_name in enumerate(MODELS_TO_TRAIN, 1):\n        print(f\"\\n{'='*60}\")\n        print(f\" [{i}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n        print(\"=\" * 60)\n        \n        # Memory check before each model\n        try:\n            if 'print_memory_status' in dir():\n                print_memory_status(f\"Before {model_name}\")\n        except:\n            pass\n        \n        # Clear GPU memory before each model\n        clear_gpu_memory()\n        \n        start_time = time.time()\n        \n        # Configure - save results to Google Drive\n        if model_name in ['lstm', 'gru', 'tcn']:\n            # Calculate safe batch size for neural models\n            if GPU_AVAILABLE and GPU_MEMORY > 0:\n                safe_batch_size = get_safe_batch_size(\n                    model_name, n_samples, n_features, SEQ_LEN, GPU_MEMORY, RECOMMENDED_BATCH_SIZE\n                )\n                estimated_mem = estimate_gpu_memory_needed(\n                    model_name, n_samples, n_features, SEQ_LEN, safe_batch_size\n                )\n                print(f\"  GPU Memory Check:\")\n                print(f\"    Available: {GPU_MEMORY:.1f} GB\")\n                print(f\"    Estimated needed: {estimated_mem:.2f} GB\")\n                print(f\"    Batch size: {safe_batch_size} (auto-adjusted from {RECOMMENDED_BATCH_SIZE})\")\n                \n                if safe_batch_size < RECOMMENDED_BATCH_SIZE:\n                    print(f\"    [WARNING] Reduced batch size due to limited GPU memory\")\n            else:\n                safe_batch_size = min(128, RECOMMENDED_BATCH_SIZE)  # Conservative for CPU\n            \n            config = TrainerConfig(\n                model_name=model_name,\n                horizon=HORIZON,\n                sequence_length=SEQ_LEN,\n                batch_size=safe_batch_size,\n                max_epochs=50,\n                early_stopping_patience=10,\n                output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n                device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n                mixed_precision=MIXED_PRECISION,\n            )\n            \n            # Try training with OOM recovery\n            trainer = Trainer(config)\n            try:\n                results = trainer.run(container)\n            except RuntimeError as e:\n                if \"out of memory\" in str(e).lower() or \"CUDA\" in str(e):\n                    print(f\"\\n  [OOM] GPU out of memory! Retrying with smaller batch size...\")\n                    clear_gpu_memory()\n                    \n                    # Memory check after OOM\n                    try:\n                        if 'print_memory_status' in dir():\n                            print_memory_status(\"After OOM Cleanup\")\n                    except:\n                        pass\n                    \n                    # Retry with halved batch size\n                    retry_batch_size = max(16, safe_batch_size // 2)\n                    print(f\"  [OOM] Reducing batch size: {safe_batch_size} -> {retry_batch_size}\")\n                    \n                    config = TrainerConfig(\n                        model_name=model_name,\n                        horizon=HORIZON,\n                        sequence_length=SEQ_LEN,\n                        batch_size=retry_batch_size,\n                        max_epochs=50,\n                        early_stopping_patience=10,\n                        output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n                        device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n                        mixed_precision=MIXED_PRECISION,\n                    )\n                    trainer = Trainer(config)\n                    results = trainer.run(container)\n                else:\n                    raise  # Re-raise if not an OOM error\n        else:\n            config = TrainerConfig(\n                model_name=model_name,\n                horizon=HORIZON,\n                output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n            )\n            trainer = Trainer(config)\n            results = trainer.run(container)\n        \n        elapsed = time.time() - start_time\n        \n        TRAINING_RESULTS[model_name] = {\n            'metrics': results.get('evaluation_metrics', {}),\n            'time': elapsed,\n            'run_id': results.get('run_id', 'unknown'),\n        }\n        \n        metrics = results.get('evaluation_metrics', {})\n        print(f\"\\n  Results:\")\n        print(f\"    Accuracy: {metrics.get('accuracy', 0):.2%}\")\n        print(f\"    Macro F1: {metrics.get('macro_f1', 0):.4f}\")\n        print(f\"    Time: {elapsed:.1f}s\")\n        \n        # Clean up trainer to free memory\n        del trainer, config\n        \n        # Save cache after each model (in case of kernel restart)\n        results_cache = Path('/content/drive/MyDrive/research/experiments/.training_results_cache.json')\n        results_cache.parent.mkdir(parents=True, exist_ok=True)\n        with open(results_cache, 'w') as f:\n            json.dump(TRAINING_RESULTS, f, indent=2)\n        \n        # Clear GPU memory after each model\n        clear_gpu_memory()\n        \n        # Memory check after each model\n        try:\n            if 'print_memory_status' in dir():\n                print_memory_status(f\"After {model_name}\")\n        except:\n            pass\n        \nexcept Exception as e:\n    print(f\"\\nError during training: {e}\")\n    import traceback\n    traceback.print_exc()\n    \n    # Memory check on error (helps diagnose OOM)\n    try:\n        if 'print_memory_status' in dir():\n            print_memory_status(\"At Error\")\n    except:\n        pass\n    \n    # Clear GPU memory on error\n    clear_gpu_memory()\n\n# Final save\nif TRAINING_RESULTS:\n    results_cache = Path('/content/drive/MyDrive/research/experiments/.training_results_cache.json')\n    results_cache.parent.mkdir(parents=True, exist_ok=True)\n    with open(results_cache, 'w') as f:\n        json.dump(TRAINING_RESULTS, f, indent=2)\n    print(f\"\\nResults cached to: {results_cache}\")\n\n# Final GPU cleanup\nclear_gpu_memory()\n\n# Memory check after training\ntry:\n    if 'print_memory_status' in dir():\n        print_memory_status(\"After All Training\")\nexcept:\n    pass\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\" TRAINING COMPLETE\")\nprint(\"=\" * 60)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title 4.3 Compare Results { display-mode: \"form\" }\n#@markdown Display comparison of all trained models.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport json\n\n# Initialize TRAINING_RESULTS if not exists\nif 'TRAINING_RESULTS' not in dir():\n    TRAINING_RESULTS = {}\n\n# Try to load from cache if empty\nif not TRAINING_RESULTS:\n    print(\"No results in memory. Checking disk cache...\")\n    results_cache = Path('/content/drive/MyDrive/research/experiments/.training_results_cache.json')\n    if results_cache.exists():\n        try:\n            with open(results_cache) as f:\n                TRAINING_RESULTS = json.load(f)\n            print(f\"Loaded {len(TRAINING_RESULTS)} model(s) from cache.\\n\")\n        except Exception as e:\n            print(f\"Could not load cache: {e}\")\n\nif TRAINING_RESULTS:\n    print(\"Model Comparison\")\n    print(\"=\" * 60)\n    \n    rows = []\n    for model, data in TRAINING_RESULTS.items():\n        metrics = data.get('metrics', {})\n        rows.append({\n            'Model': model,\n            'Accuracy': metrics.get('accuracy', 0),\n            'Macro F1': metrics.get('macro_f1', 0),\n            'Weighted F1': metrics.get('weighted_f1', 0),\n            'Time (s)': data.get('time', 0),\n        })\n    \n    comparison_df = pd.DataFrame(rows)\n    comparison_df = comparison_df.sort_values('Macro F1', ascending=False)\n    print(comparison_df.to_string(index=False))\n    \n    if len(TRAINING_RESULTS) > 1:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        \n        comparison_df_sorted = comparison_df.sort_values('Accuracy', ascending=True)\n        axes[0].barh(comparison_df_sorted['Model'], comparison_df_sorted['Accuracy'])\n        axes[0].set_xlabel('Accuracy')\n        axes[0].set_title('Model Accuracy Comparison')\n        axes[0].set_xlim(0, 1)\n        \n        comparison_df_sorted = comparison_df.sort_values('Time (s)', ascending=True)\n        axes[1].barh(comparison_df_sorted['Model'], comparison_df_sorted['Time (s)'])\n        axes[1].set_xlabel('Training Time (seconds)')\n        axes[1].set_title('Training Time Comparison')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    best_model = comparison_df.iloc[0]['Model']\n    print(f\"\\nBest model: {best_model}\")\nelse:\n    print(\"No training results available.\")\n    print(\"\\nOptions:\")\n    print(\"  1. Run Section 4.2 to train models\")\n    print(\"  2. Run Section 4.0 to check for cached results\")\n    print(\"  3. Check Google Drive for previous runs: /content/drive/MyDrive/research/experiments/runs/\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title 4.4 Evaluate on Test Set { display-mode: \"form\" }\n#@markdown Evaluate the best model on the held-out test set.\n\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\nfrom pathlib import Path\nimport json\nimport gc\n\n# Initialize and recover TRAINING_RESULTS if needed\nif 'TRAINING_RESULTS' not in dir():\n    TRAINING_RESULTS = {}\n\n# Try to load from cache if empty\nif not TRAINING_RESULTS:\n    print(\"No results in memory. Checking disk cache...\")\n    results_cache = Path('/content/drive/MyDrive/research/experiments/.training_results_cache.json')\n    if results_cache.exists():\n        try:\n            with open(results_cache) as f:\n                TRAINING_RESULTS = json.load(f)\n            print(f\"Loaded {len(TRAINING_RESULTS)} model(s) from cache.\\n\")\n        except Exception as e:\n            print(f\"Could not load cache: {e}\")\n\n# Use default HORIZON if not set\nHORIZON = HORIZON if 'HORIZON' in dir() else 20\n\nif not TRAINING_RESULTS:\n    print(\"No trained models found.\")\n    print(\"\\nOptions:\")\n    print(\"  1. Run Section 4.2 to train models\")\n    print(\"  2. Run Section 4.0 to recover from cache\")\n    print(\"  3. Manually load a model from disk (see code below)\")\n    print(\"\\n# Manual model loading example:\")\n    print(\"# from src.models import ModelRegistry\")\n    print(\"# model = ModelRegistry.create('xgboost')\")\n    print(\"# model.load(Path('/content/drive/MyDrive/research/experiments/runs/<run_id>/checkpoints/best_model'))\")\nelse:\n    # Find best model\n    best_model_name = max(TRAINING_RESULTS, key=lambda m: TRAINING_RESULTS[m].get('metrics', {}).get('macro_f1', 0))\n    best_run_id = TRAINING_RESULTS[best_model_name].get('run_id', 'unknown')\n    \n    print(\"=\" * 60)\n    print(f\" TEST SET EVALUATION: {best_model_name.upper()}\")\n    print(\"=\" * 60)\n    print(f\"Using horizon: H{HORIZON}\")\n    print(f\"Run ID: {best_run_id}\")\n    \n    try:\n        # Load test data only (not full container)\n        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n        container = TimeSeriesDataContainer.from_parquet_dir(\n            path=Path('/content/research/data/splits/scaled'),\n            horizon=HORIZON\n        )\n        X_test, y_test, _ = container.get_sklearn_arrays('test')\n        \n        # Free container after extracting test data\n        del container\n        gc.collect()\n        \n        # Load model and predict\n        from src.models import ModelRegistry\n        model = ModelRegistry.create(best_model_name)\n        model_path = Path(f'/content/drive/MyDrive/research/experiments/runs/{best_run_id}/checkpoints/best_model')\n        \n        if not model_path.exists():\n            print(f\"\\nModel checkpoint not found at: {model_path}\")\n            print(\"Searching for alternative checkpoints...\")\n            experiments_dir = Path('/content/drive/MyDrive/research/experiments/runs')\n            if experiments_dir.exists():\n                for run_dir in sorted(experiments_dir.iterdir(), reverse=True):\n                    alt_path = run_dir / 'checkpoints' / 'best_model'\n                    if alt_path.exists():\n                        print(f\"Found: {alt_path}\")\n                        model_path = alt_path\n                        break\n        \n        model.load(model_path)\n        \n        predictions = model.predict(X_test)\n        y_pred = predictions.class_predictions\n        \n        # Free model after prediction\n        del model\n        gc.collect()\n        \n        # Calculate metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        macro_f1 = f1_score(y_test, y_pred, average='macro')\n        \n        print(f\"\\nTest Set Results:\")\n        print(f\"  Samples: {len(y_test):,}\")\n        print(f\"  Accuracy: {accuracy:.2%}\")\n        print(f\"  Macro F1: {macro_f1:.4f}\")\n        \n        # Classification report\n        print(f\"\\nClassification Report:\")\n        class_names = ['Short', 'Neutral', 'Long']\n        print(classification_report(y_test, y_pred, target_names=class_names))\n        \n        # Confusion matrix\n        cm = confusion_matrix(y_test, y_pred)\n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                    xticklabels=class_names, yticklabels=class_names, ax=ax)\n        ax.set_xlabel('Predicted')\n        ax.set_ylabel('Actual')\n        ax.set_title(f'Test Set Confusion Matrix - {best_model_name.upper()}')\n        plt.tight_layout()\n        plt.show()\n        \n        # Close figure to free memory\n        plt.close(fig)\n        gc.collect()\n        \n        print(f\"\\nModel loaded from: {model_path}\")\n        \n    except FileNotFoundError as e:\n        print(f\"\\nFile not found: {e}\")\n        print(\"Make sure the data and model files exist.\")\n    except Exception as e:\n        print(f\"\\nError during evaluation: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        # Ensure cleanup happens even on error\n        gc.collect()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 5. Phase 3: Cross-Validation (Optional)",
    "",
    "Run cross-validation for robust model evaluation. (Coming soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 6. Phase 4: Ensemble Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "source": "#@title 6.1 Train Ensemble { display-mode: \"form\" }\n#@markdown Combine multiple models into an ensemble for improved predictions.\n\nimport sys\nif '/content/research' not in sys.path:\n    sys.path.insert(0, '/content/research')\n\nimport gc\n\ntrain_ensemble = False  #@param {type: \"boolean\"}\nensemble_type = \"blending\"  #@param [\"voting\", \"stacking\", \"blending\"]\nbase_models = \"xgboost,lightgbm,catboost\"  #@param {type: \"string\"}\nmeta_learner = \"logistic\"  #@param [\"logistic\", \"random_forest\", \"xgboost\"]\n\n# GPU memory utilities (define locally if not available)\ndef estimate_gpu_memory_needed(model_name, n_samples, n_features, seq_len, batch_size):\n    \"\"\"Estimate GPU memory needed for training in GB.\"\"\"\n    if model_name in ['lstm', 'gru']:\n        hidden_size = 128\n        num_layers = 2\n        params_mb = (4 * hidden_size * (n_features + hidden_size + 1) * num_layers * 4) / 1e6\n        activations_mb = batch_size * seq_len * hidden_size * 4 / 1e6\n        return (params_mb + activations_mb * 3) / 1024\n    elif model_name == 'tcn':\n        hidden_size = 256\n        params_mb = 5\n        activations_mb = batch_size * seq_len * hidden_size * 4 / 1e6\n        return (params_mb + activations_mb * 3) / 1024\n    return 0\n\ndef get_safe_batch_size(model_name, n_samples, n_features, seq_len, available_gb, base_batch_size=256):\n    \"\"\"Calculate safe batch size to avoid OOM.\"\"\"\n    if model_name not in ['lstm', 'gru', 'tcn']:\n        return base_batch_size\n    usable_gb = max(1, available_gb - 2)\n    batch_size = base_batch_size\n    while batch_size >= 16:\n        estimated = estimate_gpu_memory_needed(model_name, n_samples, n_features, seq_len, batch_size)\n        if estimated < usable_gb:\n            return batch_size\n        batch_size = batch_size // 2\n    return 16\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory cache and run garbage collection.\"\"\"\n    gc.collect()\n    try:\n        import torch\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n    except:\n        pass\n\nif train_ensemble:\n    import time\n    from pathlib import Path\n    \n    # Parse base models\n    base_model_list = [m.strip() for m in base_models.split(',')]\n    \n    print(\"=\" * 60)\n    print(f\" {ensemble_type.upper()} ENSEMBLE TRAINING\")\n    print(\"=\" * 60)\n    print(f\"Base models: {', '.join(base_model_list)}\")\n    print(f\"Meta-learner: {meta_learner}\")\n    \n    # Check for neural models and GPU memory\n    neural_models = [m for m in base_model_list if m in ['lstm', 'gru', 'tcn']]\n    if neural_models:\n        import torch\n        if torch.cuda.is_available():\n            gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n            print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n            if gpu_mem < 10:\n                print(f\"  [NOTE] Low GPU memory - batch sizes will be auto-reduced\")\n        else:\n            print(\"  [WARNING] No GPU - neural models will use CPU (slow)\")\n    print()\n    \n    # Store results for comparison\n    base_model_results = {}\n    ensemble_start_time = time.time()\n    \n    try:\n        from src.models import ModelRegistry, Trainer, TrainerConfig\n        from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n        \n        # Get GPU info\n        import torch\n        GPU_AVAILABLE = torch.cuda.is_available()\n        GPU_MEMORY = torch.cuda.get_device_properties(0).total_memory / (1024**3) if GPU_AVAILABLE else 0\n        \n        # Load data from GitHub clone\n        print(\"Loading data...\")\n        container = TimeSeriesDataContainer.from_parquet_dir(\n            path=Path('/content/research/data/splits/scaled'),\n            horizon=HORIZON\n        )\n        n_samples = container.splits['train'].n_samples\n        n_features = container.n_features\n        print(f\"  Samples: train={n_samples:,}, \"\n              f\"val={container.splits['val'].n_samples:,}\")\n        print(f\"  Features: {n_features}\")\n        print()\n        \n        # Train each base model with progress\n        print(\"-\" * 60)\n        print(\" Training Base Models\")\n        print(\"-\" * 60)\n        \n        for i, model_name in enumerate(base_model_list, 1):\n            print(f\"[{i}/{len(base_model_list)}] Training {model_name}...\", end=\" \", flush=True)\n            \n            # Clear GPU memory before each model\n            clear_gpu_memory()\n            \n            model_start = time.time()\n            \n            try:\n                # Configure base model\n                if model_name in ['lstm', 'gru', 'tcn']:\n                    # Calculate safe batch size\n                    seq_len = SEQ_LEN if 'SEQ_LEN' in dir() else 60\n                    base_batch = RECOMMENDED_BATCH_SIZE if 'RECOMMENDED_BATCH_SIZE' in dir() else 256\n                    safe_batch_size = get_safe_batch_size(\n                        model_name, n_samples, n_features, seq_len, GPU_MEMORY, base_batch\n                    )\n                    \n                    if safe_batch_size < base_batch:\n                        print(f\"(batch={safe_batch_size}) \", end=\"\", flush=True)\n                    \n                    config = TrainerConfig(\n                        model_name=model_name,\n                        horizon=HORIZON,\n                        sequence_length=seq_len,\n                        batch_size=safe_batch_size,\n                        max_epochs=50,\n                        early_stopping_patience=10,\n                        output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n                        device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n                        mixed_precision=MIXED_PRECISION if 'MIXED_PRECISION' in dir() else False,\n                    )\n                    \n                    # Try training with OOM recovery\n                    trainer = Trainer(config)\n                    try:\n                        results = trainer.run(container)\n                    except RuntimeError as e:\n                        if \"out of memory\" in str(e).lower() or \"CUDA\" in str(e):\n                            print(f\"OOM, retrying...\", end=\" \", flush=True)\n                            clear_gpu_memory()\n                            \n                            # Retry with halved batch size\n                            retry_batch_size = max(16, safe_batch_size // 2)\n                            config = TrainerConfig(\n                                model_name=model_name,\n                                horizon=HORIZON,\n                                sequence_length=seq_len,\n                                batch_size=retry_batch_size,\n                                max_epochs=50,\n                                early_stopping_patience=10,\n                                output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n                                device=\"cuda\" if GPU_AVAILABLE else \"cpu\",\n                                mixed_precision=MIXED_PRECISION if 'MIXED_PRECISION' in dir() else False,\n                            )\n                            trainer = Trainer(config)\n                            results = trainer.run(container)\n                        else:\n                            raise\n                else:\n                    config = TrainerConfig(\n                        model_name=model_name,\n                        horizon=HORIZON,\n                        output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n                    )\n                    trainer = Trainer(config)\n                    results = trainer.run(container)\n                \n                model_elapsed = time.time() - model_start\n                metrics = results.get('evaluation_metrics', {})\n                \n                base_model_results[model_name] = {\n                    'accuracy': metrics.get('accuracy', 0),\n                    'macro_f1': metrics.get('macro_f1', 0),\n                    'weighted_f1': metrics.get('weighted_f1', 0),\n                    'time': model_elapsed,\n                    'run_id': results.get('run_id', 'unknown'),\n                }\n                \n                print(f\"done ({model_elapsed:.1f}s) - Acc: {metrics.get('accuracy', 0):.1%}\")\n                \n                # Clear GPU memory after each model\n                clear_gpu_memory()\n                \n            except Exception as e:\n                print(f\"FAILED: {e}\")\n                base_model_results[model_name] = {\n                    'accuracy': 0, 'macro_f1': 0, 'weighted_f1': 0,\n                    'time': time.time() - model_start, 'error': str(e)\n                }\n                clear_gpu_memory()\n        \n        print()\n        \n        # Clear GPU memory before ensemble training\n        clear_gpu_memory()\n        \n        # Train ensemble (meta-learner)\n        print(\"-\" * 60)\n        print(\" Training Ensemble Meta-Learner\")\n        print(\"-\" * 60)\n        print(f\"Training {ensemble_type} with {meta_learner} meta-learner...\", end=\" \", flush=True)\n        \n        meta_start = time.time()\n        \n        # Configure ensemble\n        ensemble_config = TrainerConfig(\n            model_name=ensemble_type,\n            horizon=HORIZON,\n            output_dir=Path('/content/drive/MyDrive/research/experiments/runs'),\n            model_config={\n                \"base_model_names\": base_model_list,\n                \"meta_learner\": meta_learner,\n            }\n        )\n        \n        ensemble_trainer = Trainer(ensemble_config)\n        ensemble_results = ensemble_trainer.run(container)\n        \n        meta_elapsed = time.time() - meta_start\n        ensemble_metrics = ensemble_results.get('evaluation_metrics', {})\n        \n        print(f\"done ({meta_elapsed:.1f}s)\")\n        \n        # Store ensemble results\n        ensemble_accuracy = ensemble_metrics.get('accuracy', 0)\n        ensemble_macro_f1 = ensemble_metrics.get('macro_f1', 0)\n        ensemble_weighted_f1 = ensemble_metrics.get('weighted_f1', 0)\n        \n        total_elapsed = time.time() - ensemble_start_time\n        \n        # Display results\n        print()\n        print(\"=\" * 60)\n        print(\" ENSEMBLE RESULTS\")\n        print(\"=\" * 60)\n        print(f\"Accuracy:     {ensemble_accuracy:.2%}\")\n        print(f\"Macro F1:     {ensemble_macro_f1:.4f}\")\n        print(f\"Weighted F1:  {ensemble_weighted_f1:.4f}\")\n        print()\n        \n        # Comparison table\n        print(\"-\" * 60)\n        print(\" Comparison: Ensemble vs Base Models\")\n        print(\"-\" * 60)\n        print(f\"{'Model':<15} {'Accuracy':>10} {'Macro F1':>10} {'Time':>10}\")\n        print(\"-\" * 45)\n        \n        # Find best accuracy\n        all_accuracies = {k: v['accuracy'] for k, v in base_model_results.items()}\n        all_accuracies[ensemble_type.upper()] = ensemble_accuracy\n        best_model = max(all_accuracies, key=all_accuracies.get)\n        \n        # Print base model results\n        for model_name, data in base_model_results.items():\n            acc_str = f\"{data['accuracy']:.2%}\" if data['accuracy'] > 0 else \"ERROR\"\n            f1_str = f\"{data['macro_f1']:.4f}\" if data['macro_f1'] > 0 else \"N/A\"\n            time_str = f\"{data['time']:.1f}s\"\n            marker = \" <-- Best!\" if model_name == best_model else \"\"\n            print(f\"{model_name:<15} {acc_str:>10} {f1_str:>10} {time_str:>10}{marker}\")\n        \n        # Print ensemble result (highlighted)\n        marker = \" <-- Best!\" if ensemble_type.upper() == best_model else \"\"\n        print(f\"{ensemble_type.upper():<15} {ensemble_accuracy:>9.2%} {ensemble_macro_f1:>10.4f} {meta_elapsed:>9.1f}s{marker}\")\n        print(\"-\" * 45)\n        \n        # Calculate improvement\n        best_base_acc = max(v['accuracy'] for v in base_model_results.values() if v['accuracy'] > 0)\n        improvement = ensemble_accuracy - best_base_acc\n        \n        print()\n        if improvement > 0:\n            print(f\"Ensemble improvement: +{improvement:.2%} over best base model\")\n        elif improvement < 0:\n            print(f\"Ensemble underperformed by: {abs(improvement):.2%}\")\n        else:\n            print(\"Ensemble matched best base model performance\")\n        \n        print(f\"Total training time: {total_elapsed:.1f}s\")\n        print()\n        print(\"=\" * 60)\n        \n        # Store in TRAINING_RESULTS for later comparison\n        if 'TRAINING_RESULTS' not in dir():\n            TRAINING_RESULTS = {}\n        \n        # Add base models to training results\n        for model_name, data in base_model_results.items():\n            if 'error' not in data:\n                TRAINING_RESULTS[model_name] = {\n                    'metrics': {\n                        'accuracy': data['accuracy'],\n                        'macro_f1': data['macro_f1'],\n                        'weighted_f1': data['weighted_f1'],\n                    },\n                    'time': data['time'],\n                    'run_id': data.get('run_id', 'unknown'),\n                }\n        \n        # Add ensemble to training results\n        TRAINING_RESULTS[f\"{ensemble_type}_ensemble\"] = {\n            'metrics': {\n                'accuracy': ensemble_accuracy,\n                'macro_f1': ensemble_macro_f1,\n                'weighted_f1': ensemble_weighted_f1,\n            },\n            'time': total_elapsed,\n            'run_id': ensemble_results.get('run_id', 'unknown'),\n            'base_models': base_model_list,\n            'meta_learner': meta_learner,\n        }\n        \n        print(f\"Results stored in TRAINING_RESULTS['{ensemble_type}_ensemble']\")\n        \n        # Final GPU cleanup\n        clear_gpu_memory()\n        \n    except ImportError as e:\n        print(f\"\\nImport Error: {e}\")\n        print(\"Make sure all required modules are available.\")\n        print(\"Try running: !pip install xgboost lightgbm catboost scikit-learn\")\n        clear_gpu_memory()\n        \n    except FileNotFoundError as e:\n        print(f\"\\nData Error: {e}\")\n        print(\"Processed data not found. Run Section 3.2 first to prepare data.\")\n        clear_gpu_memory()\n        \n    except Exception as e:\n        print(f\"\\nUnexpected Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        print()\n        print(\"Troubleshooting tips:\")\n        print(\"  1. Verify data exists: !ls /content/research/data/splits/scaled/\")\n        print(\"  2. Check model registry: from src.models import ModelRegistry; print(ModelRegistry.list_all())\")\n        print(\"  3. Try training base models individually first (Section 4.2)\")\n        clear_gpu_memory()\n        \nelse:\n    print(\"Ensemble training skipped.\")\n    print(\"Enable 'train_ensemble' checkbox above to run.\")\n    print()\n    print(\"Available ensemble types:\")\n    print(\"  - voting: Weighted average of base model predictions\")\n    print(\"  - stacking: Train meta-learner on out-of-fold predictions\")\n    print(\"  - blending: Train meta-learner on holdout set predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 7. Save Results & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title 7.1 Summary & Saved Artifacts { display-mode: \"form\" }\n",
    "#@markdown Display summary and location of all saved files.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" SESSION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Data summary\n",
    "print(\"\\n DATA (from GitHub clone):\")\n",
    "splits_dir = Path('/content/research/data/splits/scaled')\n",
    "if splits_dir.exists():\n",
    "    for f in splits_dir.glob(\"*.parquet\"):\n",
    "        size_mb = f.stat().st_size / 1e6\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "# Training results\n",
    "print(\"\\n TRAINED MODELS (saved to Google Drive):\")\n",
    "experiments_dir = Path('/content/drive/MyDrive/research/experiments/runs')\n",
    "if experiments_dir.exists():\n",
    "    runs = list(experiments_dir.iterdir())\n",
    "    for run_dir in sorted(runs)[-5:]:\n",
    "        if run_dir.is_dir():\n",
    "            print(f\"  {run_dir.name}\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\n NEXT STEPS:\")\n",
    "print(\"  1. Review model metrics in Google Drive: experiments/runs/\")\n",
    "print(\"  2. Try different model configurations\")\n",
    "print(\"  3. Run cross-validation for robust evaluation\")\n",
    "print(\"  4. Train ensemble for best performance\")\n",
    "print(\"  5. Export best model for production\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" Data loaded from: /content/research\")\n",
    "print(\" Results saved to: /content/drive/MyDrive/research\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Appendix: Quick Commands",
    "",
    "```bash",
    "# Train single model",
    "!python scripts/train_model.py --model xgboost --horizon 20",
    "",
    "# Train neural model  ",
    "!python scripts/train_model.py --model lstm --horizon 20 --seq-len 60",
    "",
    "# Run cross-validation",
    "!python scripts/run_cv.py --models xgboost,lightgbm --horizons 20 --n-splits 5",
    "",
    "# Train ensemble",
    "!python scripts/train_model.py --model voting --horizon 20",
    "",
    "# List all models",
    "!python scripts/train_model.py --list-models",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}