{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation Pipeline\n",
    "\n",
    "This notebook demonstrates proper time-series cross-validation with:\n",
    "\n",
    "1. **Purged K-Fold CV** - Prevents information leakage between folds\n",
    "2. **Walk-Forward Feature Selection** - Select features that generalize\n",
    "3. **Out-of-Fold Predictions** - Generate OOF predictions for stacking\n",
    "4. **Hyperparameter Tuning** - Optuna-based optimization\n",
    "\n",
    "This is critical for developing robust trading strategies that don't overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install -q xgboost lightgbm catboost scikit-learn optuna pandas numpy matplotlib tqdm pyarrow\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.utils.notebook import setup_notebook, download_sample_data, plot_confusion_matrix\n",
    "env = setup_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.models import ModelRegistry\n",
    "from src.cross_validation import (\n",
    "    PurgedKFold, PurgedKFoldConfig,\n",
    "    WalkForwardFeatureSelector,\n",
    "    OOFGenerator,\n",
    "    CrossValidationRunner,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "sample_paths = download_sample_data(output_dir=\"../data/sample\", symbols=[\"SAMPLE\"])\n",
    "df = pd.read_parquet(sample_paths[\"SAMPLE\"])\n",
    "print(f\"Loaded {len(df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(df):\n",
    "    \"\"\"Compute technical features.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    for p in [5, 10, 20, 50]:\n",
    "        df[f'return_{p}'] = df['close'].pct_change(p)\n",
    "    \n",
    "    # Moving averages\n",
    "    for p in [10, 20, 50, 100]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'close_to_sma_{p}'] = df['close'] / df[f'sma_{p}'] - 1\n",
    "    \n",
    "    # RSI\n",
    "    for period in [7, 14, 21]:\n",
    "        delta = df['close'].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()\n",
    "        rs = gain / loss.replace(0, np.inf)\n",
    "        df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # ATR\n",
    "    tr = pd.concat([\n",
    "        df['high'] - df['low'],\n",
    "        abs(df['high'] - df['close'].shift(1)),\n",
    "        abs(df['low'] - df['close'].shift(1))\n",
    "    ], axis=1).max(axis=1)\n",
    "    for period in [7, 14, 21]:\n",
    "        df[f'atr_{period}'] = tr.rolling(period).mean()\n",
    "        df[f'atr_{period}_pct'] = df[f'atr_{period}'] / df['close']\n",
    "    \n",
    "    # Bollinger\n",
    "    for period in [10, 20]:\n",
    "        sma = df['close'].rolling(period).mean()\n",
    "        std = df['close'].rolling(period).std()\n",
    "        df[f'bb_position_{period}'] = (df['close'] - (sma - 2*std)) / (4*std)\n",
    "        df[f'bb_width_{period}'] = (4*std) / sma\n",
    "    \n",
    "    # Volume\n",
    "    df['volume_sma_10'] = df['volume'].rolling(10).mean()\n",
    "    df['volume_sma_20'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma_20']\n",
    "    \n",
    "    # Volatility\n",
    "    for period in [10, 20, 50]:\n",
    "        df[f'volatility_{period}'] = df['log_return'].rolling(period).std()\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Stochastic\n",
    "    low_14 = df['low'].rolling(14).min()\n",
    "    high_14 = df['high'].rolling(14).max()\n",
    "    df['stoch_k'] = 100 * (df['close'] - low_14) / (high_14 - low_14)\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def create_labels(df, horizon=20):\n",
    "    \"\"\"Create triple-barrier labels.\"\"\"\n",
    "    df = df.copy()\n",
    "    labels = np.zeros(len(df))\n",
    "    \n",
    "    for i in range(len(df) - horizon):\n",
    "        entry = df['close'].iloc[i]\n",
    "        atr = df['atr_14'].iloc[i]\n",
    "        upper = entry + 1.5 * atr\n",
    "        lower = entry - 1.0 * atr\n",
    "        \n",
    "        for j in range(1, horizon + 1):\n",
    "            if i + j >= len(df):\n",
    "                break\n",
    "            if df['high'].iloc[i + j] >= upper:\n",
    "                labels[i] = 1\n",
    "                break\n",
    "            if df['low'].iloc[i + j] <= lower:\n",
    "                labels[i] = -1\n",
    "                break\n",
    "    \n",
    "    df['label'] = labels\n",
    "    return df\n",
    "\n",
    "# Process data\n",
    "df_features = compute_features(df)\n",
    "df_labeled = create_labels(df_features)\n",
    "\n",
    "# Define features\n",
    "exclude_cols = ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'label']\n",
    "feature_cols = [c for c in df_labeled.columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Samples: {len(df_labeled):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare arrays\n",
    "X = df_labeled[feature_cols].values\n",
    "y = df_labeled['label'].values.astype(int) + 1  # Convert to 0,1,2\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Label distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Purged K-Fold Cross-Validation\n",
    "\n",
    "Standard K-Fold CV causes information leakage in time series. Purged K-Fold:\n",
    "- Maintains chronological order\n",
    "- Adds a \"purge\" gap between train and validation to remove overlapping labels\n",
    "- Adds an \"embargo\" period after validation to prevent serial correlation leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure purged K-Fold\n",
    "cv_config = PurgedKFoldConfig(\n",
    "    n_splits=5,\n",
    "    purge_bars=60,      # Remove 60 bars around train/val boundary (3x horizon)\n",
    "    embargo_bars=288,   # Embargo 288 bars after validation (~1 day)\n",
    ")\n",
    "\n",
    "cv = PurgedKFold(cv_config)\n",
    "print(f\"CV Configuration:\")\n",
    "print(f\"  Splits: {cv_config.n_splits}\")\n",
    "print(f\"  Purge bars: {cv_config.purge_bars}\")\n",
    "print(f\"  Embargo bars: {cv_config.embargo_bars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the splits\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    # Plot train indices\n",
    "    ax.scatter(train_idx, [fold_idx] * len(train_idx), c='blue', alpha=0.3, s=1, label='Train' if fold_idx == 0 else '')\n",
    "    # Plot val indices\n",
    "    ax.scatter(val_idx, [fold_idx] * len(val_idx), c='red', alpha=0.5, s=1, label='Val' if fold_idx == 0 else '')\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1}: Train {len(train_idx):,} samples, Val {len(val_idx):,} samples\")\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Fold')\n",
    "ax.set_title('Purged K-Fold Split Visualization')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation with XGBoost\n",
    "model_config = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'early_stopping_rounds': 15,\n",
    "}\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(tqdm(cv.split(X, y), total=cv_config.n_splits, desc=\"CV Folds\")):\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model = ModelRegistry.create('xgboost', config=model_config)\n",
    "    metrics = model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Evaluate\n",
    "    predictions = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, predictions.class_predictions)\n",
    "    f1 = f1_score(y_val, predictions.class_predictions, average='macro')\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold_idx + 1,\n",
    "        'train_samples': len(train_idx),\n",
    "        'val_samples': len(val_idx),\n",
    "        'train_f1': metrics.train_f1,\n",
    "        'val_f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nMean Val F1: {results_df['val_f1'].mean():.4f} (+/- {results_df['val_f1'].std():.4f})\")\n",
    "print(f\"Mean Accuracy: {results_df['accuracy'].mean():.4f} (+/- {results_df['accuracy'].std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Walk-Forward Feature Selection\n",
    "\n",
    "Select features that generalize across time periods using walk-forward validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature selector\n",
    "feature_selector = WalkForwardFeatureSelector(\n",
    "    n_splits=3,\n",
    "    purge_bars=60,\n",
    "    embargo_bars=288,\n",
    "    method='mdi',  # Mean Decrease Impurity\n",
    "    n_top_features=20,\n",
    ")\n",
    "\n",
    "print(\"Running walk-forward feature selection...\")\n",
    "selected_features = feature_selector.fit_select(X, y, feature_names=feature_cols)\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} features:\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "importance_df = feature_selector.get_importance_scores()\n",
    "\n",
    "# Plot top features\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_n = 20\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "ax.barh(top_features['feature'][::-1], top_features['importance'][::-1], color='steelblue')\n",
    "ax.set_xlabel('Mean Importance')\n",
    "ax.set_title(f'Top {top_n} Features by Walk-Forward Importance')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CV performance with selected features vs all features\n",
    "selected_indices = [feature_cols.index(f) for f in selected_features]\n",
    "X_selected = X[:, selected_indices]\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "\n",
    "# Re-run CV with selected features\n",
    "fold_results_selected = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_selected, y)):\n",
    "    X_train, X_val = X_selected[train_idx], X_selected[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = ModelRegistry.create('xgboost', config=model_config)\n",
    "    metrics = model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    predictions = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, predictions.class_predictions, average='macro')\n",
    "    \n",
    "    fold_results_selected.append({'fold': fold_idx + 1, 'val_f1': f1})\n",
    "\n",
    "results_selected_df = pd.DataFrame(fold_results_selected)\n",
    "\n",
    "print(f\"\\nAll Features - Mean Val F1: {results_df['val_f1'].mean():.4f}\")\n",
    "print(f\"Selected Features - Mean Val F1: {results_selected_df['val_f1'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Out-of-Fold Predictions for Stacking\n",
    "\n",
    "Generate OOF predictions that can be used as meta-features for stacking ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OOF generator\n",
    "oof_generator = OOFGenerator(\n",
    "    cv=cv,\n",
    "    return_probabilities=True,\n",
    ")\n",
    "\n",
    "# Generate OOF predictions for multiple models\n",
    "model_configs = {\n",
    "    'xgboost': {'n_estimators': 100, 'max_depth': 6, 'early_stopping_rounds': 15},\n",
    "    'lightgbm': {'n_estimators': 100, 'max_depth': 6, 'early_stopping_rounds': 15},\n",
    "    'catboost': {'iterations': 100, 'depth': 6, 'early_stopping_rounds': 15},\n",
    "}\n",
    "\n",
    "oof_predictions = {}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\nGenerating OOF predictions for {model_name}...\")\n",
    "    \n",
    "    # This generates predictions for each fold's validation set\n",
    "    oof_result = oof_generator.generate(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        model_factory=lambda: ModelRegistry.create(model_name, config=config),\n",
    "    )\n",
    "    \n",
    "    oof_predictions[model_name] = oof_result\n",
    "    \n",
    "    # Calculate OOF metrics\n",
    "    mask = oof_result['predictions'] >= 0  # Valid predictions\n",
    "    oof_f1 = f1_score(y[mask], oof_result['predictions'][mask], average='macro')\n",
    "    print(f\"  OOF F1: {oof_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack OOF predictions for meta-learning\n",
    "n_samples = len(y)\n",
    "n_classes = 3\n",
    "\n",
    "# Create stacking features from OOF probabilities\n",
    "stacking_features = []\n",
    "for model_name, oof_result in oof_predictions.items():\n",
    "    if 'probabilities' in oof_result:\n",
    "        stacking_features.append(oof_result['probabilities'])\n",
    "\n",
    "X_meta = np.hstack(stacking_features)\n",
    "print(f\"Stacking features shape: {X_meta.shape}\")\n",
    "print(f\"  {len(model_configs)} models x {n_classes} classes = {len(model_configs) * n_classes} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train meta-learner on stacking features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get valid samples (those with OOF predictions)\n",
    "valid_mask = X_meta.sum(axis=1) > 0  # Rows with predictions\n",
    "X_meta_valid = X_meta[valid_mask]\n",
    "y_valid = y[valid_mask]\n",
    "\n",
    "# Split for evaluation\n",
    "n_valid = len(y_valid)\n",
    "train_end = int(n_valid * 0.8)\n",
    "\n",
    "X_meta_train = X_meta_valid[:train_end]\n",
    "y_meta_train = y_valid[:train_end]\n",
    "X_meta_test = X_meta_valid[train_end:]\n",
    "y_meta_test = y_valid[train_end:]\n",
    "\n",
    "# Train meta-learner\n",
    "meta_learner = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "meta_learner.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Evaluate\n",
    "meta_predictions = meta_learner.predict(X_meta_test)\n",
    "meta_f1 = f1_score(y_meta_test, meta_predictions, average='macro')\n",
    "meta_accuracy = accuracy_score(y_meta_test, meta_predictions)\n",
    "\n",
    "print(f\"\\nMeta-Learner Performance:\")\n",
    "print(f\"  F1 (macro): {meta_f1:.4f}\")\n",
    "print(f\"  Accuracy: {meta_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for XGBoost.\"\"\"\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'early_stopping_rounds': 20,\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    fold_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = ModelRegistry.create('xgboost', config=params)\n",
    "        model.fit(X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        predictions = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, predictions.class_predictions, average='macro')\n",
    "        fold_scores.append(f1)\n",
    "    \n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "# Run optimization\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=20,  # Increase for better results\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV F1: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Optimization history\n",
    "ax = axes[0]\n",
    "trials = [t.number for t in study.trials]\n",
    "values = [t.value for t in study.trials]\n",
    "ax.plot(trials, values, 'o-', alpha=0.7)\n",
    "ax.axhline(y=study.best_value, color='r', linestyle='--', label=f'Best: {study.best_value:.4f}')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('CV F1 Score')\n",
    "ax.set_title('Optimization History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter importance\n",
    "ax = axes[1]\n",
    "importance = optuna.importance.get_param_importances(study)\n",
    "params = list(importance.keys())\n",
    "values = list(importance.values())\n",
    "ax.barh(params[::-1], values[::-1], color='steelblue')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Hyperparameter Importance')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Model Training with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "best_params = study.best_params.copy()\n",
    "best_params['early_stopping_rounds'] = 30\n",
    "\n",
    "# Split data for final evaluation\n",
    "n = len(X)\n",
    "train_end = int(n * 0.7)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "X_train_final = X[:train_end]\n",
    "y_train_final = y[:train_end]\n",
    "X_val_final = X[train_end:val_end]\n",
    "y_val_final = y[train_end:val_end]\n",
    "X_test_final = X[val_end:]\n",
    "y_test_final = y[val_end:]\n",
    "\n",
    "print(f\"Final splits:\")\n",
    "print(f\"  Train: {len(X_train_final):,}\")\n",
    "print(f\"  Val: {len(X_val_final):,}\")\n",
    "print(f\"  Test: {len(X_test_final):,}\")\n",
    "\n",
    "# Train\n",
    "final_model = ModelRegistry.create('xgboost', config=best_params)\n",
    "final_metrics = final_model.fit(X_train_final, y_train_final, X_val_final, y_val_final)\n",
    "\n",
    "# Evaluate on test set\n",
    "final_predictions = final_model.predict(X_test_final)\n",
    "final_f1 = f1_score(y_test_final, final_predictions.class_predictions, average='macro')\n",
    "final_accuracy = accuracy_score(y_test_final, final_predictions.class_predictions)\n",
    "\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"  Test F1 (macro): {final_f1:.4f}\")\n",
    "print(f\"  Test Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for final model\n",
    "plot_confusion_matrix(\n",
    "    y_test_final, final_predictions.class_predictions,\n",
    "    labels=['Short', 'Neutral', 'Long'],\n",
    "    title='Final Model - Test Set Confusion Matrix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Purged K-Fold CV** - Proper time-series cross-validation that prevents information leakage\n",
    "2. **Walk-Forward Feature Selection** - Selecting features that generalize across time\n",
    "3. **OOF Predictions** - Generating predictions for stacking ensembles\n",
    "4. **Hyperparameter Tuning** - Using Optuna for automated optimization\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Always use purged CV for time series to prevent lookahead bias\n",
    "- Feature selection should be done within the CV loop\n",
    "- OOF predictions enable powerful stacking ensembles\n",
    "- Hyperparameter tuning should use CV, not a single train/val split\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply these techniques to your real trading data\n",
    "- Build stacking ensembles using the OOF predictions\n",
    "- Run backtests to validate real-world performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
