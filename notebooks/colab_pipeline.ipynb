{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - OHLCV Pipeline\n",
    "\n",
    "This notebook runs the ensemble price prediction pipeline on Google Colab.\n",
    "\n",
    "## Prerequisites\n",
    "1. Upload your raw data files (`MES_1m.parquet`, `MGC_1m.parquet`) to Google Drive\n",
    "2. Create a folder structure: `My Drive/research/data/raw/`\n",
    "\n",
    "## Pipeline Stages\n",
    "1. **Data Ingestion** - Load and validate raw OHLCV data\n",
    "2. **Data Cleaning** - Resample 1min → 5min, handle gaps\n",
    "3. **Feature Engineering** - Generate 150+ technical indicators\n",
    "4. **Labeling** - Triple-barrier label generation\n",
    "5. **Optimization** - Optuna parameter tuning\n",
    "6. **Splits** - Train/Val/Test with purge/embargo\n",
    "7. **Scaling** - Train-only feature scaling\n",
    "8. **Validation** - Data quality checks\n",
    "9. **Reporting** - Generate summary report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set project root\n",
    "import os\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/research'\n",
    "os.environ['PROJECT_ROOT'] = PROJECT_ROOT\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already present)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_path = Path('/content/research')\n",
    "if not repo_path.exists():\n",
    "    !git clone https://github.com/Snehpatel101/research.git /content/research\n",
    "else:\n",
    "    print(\"Repository already cloned\")\n",
    "    # Pull latest changes\n",
    "    !cd /content/research && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package\n",
    "!pip install -e /content/research --quiet\n",
    "\n",
    "# Verify installation\n",
    "import src\n",
    "print(f\"Package version: {src.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create required directories\n",
    "from pathlib import Path\n",
    "\n",
    "dirs = [\n",
    "    f\"{PROJECT_ROOT}/data/raw\",\n",
    "    f\"{PROJECT_ROOT}/data/clean\",\n",
    "    f\"{PROJECT_ROOT}/data/features\",\n",
    "    f\"{PROJECT_ROOT}/data/labels\",\n",
    "    f\"{PROJECT_ROOT}/data/final\",\n",
    "    f\"{PROJECT_ROOT}/data/splits\",\n",
    "    f\"{PROJECT_ROOT}/runs\",\n",
    "    f\"{PROJECT_ROOT}/results\",\n",
    "    f\"{PROJECT_ROOT}/config\",\n",
    "]\n",
    "\n",
    "for d in dirs:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for raw data files\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "raw_dir = Path(f\"{PROJECT_ROOT}/data/raw\")\n",
    "\n",
    "symbols = ['MES', 'MGC']\n",
    "missing = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    parquet_path = raw_dir / f\"{symbol}_1m.parquet\"\n",
    "    csv_path = raw_dir / f\"{symbol}_1m.csv\"\n",
    "    \n",
    "    if parquet_path.exists():\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        print(f\"{symbol}: {len(df):,} rows ({parquet_path.stat().st_size / 1e6:.1f} MB)\")\n",
    "    elif csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"{symbol}: {len(df):,} rows (CSV)\")\n",
    "    else:\n",
    "        missing.append(symbol)\n",
    "        print(f\"{symbol}: NOT FOUND\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n*** Upload missing files: {missing} ***\")\n",
    "    print(f\"Expected location: {raw_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.phase1.pipeline_config import PipelineConfig, create_default_config\n",
    "from pathlib import Path\n",
    "\n",
    "# Create configuration\n",
    "config = create_default_config(\n",
    "    symbols=['MES', 'MGC'],\n",
    "    project_root=Path(PROJECT_ROOT),\n",
    "    start_date=None,  # Use all available data\n",
    "    end_date=None,\n",
    "    target_timeframe='5min',\n",
    "    label_horizons=[5, 10, 15, 20],\n",
    "    train_ratio=0.70,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    ")\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"Pipeline Configuration:\")\n",
    "print(f\"  Run ID: {config.run_id}\")\n",
    "print(f\"  Symbols: {config.symbols}\")\n",
    "print(f\"  Timeframe: {config.target_timeframe}\")\n",
    "print(f\"  Horizons: {config.label_horizons}\")\n",
    "print(f\"  Train/Val/Test: {config.train_ratio}/{config.val_ratio}/{config.test_ratio}\")\n",
    "print(f\"  Project Root: {config.project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.runner import PipelineRunner\n",
    "\n",
    "# Create and run pipeline\n",
    "runner = PipelineRunner(config)\n",
    "\n",
    "print(\"Starting pipeline...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "success = runner.run()\n",
    "\n",
    "print(\"=\"*50)\n",
    "if success:\n",
    "    print(\"Pipeline completed successfully!\")\n",
    "else:\n",
    "    print(\"Pipeline failed. Check logs above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output files\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def show_directory_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"{prefix}{path.name}/ (not found)\")\n",
    "        return\n",
    "    \n",
    "    items = sorted(path.iterdir())\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        connector = \"└── \" if is_last else \"├── \"\n",
    "        \n",
    "        if item.is_file():\n",
    "            size = item.stat().st_size\n",
    "            if size > 1e9:\n",
    "                size_str = f\"{size/1e9:.1f}GB\"\n",
    "            elif size > 1e6:\n",
    "                size_str = f\"{size/1e6:.1f}MB\"\n",
    "            elif size > 1e3:\n",
    "                size_str = f\"{size/1e3:.1f}KB\"\n",
    "            else:\n",
    "                size_str = f\"{size}B\"\n",
    "            print(f\"{prefix}{connector}{item.name} ({size_str})\")\n",
    "        else:\n",
    "            print(f\"{prefix}{connector}{item.name}/\")\n",
    "            new_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "            show_directory_tree(item, new_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "print(\"Data Directory:\")\n",
    "show_directory_tree(f\"{PROJECT_ROOT}/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display final labeled data sample\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "final_dir = Path(f\"{PROJECT_ROOT}/data/final\")\n",
    "files = list(final_dir.glob(\"*_labeled.parquet\"))\n",
    "\n",
    "if files:\n",
    "    df = pd.read_parquet(files[0])\n",
    "    print(f\"Loaded: {files[0].name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "    print(df.columns.tolist()[:20], \"...\")\n",
    "    print(f\"\\nSample:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No final labeled data found. Run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display label distribution\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'df' in dir() and df is not None:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    for i, h in enumerate([5, 10, 15, 20]):\n",
    "        col = f'label_h{h}'\n",
    "        if col in df.columns:\n",
    "            counts = df[col].value_counts().sort_index()\n",
    "            colors = ['red', 'gray', 'green']\n",
    "            axes[i].bar(counts.index, counts.values, color=colors)\n",
    "            axes[i].set_title(f'H{h} Labels')\n",
    "            axes[i].set_xlabel('Label')\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].set_xticks([-1, 0, 1])\n",
    "            axes[i].set_xticklabels(['Short', 'Neutral', 'Long'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Individual Stages (Optional)\n",
    "\n",
    "If you want to run specific stages instead of the full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run only data ingestion\n",
    "from src.phase1.stages import DataIngestor\n",
    "from pathlib import Path\n",
    "\n",
    "# ingestor = DataIngestor(\n",
    "#     raw_data_dir=Path(f\"{PROJECT_ROOT}/data/raw\"),\n",
    "#     output_dir=Path(f\"{PROJECT_ROOT}/data/raw/validated\")\n",
    "# )\n",
    "# \n",
    "# df, metadata = ingestor.ingest_file(\n",
    "#     file_path=Path(f\"{PROJECT_ROOT}/data/raw/MES_1m.parquet\"),\n",
    "#     symbol='MES',\n",
    "#     validate=True\n",
    "# )\n",
    "# print(f\"Ingested {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Load pre-computed train/val/test splits with memory management\nfrom pathlib import Path\nimport pandas as pd\nimport gc\n\nsplits_dir = Path(f\"{PROJECT_ROOT}/data/splits/scaled\")\n\nif splits_dir.exists():\n    # Load only what you need - for training, typically train + val\n    train_df = pd.read_parquet(splits_dir / \"train_scaled.parquet\")\n    val_df = pd.read_parquet(splits_dir / \"val_scaled.parquet\")\n    # Only load test when needed for final evaluation\n    # test_df = pd.read_parquet(splits_dir / \"test_scaled.parquet\")\n    \n    print(f\"Train: {len(train_df):,} rows, {train_df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n    print(f\"Val:   {len(val_df):,} rows, {val_df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n    \n    # Force garbage collection after loading\n    gc.collect()\nelse:\n    print(\"Splits not found. Run pipeline first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps: Model Training (Phase 2)\n",
    "\n",
    "After running the data pipeline, you can train models using the prepared datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data for sklearn model with proper memory management\nfrom src.phase1.stages.datasets.container import TimeSeriesDataContainer\nimport gc\n\nsplits_dir = Path(f\"{PROJECT_ROOT}/data/splits/scaled\")\n\nif splits_dir.exists():\n    # Load container for specified horizon\n    HORIZON = 20\n    container = TimeSeriesDataContainer.from_parquet_dir(\n        path=splits_dir,\n        horizon=HORIZON,\n        exclude_invalid_labels=True\n    )\n    \n    # Extract only the arrays needed for training (train + val)\n    # Do NOT load test data until final evaluation\n    X_train, y_train, w_train = container.get_sklearn_arrays('train')\n    X_val, y_val, w_val = container.get_sklearn_arrays('val')\n    \n    # Store feature column names before deleting container\n    feature_columns = container.feature_columns.copy()\n    n_features = container.n_features\n    \n    print(f\"X_train shape: {X_train.shape}\")\n    print(f\"X_val shape: {X_val.shape}\")\n    print(f\"Features: {n_features}\")\n    \n    # CRITICAL: Delete container immediately after extracting arrays\n    # Container holds 3 full DataFrames (train, val, test) in memory\n    del container\n    gc.collect()\n    \n    print(\"Container deleted, memory freed\")\nelse:\n    print(\"Splits not found. Run pipeline first.\")"
  },
  {
   "cell_type": "code",
   "source": "# Train a model with memory-efficient data loading\n# This cell shows the recommended pattern for model training\nimport gc\n\n# Training loop - uses X_train, y_train, w_train from previous cell\n# Example with XGBoost (uncomment to use):\n#\n# from xgboost import XGBClassifier\n# \n# model = XGBClassifier(\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     tree_method='hist',  # Memory efficient\n#     random_state=42\n# )\n# \n# model.fit(\n#     X_train, y_train,\n#     sample_weight=w_train,\n#     eval_set=[(X_val, y_val)],\n#     verbose=False\n# )\n# \n# # After training, delete training data if no longer needed\n# del X_train, y_train, w_train\n# gc.collect()\n# print(\"Training data deleted after model fit\")\n# \n# # Validation predictions\n# val_preds = model.predict(X_val)\n# val_proba = model.predict_proba(X_val)\n\nprint(\"See commented code for memory-efficient training pattern\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Final evaluation: Load test data ONLY when needed\n# This avoids keeping test data in memory during training\nfrom src.phase1.stages.datasets.container import TimeSeriesDataContainer\nimport gc\n\ndef load_test_data_for_evaluation(splits_dir, horizon=20):\n    \"\"\"\n    Load test data on-demand for final model evaluation.\n    \n    This function creates a temporary container, extracts test arrays,\n    and immediately cleans up to minimize memory usage.\n    \"\"\"\n    container = TimeSeriesDataContainer.from_parquet_dir(\n        path=splits_dir,\n        horizon=horizon,\n        exclude_invalid_labels=True\n    )\n    \n    # Extract only test arrays\n    X_test, y_test, w_test = container.get_sklearn_arrays('test')\n    \n    # Immediately delete container\n    del container\n    gc.collect()\n    \n    return X_test, y_test, w_test\n\n\n# Usage (uncomment when ready for final evaluation):\n# X_test, y_test, w_test = load_test_data_for_evaluation(splits_dir, horizon=20)\n# \n# # Run final predictions\n# test_preds = model.predict(X_test)\n# test_proba = model.predict_proba(X_test)\n# \n# # Calculate metrics\n# from sklearn.metrics import accuracy_score, classification_report\n# print(f\"Test Accuracy: {accuracy_score(y_test, test_preds):.4f}\")\n# print(classification_report(y_test, test_preds))\n# \n# # Clean up test data after evaluation\n# del X_test, y_test, w_test\n# gc.collect()\n\nprint(\"Test data loader function defined - use for final evaluation only\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook complete!\")\n",
    "print(f\"\\nResults saved to: {PROJECT_ROOT}\")"
   ]
  }
 ]
}