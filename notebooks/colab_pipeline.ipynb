{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Model Factory - OHLCV Pipeline\n",
        "\n",
        "This notebook runs the ensemble price prediction pipeline on Google Colab.\n",
        "\n",
        "## Prerequisites\n",
        "1. Upload your raw data files (`MES_1m.parquet`, `MGC_1m.parquet`) to Google Drive\n",
        "2. Create a folder structure: `My Drive/research/data/raw/`\n",
        "\n",
        "## Pipeline Stages\n",
        "1. **Data Ingestion** - Load and validate raw OHLCV data\n",
        "2. **Data Cleaning** - Resample 1min → 5min, handle gaps\n",
        "3. **Feature Engineering** - Generate 150+ technical indicators\n",
        "4. **Labeling** - Triple-barrier label generation\n",
        "5. **Optimization** - Optuna parameter tuning\n",
        "6. **Splits** - Train/Val/Test with purge/embargo\n",
        "7. **Scaling** - Train-only feature scaling\n",
        "8. **Validation** - Data quality checks\n",
        "9. **Reporting** - Generate summary report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set project root\n",
        "import os\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/research'\n",
        "os.environ['PROJECT_ROOT'] = PROJECT_ROOT\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository (if not already present)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "repo_path = Path('/content/research')\n",
        "if not repo_path.exists():\n",
        "    !git clone https://github.com/Snehpatel101/research.git /content/research\n",
        "else:\n",
        "    print(\"Repository already cloned\")\n",
        "    # Pull latest changes\n",
        "    !cd /content/research && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the package\n",
        "!pip install -e /content/research --quiet\n",
        "\n",
        "# Verify installation\n",
        "import src\n",
        "print(f\"Package version: {src.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create required directories\n",
        "from pathlib import Path\n",
        "\n",
        "dirs = [\n",
        "    f\"{PROJECT_ROOT}/data/raw\",\n",
        "    f\"{PROJECT_ROOT}/data/clean\",\n",
        "    f\"{PROJECT_ROOT}/data/features\",\n",
        "    f\"{PROJECT_ROOT}/data/labels\",\n",
        "    f\"{PROJECT_ROOT}/data/final\",\n",
        "    f\"{PROJECT_ROOT}/data/splits\",\n",
        "    f\"{PROJECT_ROOT}/runs\",\n",
        "    f\"{PROJECT_ROOT}/results\",\n",
        "    f\"{PROJECT_ROOT}/config\",\n",
        "]\n",
        "\n",
        "for d in dirs:\n",
        "    Path(d).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Created: {d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify Data Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for raw data files\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "raw_dir = Path(f\"{PROJECT_ROOT}/data/raw\")\n",
        "\n",
        "symbols = ['MES', 'MGC']\n",
        "missing = []\n",
        "\n",
        "for symbol in symbols:\n",
        "    parquet_path = raw_dir / f\"{symbol}_1m.parquet\"\n",
        "    csv_path = raw_dir / f\"{symbol}_1m.csv\"\n",
        "    \n",
        "    if parquet_path.exists():\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "        print(f\"{symbol}: {len(df):,} rows ({parquet_path.stat().st_size / 1e6:.1f} MB)\")\n",
        "    elif csv_path.exists():\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"{symbol}: {len(df):,} rows (CSV)\")\n",
        "    else:\n",
        "        missing.append(symbol)\n",
        "        print(f\"{symbol}: NOT FOUND\")\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n*** Upload missing files: {missing} ***\")\n",
        "    print(f\"Expected location: {raw_dir}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.phase1.pipeline_config import PipelineConfig, create_default_config\n",
        "from pathlib import Path\n",
        "\n",
        "# Create configuration\n",
        "config = create_default_config(\n",
        "    symbols=['MES', 'MGC'],\n",
        "    project_root=Path(PROJECT_ROOT),\n",
        "    start_date=None,  # Use all available data\n",
        "    end_date=None,\n",
        "    target_timeframe='5min',\n",
        "    label_horizons=[5, 10, 15, 20],\n",
        "    train_ratio=0.70,\n",
        "    val_ratio=0.15,\n",
        "    test_ratio=0.15,\n",
        ")\n",
        "\n",
        "# Display configuration summary\n",
        "print(\"Pipeline Configuration:\")\n",
        "print(f\"  Run ID: {config.run_id}\")\n",
        "print(f\"  Symbols: {config.symbols}\")\n",
        "print(f\"  Timeframe: {config.target_timeframe}\")\n",
        "print(f\"  Horizons: {config.label_horizons}\")\n",
        "print(f\"  Train/Val/Test: {config.train_ratio}/{config.val_ratio}/{config.test_ratio}\")\n",
        "print(f\"  Project Root: {config.project_root}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Full Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.pipeline.runner import PipelineRunner\n",
        "\n",
        "# Create and run pipeline\n",
        "runner = PipelineRunner(config)\n",
        "\n",
        "print(\"Starting pipeline...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "success = runner.run()\n",
        "\n",
        "print(\"=\"*50)\n",
        "if success:\n",
        "    print(\"Pipeline completed successfully!\")\n",
        "else:\n",
        "    print(\"Pipeline failed. Check logs above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check output files\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def show_directory_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
        "    if current_depth >= max_depth:\n",
        "        return\n",
        "    \n",
        "    path = Path(path)\n",
        "    if not path.exists():\n",
        "        print(f\"{prefix}{path.name}/ (not found)\")\n",
        "        return\n",
        "    \n",
        "    items = sorted(path.iterdir())\n",
        "    for i, item in enumerate(items):\n",
        "        is_last = i == len(items) - 1\n",
        "        connector = \"└── \" if is_last else \"├── \"\n",
        "        \n",
        "        if item.is_file():\n",
        "            size = item.stat().st_size\n",
        "            if size > 1e9:\n",
        "                size_str = f\"{size/1e9:.1f}GB\"\n",
        "            elif size > 1e6:\n",
        "                size_str = f\"{size/1e6:.1f}MB\"\n",
        "            elif size > 1e3:\n",
        "                size_str = f\"{size/1e3:.1f}KB\"\n",
        "            else:\n",
        "                size_str = f\"{size}B\"\n",
        "            print(f\"{prefix}{connector}{item.name} ({size_str})\")\n",
        "        else:\n",
        "            print(f\"{prefix}{connector}{item.name}/\")\n",
        "            new_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
        "            show_directory_tree(item, new_prefix, max_depth, current_depth + 1)\n",
        "\n",
        "print(\"Data Directory:\")\n",
        "show_directory_tree(f\"{PROJECT_ROOT}/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display final labeled data sample\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "final_dir = Path(f\"{PROJECT_ROOT}/data/final\")\n",
        "files = list(final_dir.glob(\"*_labeled.parquet\"))\n",
        "\n",
        "if files:\n",
        "    df = pd.read_parquet(files[0])\n",
        "    print(f\"Loaded: {files[0].name}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
        "    print(df.columns.tolist()[:20], \"...\")\n",
        "    print(f\"\\nSample:\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"No final labeled data found. Run the pipeline first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display label distribution\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'df' in dir() and df is not None:\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    \n",
        "    for i, h in enumerate([5, 10, 15, 20]):\n",
        "        col = f'label_h{h}'\n",
        "        if col in df.columns:\n",
        "            counts = df[col].value_counts().sort_index()\n",
        "            colors = ['red', 'gray', 'green']\n",
        "            axes[i].bar(counts.index, counts.values, color=colors)\n",
        "            axes[i].set_title(f'H{h} Labels')\n",
        "            axes[i].set_xlabel('Label')\n",
        "            axes[i].set_ylabel('Count')\n",
        "            axes[i].set_xticks([-1, 0, 1])\n",
        "            axes[i].set_xticklabels(['Short', 'Neutral', 'Long'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run Individual Stages (Optional)\n",
        "\n",
        "If you want to run specific stages instead of the full pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Run only data ingestion\n",
        "from src.phase1.stages import DataIngestor\n",
        "from pathlib import Path\n",
        "\n",
        "# ingestor = DataIngestor(\n",
        "#     raw_data_dir=Path(f\"{PROJECT_ROOT}/data/raw\"),\n",
        "#     output_dir=Path(f\"{PROJECT_ROOT}/data/raw/validated\")\n",
        "# )\n",
        "# \n",
        "# df, metadata = ingestor.ingest_file(\n",
        "#     file_path=Path(f\"{PROJECT_ROOT}/data/raw/MES_1m.parquet\"),\n",
        "#     symbol='MES',\n",
        "#     validate=True\n",
        "# )\n",
        "# print(f\"Ingested {len(df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load pre-computed train/val/test splits\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "splits_dir = Path(f\"{PROJECT_ROOT}/data/splits/scaled\")\n",
        "\n",
        "if splits_dir.exists():\n",
        "    train_df = pd.read_parquet(splits_dir / \"train_scaled.parquet\")\n",
        "    val_df = pd.read_parquet(splits_dir / \"val_scaled.parquet\")\n",
        "    test_df = pd.read_parquet(splits_dir / \"test_scaled.parquet\")\n",
        "    \n",
        "    print(f\"Train: {len(train_df):,} rows\")\n",
        "    print(f\"Val:   {len(val_df):,} rows\")\n",
        "    print(f\"Test:  {len(test_df):,} rows\")\n",
        "else:\n",
        "    print(\"Splits not found. Run pipeline first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Next Steps: Model Training (Phase 2)\n",
        "\n",
        "After running the data pipeline, you can train models using the prepared datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Prepare data for sklearn model\n",
        "# from src.phase1.stages.datasets.container import TimeSeriesDataContainer, DataContainerConfig\n",
        "#\n",
        "# container_config = DataContainerConfig(\n",
        "#     horizon=20,\n",
        "#     feature_columns=[...],  # List your feature columns\n",
        "#     label_column='label_h20',\n",
        "#     weight_column='sample_weight_h20'\n",
        "# )\n",
        "#\n",
        "# container = TimeSeriesDataContainer(container_config)\n",
        "# container.load_splits(splits_dir)\n",
        "#\n",
        "# X_train, y_train, w_train = container.get_sklearn_arrays('train')\n",
        "# X_val, y_val, w_val = container.get_sklearn_arrays('val')\n",
        "#\n",
        "# print(f\"X_train shape: {X_train.shape}\")\n",
        "# print(f\"y_train shape: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Notebook complete!\")\n",
        "print(f\"\\nResults saved to: {PROJECT_ROOT}\")"
      ]
    }
  ]
}
