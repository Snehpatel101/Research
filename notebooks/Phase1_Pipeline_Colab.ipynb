{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: AI Ensemble Trading - Data Preparation Pipeline\n",
    "\n",
    "This notebook runs the complete Phase 1 pipeline for preparing your trading data:\n",
    "1. **Data Ingestion** - Load raw 1-minute OHLCV data\n",
    "2. **Data Cleaning** - Fix gaps, outliers, duplicates\n",
    "3. **Feature Engineering** - Generate 60+ technical indicators\n",
    "4. **Triple-Barrier Labeling** - Create trading labels\n",
    "5. **GA Optimization** - Optimize label parameters\n",
    "6. **Final Labels + Weights** - Apply quality-based weighting\n",
    "7. **Time-Based Splits** - Create train/val/test sets\n",
    "8. **Validation + Backtest** - Verify data quality\n",
    "\n",
    "## Instructions\n",
    "1. Upload your data files (MES_1m.csv, MGC_1m.csv) to Google Drive\n",
    "2. Update the `DATA_PATHS` dictionary below with your file IDs\n",
    "3. Run all cells in order\n",
    "4. Download the final artifacts from the `outputs/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy numba deap scipy matplotlib seaborn tqdm pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access your data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THESE PATHS\n",
    "DATA_PATHS = {\n",
    "    'MES': '/content/drive/MyDrive/trading_data/MES_1m.csv',  # Update this path\n",
    "    'MGC': '/content/drive/MyDrive/trading_data/MGC_1m.csv',  # Update this path\n",
    "}\n",
    "\n",
    "# Pipeline configuration\n",
    "CONFIG = {\n",
    "    'symbols': ['MES', 'MGC'],\n",
    "    'horizons': [1, 5, 20],  # Prediction horizons in bars\n",
    "    'train_ratio': 0.70,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    'purge_bars': 20,\n",
    "    'embargo_bars': 288,  # ~1 day for 5-min data\n",
    "    'ga_population': 30,  # Reduced for Colab speed\n",
    "    'ga_generations': 20,\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "import os\n",
    "for d in ['outputs/raw', 'outputs/clean', 'outputs/features', \n",
    "          'outputs/labels', 'outputs/final', 'outputs/splits', \n",
    "          'outputs/reports', 'outputs/ga_results']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Symbols: {CONFIG['symbols']}\")\n",
    "print(f\"Horizons: {CONFIG['horizons']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_standardize(filepath, symbol):\n",
    "    \"\"\"Load raw data and standardize column names.\"\"\"\n",
    "    print(f\"Loading {symbol} from {filepath}...\")\n",
    "    \n",
    "    # Load data\n",
    "    if filepath.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filepath)\n",
    "    else:\n",
    "        df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(f\"  Raw shape: {df.shape}\")\n",
    "    print(f\"  Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Standardize column names\n",
    "    col_map = {}\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'time' in col_lower or 'date' in col_lower:\n",
    "            col_map[col] = 'datetime'\n",
    "        elif col_lower == 'open' or col_lower == 'o':\n",
    "            col_map[col] = 'open'\n",
    "        elif col_lower == 'high' or col_lower == 'h':\n",
    "            col_map[col] = 'high'\n",
    "        elif col_lower == 'low' or col_lower == 'l':\n",
    "            col_map[col] = 'low'\n",
    "        elif col_lower == 'close' or col_lower == 'c':\n",
    "            col_map[col] = 'close'\n",
    "        elif col_lower in ['volume', 'vol', 'v']:\n",
    "            col_map[col] = 'volume'\n",
    "    \n",
    "    df = df.rename(columns=col_map)\n",
    "    \n",
    "    # Convert datetime\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    # Add symbol\n",
    "    df['symbol'] = symbol\n",
    "    \n",
    "    # Validate OHLC\n",
    "    df['high'] = df[['high', 'open', 'close']].max(axis=1)\n",
    "    df['low'] = df[['low', 'open', 'close']].min(axis=1)\n",
    "    \n",
    "    print(f\"  Standardized shape: {df.shape}\")\n",
    "    print(f\"  Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load all symbols\n",
    "raw_data = {}\n",
    "for symbol, path in DATA_PATHS.items():\n",
    "    if os.path.exists(path):\n",
    "        raw_data[symbol] = load_and_standardize(path, symbol)\n",
    "        # Save as parquet\n",
    "        raw_data[symbol].to_parquet(f'outputs/raw/{symbol}_1m.parquet', index=False)\n",
    "    else:\n",
    "        print(f\"WARNING: File not found: {path}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(raw_data)} symbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, symbol, max_gap_fill=30):\n",
    "    \"\"\"Clean and resample data to 5-minute bars.\"\"\"\n",
    "    print(f\"\\nCleaning {symbol}...\")\n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # 1. Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['datetime'], keep='first')\n",
    "    print(f\"  After dedup: {len(df)} rows\")\n",
    "    \n",
    "    # 2. Detect gaps\n",
    "    df = df.set_index('datetime')\n",
    "    time_diff = df.index.to_series().diff().dt.total_seconds() / 60\n",
    "    gaps = time_diff[time_diff > 2].dropna()\n",
    "    print(f\"  Gaps found: {len(gaps)} (largest: {gaps.max() if len(gaps) > 0 else 0:.0f} min)\")\n",
    "    \n",
    "    # 3. Fill small gaps (up to max_gap_fill minutes)\n",
    "    full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='1min')\n",
    "    df = df.reindex(full_index)\n",
    "    df = df.ffill(limit=max_gap_fill)\n",
    "    df = df.dropna()\n",
    "    print(f\"  After gap fill: {len(df)} rows\")\n",
    "    \n",
    "    # 4. Resample to 5-minute bars\n",
    "    df_5m = df.resample('5min').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum',\n",
    "        'symbol': 'first'\n",
    "    }).dropna()\n",
    "    \n",
    "    df_5m = df_5m.reset_index().rename(columns={'index': 'datetime'})\n",
    "    \n",
    "    # 5. Remove outliers (price spikes > 5 ATRs)\n",
    "    atr = (df_5m['high'] - df_5m['low']).rolling(20).mean()\n",
    "    price_change = df_5m['close'].diff().abs()\n",
    "    spike_mask = price_change > (5 * atr)\n",
    "    if spike_mask.any():\n",
    "        print(f\"  Removed {spike_mask.sum()} spike outliers\")\n",
    "        df_5m.loc[spike_mask, ['open', 'high', 'low', 'close']] = np.nan\n",
    "        df_5m = df_5m.ffill().dropna()\n",
    "    \n",
    "    print(f\"  Final: {len(df_5m)} 5-min bars\")\n",
    "    \n",
    "    return df_5m\n",
    "\n",
    "# Clean all symbols\n",
    "clean_data_dict = {}\n",
    "for symbol, df in raw_data.items():\n",
    "    clean_data_dict[symbol] = clean_data(df.copy(), symbol)\n",
    "    clean_data_dict[symbol].to_parquet(f'outputs/clean/{symbol}_5m_clean.parquet', index=False)\n",
    "\n",
    "print(\"\\nCleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(df):\n",
    "    \"\"\"Compute 60+ technical features.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === Returns ===\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['simple_return'] = df['close'].pct_change()\n",
    "    for p in [5, 10, 20]:\n",
    "        df[f'return_{p}'] = df['close'].pct_change(p)\n",
    "    \n",
    "    # === Moving Averages ===\n",
    "    for p in [10, 20, 50, 100, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'close_to_sma_{p}'] = df['close'] / df[f'sma_{p}'] - 1\n",
    "    \n",
    "    for p in [9, 21, 50]:\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "        df[f'close_to_ema_{p}'] = df['close'] / df[f'ema_{p}'] - 1\n",
    "    \n",
    "    # === RSI ===\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss.replace(0, np.inf)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['rsi_oversold'] = (df['rsi'] < 30).astype(int)\n",
    "    df['rsi_overbought'] = (df['rsi'] > 70).astype(int)\n",
    "    \n",
    "    # === MACD ===\n",
    "    ema12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # === Bollinger Bands ===\n",
    "    sma20 = df['close'].rolling(20).mean()\n",
    "    std20 = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = sma20 + 2 * std20\n",
    "    df['bb_lower'] = sma20 - 2 * std20\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / sma20\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "    \n",
    "    # === ATR ===\n",
    "    tr = pd.concat([\n",
    "        df['high'] - df['low'],\n",
    "        abs(df['high'] - df['close'].shift(1)),\n",
    "        abs(df['low'] - df['close'].shift(1))\n",
    "    ], axis=1).max(axis=1)\n",
    "    \n",
    "    for p in [7, 14, 21]:\n",
    "        df[f'atr_{p}'] = tr.rolling(p).mean()\n",
    "        df[f'atr_{p}_pct'] = df[f'atr_{p}'] / df['close']\n",
    "    \n",
    "    # === Stochastic ===\n",
    "    low_14 = df['low'].rolling(14).min()\n",
    "    high_14 = df['high'].rolling(14).max()\n",
    "    df['stoch_k'] = 100 * (df['close'] - low_14) / (high_14 - low_14)\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    # === ADX ===\n",
    "    high_diff = df['high'].diff()\n",
    "    low_diff = -df['low'].diff()\n",
    "    plus_dm = np.where((high_diff > low_diff) & (high_diff > 0), high_diff, 0)\n",
    "    minus_dm = np.where((low_diff > high_diff) & (low_diff > 0), low_diff, 0)\n",
    "    atr14 = tr.rolling(14).mean()\n",
    "    plus_di = 100 * pd.Series(plus_dm).rolling(14).mean() / atr14\n",
    "    minus_di = 100 * pd.Series(minus_dm).rolling(14).mean() / atr14\n",
    "    dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)\n",
    "    df['adx'] = dx.rolling(14).mean()\n",
    "    df['plus_di'] = plus_di\n",
    "    df['minus_di'] = minus_di\n",
    "    \n",
    "    # === Volume Features ===\n",
    "    df['volume_sma_20'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma_20']\n",
    "    df['obv'] = (np.sign(df['close'].diff()) * df['volume']).cumsum()\n",
    "    \n",
    "    # === Temporal Features ===\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dow'] = df['datetime'].dt.dayofweek\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 5)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 5)\n",
    "    df['is_rth'] = ((df['hour'] >= 9) & (df['hour'] < 16)).astype(int)\n",
    "    df = df.drop(columns=['hour', 'dow'])\n",
    "    \n",
    "    # === Regime Features ===\n",
    "    vol_20 = df['log_return'].rolling(20).std()\n",
    "    vol_60 = df['log_return'].rolling(60).std()\n",
    "    df['vol_regime'] = np.where(vol_20 > vol_60, 1, -1)\n",
    "    df['trend_regime'] = np.where(df['sma_20'] > df['sma_50'], 1, -1)\n",
    "    \n",
    "    # Drop NaN rows\n",
    "    df = df.dropna()\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate features for all symbols\n",
    "feature_data = {}\n",
    "for symbol, df in clean_data_dict.items():\n",
    "    print(f\"\\nGenerating features for {symbol}...\")\n",
    "    feature_data[symbol] = compute_features(df)\n",
    "    n_features = len([c for c in feature_data[symbol].columns \n",
    "                      if c not in ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume']])\n",
    "    print(f\"  Generated {n_features} features\")\n",
    "    print(f\"  Shape: {feature_data[symbol].shape}\")\n",
    "    feature_data[symbol].to_parquet(f'outputs/features/{symbol}_5m_features.parquet', index=False)\n",
    "\n",
    "print(\"\\nFeature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Triple-Barrier Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def apply_triple_barrier_numba(close, high, low, atr, k_up, k_down, max_bars):\n",
    "    \"\"\"Numba-optimized triple barrier labeling.\"\"\"\n",
    "    n = len(close)\n",
    "    labels = np.zeros(n, dtype=np.int32)\n",
    "    bars_to_hit = np.zeros(n, dtype=np.int32)\n",
    "    mae = np.zeros(n, dtype=np.float64)\n",
    "    mfe = np.zeros(n, dtype=np.float64)\n",
    "    \n",
    "    for i in prange(n - max_bars):\n",
    "        entry = close[i]\n",
    "        curr_atr = atr[i]\n",
    "        \n",
    "        if curr_atr <= 0 or np.isnan(curr_atr):\n",
    "            continue\n",
    "        \n",
    "        upper = entry + k_up * curr_atr\n",
    "        lower = entry - k_down * curr_atr\n",
    "        \n",
    "        max_adverse = 0.0\n",
    "        max_favorable = 0.0\n",
    "        hit_bar = max_bars\n",
    "        hit_label = 0\n",
    "        \n",
    "        for j in range(1, max_bars + 1):\n",
    "            idx = i + j\n",
    "            if idx >= n:\n",
    "                break\n",
    "            \n",
    "            # Track MFE/MAE\n",
    "            favorable = (high[idx] - entry) / entry\n",
    "            adverse = (entry - low[idx]) / entry\n",
    "            max_favorable = max(max_favorable, favorable)\n",
    "            max_adverse = max(max_adverse, adverse)\n",
    "            \n",
    "            # Check barriers\n",
    "            if high[idx] >= upper:\n",
    "                hit_bar = j\n",
    "                hit_label = 1\n",
    "                break\n",
    "            if low[idx] <= lower:\n",
    "                hit_bar = j\n",
    "                hit_label = -1\n",
    "                break\n",
    "        \n",
    "        labels[i] = hit_label\n",
    "        bars_to_hit[i] = hit_bar\n",
    "        mae[i] = max_adverse\n",
    "        mfe[i] = max_favorable\n",
    "    \n",
    "    return labels, bars_to_hit, mae, mfe\n",
    "\n",
    "def apply_labels(df, horizon, k_up, k_down, max_bars):\n",
    "    \"\"\"Apply triple barrier labeling to dataframe.\"\"\"\n",
    "    atr_col = f'atr_{max(7, min(horizon * 2, 21))}'\n",
    "    if atr_col not in df.columns:\n",
    "        atr_col = 'atr_14'\n",
    "    \n",
    "    labels, bars_to_hit, mae, mfe = apply_triple_barrier_numba(\n",
    "        df['close'].values.astype(np.float64),\n",
    "        df['high'].values.astype(np.float64),\n",
    "        df['low'].values.astype(np.float64),\n",
    "        df[atr_col].values.astype(np.float64),\n",
    "        k_up, k_down, max_bars\n",
    "    )\n",
    "    \n",
    "    df[f'label_h{horizon}'] = labels\n",
    "    df[f'bars_to_hit_h{horizon}'] = bars_to_hit\n",
    "    df[f'mae_h{horizon}'] = mae\n",
    "    df[f'mfe_h{horizon}'] = mfe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Initial labeling parameters\n",
    "INIT_PARAMS = {\n",
    "    1: (1.0, 1.0, 2),\n",
    "    5: (1.5, 1.0, 10),\n",
    "    20: (2.0, 1.5, 40)\n",
    "}\n",
    "\n",
    "# Apply initial labels\n",
    "labeled_data = {}\n",
    "for symbol, df in feature_data.items():\n",
    "    print(f\"\\nLabeling {symbol}...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    for horizon in CONFIG['horizons']:\n",
    "        k_up, k_down, max_bars = INIT_PARAMS[horizon]\n",
    "        df = apply_labels(df, horizon, k_up, k_down, max_bars)\n",
    "        \n",
    "        # Print distribution\n",
    "        dist = pd.Series(df[f'label_h{horizon}']).value_counts().sort_index()\n",
    "        print(f\"  H{horizon}: Long={dist.get(1, 0):,} Neutral={dist.get(0, 0):,} Short={dist.get(-1, 0):,}\")\n",
    "    \n",
    "    labeled_data[symbol] = df\n",
    "    df.to_parquet(f'outputs/labels/{symbol}_labels_init.parquet', index=False)\n",
    "\n",
    "print(\"\\nInitial labeling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: GA Optimization (Optional - Takes ~5-10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_GA_OPTIMIZATION = True  # Set to False to skip GA and use initial params\n",
    "\n",
    "if RUN_GA_OPTIMIZATION:\n",
    "    from deap import base, creator, tools, algorithms\n",
    "    import random\n",
    "    import json\n",
    "    \n",
    "    def evaluate_params(individual, df, horizon):\n",
    "        \"\"\"Evaluate labeling parameters.\"\"\"\n",
    "        k_up, k_down, max_bars = individual\n",
    "        max_bars = int(max_bars)\n",
    "        \n",
    "        # Apply labels\n",
    "        atr_col = f'atr_{max(7, min(horizon * 2, 21))}'\n",
    "        if atr_col not in df.columns:\n",
    "            atr_col = 'atr_14'\n",
    "        \n",
    "        labels, bars_to_hit, _, _ = apply_triple_barrier_numba(\n",
    "            df['close'].values.astype(np.float64),\n",
    "            df['high'].values.astype(np.float64),\n",
    "            df['low'].values.astype(np.float64),\n",
    "            df[atr_col].values.astype(np.float64),\n",
    "            k_up, k_down, max_bars\n",
    "        )\n",
    "        \n",
    "        # Compute fitness\n",
    "        n_long = np.sum(labels == 1)\n",
    "        n_short = np.sum(labels == -1)\n",
    "        n_neutral = np.sum(labels == 0)\n",
    "        total = len(labels)\n",
    "        \n",
    "        if n_long + n_short < 100:\n",
    "            return (0.0,)\n",
    "        \n",
    "        # Balance score (prefer 30-40% signals)\n",
    "        signal_ratio = (n_long + n_short) / total\n",
    "        balance_score = 1 - abs(signal_ratio - 0.35) * 2\n",
    "        \n",
    "        # Win rate score (prefer 45-55%)\n",
    "        win_rate = n_long / (n_long + n_short) if (n_long + n_short) > 0 else 0.5\n",
    "        winrate_score = 1 - abs(win_rate - 0.50) * 4\n",
    "        \n",
    "        # Speed score (faster hits are better)\n",
    "        avg_bars = np.mean(bars_to_hit[labels != 0]) if np.sum(labels != 0) > 0 else max_bars\n",
    "        speed_score = 1 - (avg_bars / max_bars)\n",
    "        \n",
    "        fitness = 0.4 * balance_score + 0.3 * winrate_score + 0.3 * speed_score\n",
    "        return (max(0, fitness),)\n",
    "    \n",
    "    def run_ga(df, horizon, pop_size=30, n_gen=20):\n",
    "        \"\"\"Run GA optimization for a horizon.\"\"\"\n",
    "        # Setup DEAP\n",
    "        if hasattr(creator, 'FitnessMax'):\n",
    "            del creator.FitnessMax\n",
    "        if hasattr(creator, 'Individual'):\n",
    "            del creator.Individual\n",
    "        \n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "        \n",
    "        toolbox = base.Toolbox()\n",
    "        \n",
    "        # Attributes\n",
    "        toolbox.register(\"attr_k_up\", random.uniform, 0.5, 3.0)\n",
    "        toolbox.register(\"attr_k_down\", random.uniform, 0.5, 3.0)\n",
    "        toolbox.register(\"attr_max_bars\", random.uniform, horizon, horizon * 5)\n",
    "        \n",
    "        toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                        (toolbox.attr_k_up, toolbox.attr_k_down, toolbox.attr_max_bars), n=1)\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        \n",
    "        toolbox.register(\"evaluate\", evaluate_params, df=df, horizon=horizon)\n",
    "        toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "        toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.3, indpb=0.3)\n",
    "        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        \n",
    "        # Run GA\n",
    "        pop = toolbox.population(n=pop_size)\n",
    "        hof = tools.HallOfFame(1)\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"max\", np.max)\n",
    "        stats.register(\"avg\", np.mean)\n",
    "        \n",
    "        pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.7, mutpb=0.2,\n",
    "                                        ngen=n_gen, stats=stats, halloffame=hof, verbose=False)\n",
    "        \n",
    "        best = hof[0]\n",
    "        return {\n",
    "            'k_up': best[0],\n",
    "            'k_down': best[1],\n",
    "            'max_bars': int(best[2]),\n",
    "            'fitness': best.fitness.values[0]\n",
    "        }\n",
    "    \n",
    "    # Run GA for each symbol and horizon\n",
    "    ga_results = {}\n",
    "    for symbol, df in labeled_data.items():\n",
    "        print(f\"\\nOptimizing {symbol}...\")\n",
    "        ga_results[symbol] = {}\n",
    "        \n",
    "        # Use subset for speed\n",
    "        subset = df.sample(frac=0.2, random_state=42)\n",
    "        \n",
    "        for horizon in CONFIG['horizons']:\n",
    "            print(f\"  Horizon {horizon}...\", end=\" \")\n",
    "            result = run_ga(subset, horizon, \n",
    "                           pop_size=CONFIG['ga_population'],\n",
    "                           n_gen=CONFIG['ga_generations'])\n",
    "            ga_results[symbol][horizon] = result\n",
    "            print(f\"k_up={result['k_up']:.2f}, k_down={result['k_down']:.2f}, \"\n",
    "                  f\"max_bars={result['max_bars']}, fitness={result['fitness']:.3f}\")\n",
    "    \n",
    "    # Save GA results\n",
    "    with open('outputs/ga_results/ga_optimization_results.json', 'w') as f:\n",
    "        json.dump({s: {str(h): v for h, v in hs.items()} \n",
    "                   for s, hs in ga_results.items()}, f, indent=2)\n",
    "    \n",
    "    print(\"\\nGA optimization complete!\")\n",
    "else:\n",
    "    ga_results = None\n",
    "    print(\"GA optimization skipped. Using initial parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Labels with Quality Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quality_weights(df, horizon):\n",
    "    \"\"\"Compute quality scores and sample weights.\"\"\"\n",
    "    labels = df[f'label_h{horizon}']\n",
    "    bars = df[f'bars_to_hit_h{horizon}']\n",
    "    mae = df[f'mae_h{horizon}']\n",
    "    mfe = df[f'mfe_h{horizon}']\n",
    "    \n",
    "    # Get max_bars for this horizon\n",
    "    max_bars = bars.max()\n",
    "    \n",
    "    # Speed score (faster = better)\n",
    "    speed_score = 1 - (bars / max_bars)\n",
    "    \n",
    "    # MAE score (lower adverse excursion = better)\n",
    "    mae_score = 1 - mae.clip(0, 0.05) / 0.05\n",
    "    \n",
    "    # MFE score (higher favorable excursion = better)\n",
    "    mfe_score = mfe.clip(0, 0.05) / 0.05\n",
    "    \n",
    "    # Combined quality\n",
    "    quality = 0.3 * speed_score + 0.4 * mae_score + 0.3 * mfe_score\n",
    "    quality = quality.clip(0, 1)\n",
    "    \n",
    "    # Assign weights based on quality tiers\n",
    "    weights = np.ones(len(df))\n",
    "    q80 = quality.quantile(0.8)\n",
    "    q20 = quality.quantile(0.2)\n",
    "    \n",
    "    weights = np.where(quality >= q80, 1.5,  # Top 20%\n",
    "              np.where(quality <= q20, 0.5,  # Bottom 20%\n",
    "                       1.0))                  # Middle 60%\n",
    "    \n",
    "    df[f'quality_h{horizon}'] = quality\n",
    "    df[f'sample_weight_h{horizon}'] = weights\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply final labels using GA-optimized params (or initial params)\n",
    "final_data = {}\n",
    "for symbol, df in labeled_data.items():\n",
    "    print(f\"\\nFinalizing {symbol}...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    for horizon in CONFIG['horizons']:\n",
    "        # Get parameters\n",
    "        if ga_results and symbol in ga_results:\n",
    "            params = ga_results[symbol][horizon]\n",
    "            k_up, k_down, max_bars = params['k_up'], params['k_down'], params['max_bars']\n",
    "        else:\n",
    "            k_up, k_down, max_bars = INIT_PARAMS[horizon]\n",
    "        \n",
    "        # Re-apply labels with optimized params\n",
    "        atr_col = f'atr_{max(7, min(horizon * 2, 21))}'\n",
    "        if atr_col not in df.columns:\n",
    "            atr_col = 'atr_14'\n",
    "        \n",
    "        labels, bars_to_hit, mae, mfe = apply_triple_barrier_numba(\n",
    "            df['close'].values.astype(np.float64),\n",
    "            df['high'].values.astype(np.float64),\n",
    "            df['low'].values.astype(np.float64),\n",
    "            df[atr_col].values.astype(np.float64),\n",
    "            k_up, k_down, max_bars\n",
    "        )\n",
    "        \n",
    "        df[f'label_h{horizon}'] = labels\n",
    "        df[f'bars_to_hit_h{horizon}'] = bars_to_hit\n",
    "        df[f'mae_h{horizon}'] = mae\n",
    "        df[f'mfe_h{horizon}'] = mfe\n",
    "        \n",
    "        # Compute quality weights\n",
    "        df = compute_quality_weights(df, horizon)\n",
    "        \n",
    "        # Print stats\n",
    "        dist = pd.Series(labels).value_counts().sort_index()\n",
    "        print(f\"  H{horizon}: Long={dist.get(1, 0):,} Neutral={dist.get(0, 0):,} \"\n",
    "              f\"Short={dist.get(-1, 0):,} | Avg Quality={df[f'quality_h{horizon}'].mean():.3f}\")\n",
    "    \n",
    "    final_data[symbol] = df\n",
    "    df.to_parquet(f'outputs/final/{symbol}_final_labeled.parquet', index=False)\n",
    "\n",
    "print(\"\\nFinal labeling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Time-Based Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Combine all symbols\n",
    "print(\"Creating combined dataset...\")\n",
    "combined_df = pd.concat(final_data.values(), ignore_index=True)\n",
    "combined_df = combined_df.sort_values('datetime').reset_index(drop=True)\n",
    "print(f\"Combined dataset: {len(combined_df):,} rows\")\n",
    "\n",
    "# Save combined dataset\n",
    "combined_df.to_parquet('outputs/final/combined_final_labeled.parquet', index=False)\n",
    "\n",
    "# Create splits\n",
    "n = len(combined_df)\n",
    "train_end = int(n * CONFIG['train_ratio'])\n",
    "val_end = int(n * (CONFIG['train_ratio'] + CONFIG['val_ratio']))\n",
    "\n",
    "# Apply purging and embargo\n",
    "train_end_purged = train_end - CONFIG['purge_bars']\n",
    "val_start = train_end + CONFIG['embargo_bars']\n",
    "test_start = val_end + CONFIG['embargo_bars']\n",
    "\n",
    "# Create indices\n",
    "train_indices = np.arange(0, train_end_purged)\n",
    "val_indices = np.arange(val_start, val_end)\n",
    "test_indices = np.arange(test_start, n)\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(train_indices):,} samples ({len(train_indices)/n*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_indices):,} samples ({len(val_indices)/n*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_indices):,} samples ({len(test_indices)/n*100:.1f}%)\")\n",
    "\n",
    "# Get date ranges\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"  Train: {combined_df.iloc[train_indices]['datetime'].min()} to {combined_df.iloc[train_indices]['datetime'].max()}\")\n",
    "print(f\"  Val:   {combined_df.iloc[val_indices]['datetime'].min()} to {combined_df.iloc[val_indices]['datetime'].max()}\")\n",
    "print(f\"  Test:  {combined_df.iloc[test_indices]['datetime'].min()} to {combined_df.iloc[test_indices]['datetime'].max()}\")\n",
    "\n",
    "# Save indices\n",
    "np.save('outputs/splits/train_indices.npy', train_indices)\n",
    "np.save('outputs/splits/val_indices.npy', val_indices)\n",
    "np.save('outputs/splits/test_indices.npy', test_indices)\n",
    "\n",
    "# Save metadata\n",
    "split_config = {\n",
    "    'total_samples': n,\n",
    "    'train_samples': len(train_indices),\n",
    "    'val_samples': len(val_indices),\n",
    "    'test_samples': len(test_indices),\n",
    "    'purge_bars': CONFIG['purge_bars'],\n",
    "    'embargo_bars': CONFIG['embargo_bars'],\n",
    "    'train_date_start': str(combined_df.iloc[train_indices]['datetime'].min()),\n",
    "    'train_date_end': str(combined_df.iloc[train_indices]['datetime'].max()),\n",
    "    'val_date_start': str(combined_df.iloc[val_indices]['datetime'].min()),\n",
    "    'val_date_end': str(combined_df.iloc[val_indices]['datetime'].max()),\n",
    "    'test_date_start': str(combined_df.iloc[test_indices]['datetime'].min()),\n",
    "    'test_date_end': str(combined_df.iloc[test_indices]['datetime'].max()),\n",
    "}\n",
    "\n",
    "with open('outputs/splits/split_config.json', 'w') as f:\n",
    "    json.dump(split_config, f, indent=2)\n",
    "\n",
    "print(\"\\nSplits saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Validation & Baseline Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 1 VALIDATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Data Integrity\n",
    "print(\"\\n--- Data Integrity ---\")\n",
    "print(f\"Total samples: {len(combined_df):,}\")\n",
    "print(f\"Duplicate timestamps: {combined_df.duplicated(subset=['datetime', 'symbol']).sum()}\")\n",
    "print(f\"NaN values: {combined_df.isnull().sum().sum()}\")\n",
    "print(f\"Inf values: {np.isinf(combined_df.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Feature Statistics\n",
    "print(\"\\n--- Feature Statistics ---\")\n",
    "feature_cols = [c for c in combined_df.columns \n",
    "                if c not in ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume']\n",
    "                and not c.startswith('label_') and not c.startswith('bars_to_hit_')\n",
    "                and not c.startswith('mae_') and not c.startswith('mfe_')\n",
    "                and not c.startswith('quality_') and not c.startswith('sample_weight_')]\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Label Distribution\n",
    "print(\"\\n--- Label Distribution ---\")\n",
    "for horizon in CONFIG['horizons']:\n",
    "    col = f'label_h{horizon}'\n",
    "    if col in combined_df.columns:\n",
    "        dist = combined_df[col].value_counts().sort_index()\n",
    "        total = len(combined_df)\n",
    "        print(f\"\\nHorizon {horizon}:\")\n",
    "        print(f\"  Short (-1): {dist.get(-1, 0):,} ({dist.get(-1, 0)/total*100:.1f}%)\")\n",
    "        print(f\"  Neutral (0): {dist.get(0, 0):,} ({dist.get(0, 0)/total*100:.1f}%)\")\n",
    "        print(f\"  Long (+1): {dist.get(1, 0):,} ({dist.get(1, 0)/total*100:.1f}%)\")\n",
    "\n",
    "# Plot label distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, horizon in enumerate(CONFIG['horizons']):\n",
    "    col = f'label_h{horizon}'\n",
    "    combined_df[col].value_counts().sort_index().plot(kind='bar', ax=axes[i], \n",
    "                                                       color=['red', 'gray', 'green'])\n",
    "    axes[i].set_title(f'Horizon {horizon} Label Distribution')\n",
    "    axes[i].set_xlabel('Label')\n",
    "    axes[i].set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/reports/label_distribution.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Quality distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, horizon in enumerate(CONFIG['horizons']):\n",
    "    col = f'quality_h{horizon}'\n",
    "    combined_df[col].hist(bins=50, ax=axes[i], color='steelblue', alpha=0.7)\n",
    "    axes[i].set_title(f'Horizon {horizon} Quality Score Distribution')\n",
    "    axes[i].set_xlabel('Quality Score')\n",
    "    axes[i].set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/reports/quality_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Baseline Backtest\n",
    "print(\"\\n--- Baseline Backtest ---\")\n",
    "print(\"(Trading in label direction when quality > 0.5)\\n\")\n",
    "\n",
    "for horizon in CONFIG['horizons']:\n",
    "    # Get signals (shifted to prevent lookahead)\n",
    "    signals = combined_df[f'label_h{horizon}'].shift(1)\n",
    "    quality = combined_df[f'quality_h{horizon}'].shift(1)\n",
    "    returns = combined_df['close'].pct_change(horizon).shift(-horizon)\n",
    "    \n",
    "    # Filter by quality\n",
    "    mask = (quality > 0.5) & (signals != 0)\n",
    "    \n",
    "    # Calculate strategy returns\n",
    "    strategy_returns = signals * returns\n",
    "    strategy_returns = strategy_returns[mask].dropna()\n",
    "    \n",
    "    if len(strategy_returns) > 0:\n",
    "        n_trades = len(strategy_returns)\n",
    "        win_rate = (strategy_returns > 0).mean()\n",
    "        avg_return = strategy_returns.mean()\n",
    "        sharpe = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252 * 78) if strategy_returns.std() > 0 else 0\n",
    "        \n",
    "        # Profit factor\n",
    "        wins = strategy_returns[strategy_returns > 0].sum()\n",
    "        losses = abs(strategy_returns[strategy_returns < 0].sum())\n",
    "        pf = wins / losses if losses > 0 else np.inf\n",
    "        \n",
    "        print(f\"Horizon {horizon}:\")\n",
    "        print(f\"  Trades: {n_trades:,}\")\n",
    "        print(f\"  Win Rate: {win_rate*100:.1f}%\")\n",
    "        print(f\"  Avg Return: {avg_return*100:.3f}%\")\n",
    "        print(f\"  Profit Factor: {pf:.2f}\")\n",
    "        print(f\"  Sharpe Ratio: {sharpe:.2f}\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1 COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of outputs\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "summary = f\"\"\"\n",
    "# Phase 1 Pipeline Results\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Dataset Summary\n",
    "- Total samples: {len(combined_df):,}\n",
    "- Symbols: {', '.join(CONFIG['symbols'])}\n",
    "- Features: {len(feature_cols)}\n",
    "- Horizons: {CONFIG['horizons']}\n",
    "\n",
    "## Splits\n",
    "- Train: {len(train_indices):,} samples\n",
    "- Validation: {len(val_indices):,} samples  \n",
    "- Test: {len(test_indices):,} samples\n",
    "\n",
    "## Output Files\n",
    "\"\"\"\n",
    "\n",
    "for root, dirs, files in os.walk('outputs'):\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        size = os.path.getsize(path) / 1024 / 1024  # MB\n",
    "        summary += f\"- {path}: {size:.2f} MB\\n\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open('outputs/reports/phase1_summary.md', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "# Zip outputs for download\n",
    "!zip -r phase1_outputs.zip outputs/\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Download 'phase1_outputs.zip' from the Files panel on the left\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example for Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load data for model training\n",
    "print(\"Example: Loading data for Phase 2 model training\\n\")\n",
    "\n",
    "# Load splits\n",
    "train_idx = np.load('outputs/splits/train_indices.npy')\n",
    "val_idx = np.load('outputs/splits/val_indices.npy')\n",
    "test_idx = np.load('outputs/splits/test_indices.npy')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet('outputs/final/combined_final_labeled.parquet')\n",
    "\n",
    "# Get train/val/test splits\n",
    "train_df = df.iloc[train_idx]\n",
    "val_df = df.iloc[val_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "\n",
    "# Get features (exclude non-feature columns)\n",
    "exclude_cols = ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume']\n",
    "label_cols = [c for c in df.columns if c.startswith(('label_', 'bars_to_hit_', 'mae_', 'mfe_', 'quality_', 'sample_weight_'))]\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols + label_cols]\n",
    "\n",
    "# Prepare training data for horizon 5\n",
    "horizon = 5\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df[f'label_h{horizon}'].values\n",
    "weights_train = train_df[f'sample_weight_h{horizon}'].values\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df[f'label_h{horizon}'].values\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df[f'label_h{horizon}'].values\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "print(f\"Sample features: {feature_cols[:10]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data ready for Phase 2 model training!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
