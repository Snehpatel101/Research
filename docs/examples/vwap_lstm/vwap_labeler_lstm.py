# -*- coding: utf-8 -*-
"""VWAP labeler LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CS1IB80woJU8IzwD0scRTJj75cAxGK6S
"""

pip install torch numpy pandas pandas-ta scikit-learn pyarrow -q

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VWAP MEAN REVERSION V2.1 - LSTM VERSION WITH RELAXED LABELING
================================================================
Ranging market specialist using LSTM for sequence modeling

üÜï V2.1 CHANGES FROM V2.0:
- ‚úÖ RELAXED regime filtering (higher ADX threshold, optional per tier)
- ‚úÖ RELAXED quality gates (lower volume/extreme requirements)
- ‚úÖ WIDER percentile thresholds (more candidates)
- ‚úÖ ADDED C-tier for additional signals
- ‚úÖ Target: 25-35% total signal rate (was 1-2%)

STRATEGY:
- Fade extremes from VWAP with volume confirmation
- Shorter horizons (45-60 min trades)
- TIERED quality system: A+ (premium), A (high), B+ (good), C (acceptable)
- Optimized for prop firm challenges
"""

import numpy as np
import pandas as pd
import pandas_ta as ta
from collections import Counter
import warnings
import os
import time
from numba import njit, prange

warnings.filterwarnings('ignore')
pd.options.mode.chained_assignment = None

# =========================================================
# CONFIGURATION
# =========================================================

CACHE_DIR = "/content/drive/MyDrive/AI_Cache/VWAP_MeanReversion_LSTM_v1_0"

# LSTM configuration
SEQUENCE_LENGTH = 45  # 45 bars = 135 minutes
PRIMARY_HORIZON = 20  # 20 bars = 60 minutes

SPLIT_DATE = "2025-01-01"

# Tighter stops for mean reversion
MARKET_PARAMS = {
    'ES': {
        'long_stop_pts': 6.0,
        'long_wiggle': 1.5,
        'long_target_1R': 6.0,
        'long_target_2R': 12.0,
        'long_target_3R': 18.0,
        'short_stop_pts': 6.0,
        'short_wiggle': 1.5,
        'short_target_1R': 6.0,
        'short_target_2R': 12.0,
        'short_target_3R': 18.0,
    },
    'NQ': {
        'long_stop_pts': 30.0,
        'long_wiggle': 8.0,
        'long_target_1R': 30.0,
        'long_target_2R': 60.0,
        'long_target_3R': 90.0,
        'short_stop_pts': 30.0,
        'short_wiggle': 8.0,
        'short_target_1R': 30.0,
        'short_target_2R': 60.0,
        'short_target_3R': 90.0,
    },
    'RTY': {
        'long_stop_pts': 4.0,
        'long_wiggle': 1.0,
        'long_target_1R': 4.0,
        'long_target_2R': 8.0,
        'long_target_3R': 12.0,
        'short_stop_pts': 4.0,
        'short_wiggle': 1.0,
        'short_target_1R': 4.0,
        'short_target_2R': 8.0,
        'short_target_3R': 12.0,
    },
    'YM': {
        'long_stop_pts': 40.0,
        'long_wiggle': 10.0,
        'long_target_1R': 40.0,
        'long_target_2R': 80.0,
        'long_target_3R': 120.0,
        'short_stop_pts': 40.0,
        'short_wiggle': 10.0,
        'short_target_1R': 40.0,
        'short_target_2R': 80.0,
        'short_target_3R': 120.0,
    },
}

# LSTM features (33 original + 6 regime = 39 total)
FEATURE_COLS = [
    # Price features
    'close_norm',
    'high_norm',
    'low_norm',
    'open_norm',
    'hl_range',
    'oc_range',

    # VWAP features
    'vwap_distance',
    'vwap_zscore',
    'above_vwap',
    'vwap_slope',
    'vwap_acceleration',

    # Volume features
    'volume_norm',
    'volume_surge',
    'volume_imbalance',
    'volume_price_trend',

    # Momentum indicators
    'rsi',
    'rsi_divergence',
    'stoch_k',
    'stoch_d',

    # Volatility
    'atr_norm',
    'bb_position',
    'bb_width',

    # Price action
    'higher_high',
    'lower_low',
    'price_velocity',
    'price_acceleration',

    # Mean reversion signals
    'mean_reversion_strength',
    'price_extreme_score',
    'reversion_probability',

    # Temporal
    'hour_sin',
    'hour_cos',
    'day_of_week',
    'session_time',

    # Regime features
    'adx_norm',
    'trend_strength_norm',
    'regime_ranging',
    'vwap_range_position',
    'bars_since_regime_change',
    'volatility_regime',
]

RAW_FILE_PATHS = {
    'ES': "/content/drive/MyDrive/Futures Data/ES_continuous_3T.csv",
    'NQ': "/content/drive/MyDrive/Futures Data/NQ_continuous_3T.csv",
    'RTY': "/content/drive/MyDrive/Futures Data/RTY_continuous_3T.csv",
    'YM': "/content/drive/MyDrive/Futures Data/YM_continuous_3T.csv",
}

print(f"‚úÖ VWAP Mean Reversion V2.1 - LSTM VERSION (RELAXED LABELING)")
print(f"üìä Sequence length: {SEQUENCE_LENGTH} bars ({SEQUENCE_LENGTH*3} min)")
print(f"üìä Prediction horizon: {PRIMARY_HORIZON} bars ({PRIMARY_HORIZON*3} min)")
print(f"üìä Features: {len(FEATURE_COLS)} (33 base + 6 regime)")
print(f"üéØ Strategy: Fade extremes from VWAP")
print(f"üéØ Quality Tiers: A+ (premium), A (high), B+ (good), C (acceptable)")
print(f"üÜï V2.1 Changes:")
print(f"   - RELAXED regime filter (ADX < 35, not 25)")
print(f"   - RELAXED quality gates (lower thresholds)")
print(f"   - WIDER percentile ranges (more candidates)")
print(f"   - ADDED C-tier for more signals")
print(f"   - TARGET: 25-35% signal rate (was 1-2%)")
print(f"üìÖ Split: Train < {SPLIT_DATE} | Test >= {SPLIT_DATE}")
print(f"üíæ Cache: {CACHE_DIR}\n")


# =========================================================
# FIRST-TOUCH LABELING WITH MIN HOLD
# =========================================================

@njit(parallel=True)
def first_touch_labeling_v2(
    high_vals, low_vals, close_vals,
    long_stop_levels, long_target_1R, long_target_2R, long_target_3R,
    short_stop_levels, short_target_1R, short_target_2R, short_target_3R,
    horizon, labels_length, min_hold_bars=5
):
    """
    First-touch labeling with MINIMUM HOLD TIME
    Filters noise and helps LSTM learn sustained moves
    """
    long_hit_1R = np.zeros(labels_length, dtype=np.bool_)
    long_hit_2R = np.zeros(labels_length, dtype=np.bool_)
    long_hit_3R = np.zeros(labels_length, dtype=np.bool_)
    long_stopped = np.zeros(labels_length, dtype=np.bool_)

    short_hit_1R = np.zeros(labels_length, dtype=np.bool_)
    short_hit_2R = np.zeros(labels_length, dtype=np.bool_)
    short_hit_3R = np.zeros(labels_length, dtype=np.bool_)
    short_stopped = np.zeros(labels_length, dtype=np.bool_)

    for i in prange(labels_length):
        # LONG analysis
        long_stop_bar = -1
        long_1R_bar = -1
        long_2R_bar = -1
        long_3R_bar = -1

        for j in range(horizon):
            idx = i + 1 + j
            if idx >= len(high_vals):
                break

            h = high_vals[idx]
            l = low_vals[idx]

            # Stop can hit anytime
            if long_stop_bar == -1 and l <= long_stop_levels[i]:
                long_stop_bar = j

            # Targets only valid AFTER minimum hold time
            if j >= min_hold_bars:
                if long_1R_bar == -1 and h >= long_target_1R[i]:
                    long_1R_bar = j
                if long_2R_bar == -1 and h >= long_target_2R[i]:
                    long_2R_bar = j
                if long_3R_bar == -1 and h >= long_target_3R[i]:
                    long_3R_bar = j

        if long_1R_bar != -1 and (long_stop_bar == -1 or long_1R_bar <= long_stop_bar):
            long_hit_1R[i] = True
            if long_2R_bar != -1 and (long_stop_bar == -1 or long_2R_bar <= long_stop_bar):
                long_hit_2R[i] = True
            if long_3R_bar != -1 and (long_stop_bar == -1 or long_3R_bar <= long_stop_bar):
                long_hit_3R[i] = True
        elif long_stop_bar != -1:
            long_stopped[i] = True

        # SHORT analysis
        short_stop_bar = -1
        short_1R_bar = -1
        short_2R_bar = -1
        short_3R_bar = -1

        for j in range(horizon):
            idx = i + 1 + j
            if idx >= len(high_vals):
                break

            h = high_vals[idx]
            l = low_vals[idx]

            if short_stop_bar == -1 and h >= short_stop_levels[i]:
                short_stop_bar = j

            if j >= min_hold_bars:
                if short_1R_bar == -1 and l <= short_target_1R[i]:
                    short_1R_bar = j
                if short_2R_bar == -1 and l <= short_target_2R[i]:
                    short_2R_bar = j
                if short_3R_bar == -1 and l <= short_target_3R[i]:
                    short_3R_bar = j

        if short_1R_bar != -1 and (short_stop_bar == -1 or short_1R_bar <= short_stop_bar):
            short_hit_1R[i] = True
            if short_2R_bar != -1 and (short_stop_bar == -1 or short_2R_bar <= short_stop_bar):
                short_hit_2R[i] = True
            if short_3R_bar != -1 and (short_stop_bar == -1 or short_3R_bar <= short_stop_bar):
                short_hit_3R[i] = True
        elif short_stop_bar != -1:
            short_stopped[i] = True

    return (long_hit_1R, long_hit_2R, long_hit_3R, long_stopped,
            short_hit_1R, short_hit_2R, short_hit_3R, short_stopped)


# =========================================================
# DATA LOADING
# =========================================================

def load_data(filename: str, ticker_name: str) -> pd.DataFrame:
    """Load base 3-minute data"""
    try:
        if not os.path.exists(filename):
            raise FileNotFoundError(f"File not found: {filename}")

        df = pd.read_csv(filename)
        df.rename(columns={
            'ts_event': 'timestamp', 'time': 'timestamp',
            'Volume': 'volume', 'Open': 'open', 'High': 'high',
            'Low': 'low', 'Close': 'close'
        }, inplace=True)

        df.columns = df.columns.str.lower()

        if 'timestamp' not in df.columns:
            raise ValueError("No timestamp column found")

        if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
            df['timestamp'] = pd.to_datetime(df['timestamp'])

        df.set_index('timestamp', inplace=True)
        df.sort_index(inplace=True)
        df = df[~df.index.duplicated(keep='first')]

        try:
            df.index = df.index.tz_localize('UTC').tz_convert('America/New_York')
        except TypeError:
            if df.index.tz is not None:
                df.index = df.index.tz_convert('America/New_York')

        df = df[['open', 'high', 'low', 'close', 'volume']].dropna()

        print(f"[{ticker_name}] üìä Data range: {df.index[0].date()} to {df.index[-1].date()}")
        print(f"[{ticker_name}] üìä Total bars: {len(df):,}")

        return df

    except Exception as e:
        print(f"[{ticker_name}] ‚ùå Load failed: {e}")
        return None


# =========================================================
# VWAP CALCULATION
# =========================================================

def calculate_session_vwap(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate VWAP that resets daily at market open (9:30 ET)"""
    df = df.copy()

    df['session'] = ((df.index.hour == 9) & (df.index.minute == 30)).astype(int).cumsum()
    df['cum_volume'] = df.groupby('session')['volume'].cumsum()
    df['cum_vp'] = df.groupby('session').apply(
        lambda x: (x['close'] * x['volume']).cumsum()
    ).droplevel(0)

    df['vwap'] = df['cum_vp'] / (df['cum_volume'] + 1e-6)

    df['vwap_dev'] = df.groupby('session').apply(
        lambda x: ((x['close'] - x['vwap']) ** 2 * x['volume']).cumsum() / (x['cum_volume'] + 1e-6)
    ).droplevel(0)
    df['vwap_std'] = np.sqrt(df['vwap_dev'])

    df.drop(columns=['session', 'cum_volume', 'cum_vp', 'vwap_dev'], inplace=True)

    return df


# =========================================================
# LSTM FEATURE ENGINEERING
# =========================================================

def create_lstm_features(df: pd.DataFrame, ticker_name: str) -> pd.DataFrame:
    """Create features optimized for LSTM (temporal patterns)"""
    df = df.copy()

    print(f"  üìä Creating LSTM-friendly features...")

    # Price normalization
    close_ma = df['close'].rolling(20, min_periods=10).mean()
    close_std = df['close'].rolling(20, min_periods=10).std()

    df['close_norm'] = (df['close'] - close_ma) / (close_std + 1e-6)
    df['high_norm'] = (df['high'] - close_ma) / (close_std + 1e-6)
    df['low_norm'] = (df['low'] - close_ma) / (close_std + 1e-6)
    df['open_norm'] = (df['open'] - close_ma) / (close_std + 1e-6)

    df['hl_range'] = (df['high'] - df['low']) / (close_ma + 1e-6)
    df['oc_range'] = (df['close'] - df['open']) / (close_ma + 1e-6)

    # VWAP features
    df = calculate_session_vwap(df)

    df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14)
    df['atr'] = df['atr'].ffill().fillna(1e-6)
    df['atr_norm'] = df['atr'] / (close_ma + 1e-6)

    # Safe VWAP distance (prevent extreme values)
    atr_safe = np.maximum(df['atr'], 0.5)
    df['vwap_distance'] = (df['close'] - df['vwap']) / atr_safe
    df['vwap_distance'] = df['vwap_distance'].clip(-10, 10)

    # Safe VWAP z-score
    vwap_std_safe = np.maximum(df['vwap_std'], 0.5)
    df['vwap_zscore'] = (df['close'] - df['vwap']) / vwap_std_safe
    df['vwap_zscore'] = df['vwap_zscore'].clip(-5, 5)

    df['above_vwap'] = (df['close'] > df['vwap']).astype(float)
    df['vwap_slope'] = df['vwap'].pct_change(5).fillna(0)
    df['vwap_acceleration'] = df['vwap_slope'].diff(3).fillna(0)

    # Volume features
    vol_ma = df['volume'].rolling(20, min_periods=10).mean()
    vol_std = df['volume'].rolling(20, min_periods=10).std()

    df['volume_norm'] = (df['volume'] - vol_ma) / (vol_std + 1e-6)
    df['volume_surge'] = df['volume'] / (vol_ma + 1e-6)

    df['vol_above'] = np.where(df['close'] > df['vwap'], df['volume'], 0)
    df['vol_below'] = np.where(df['close'] <= df['vwap'], df['volume'], 0)

    vol_above_sum = df['vol_above'].rolling(20, min_periods=10).sum()
    vol_below_sum = df['vol_below'].rolling(20, min_periods=10).sum()

    df['volume_imbalance'] = (vol_above_sum - vol_below_sum) / (vol_above_sum + vol_below_sum + 1e-6)

    price_change = df['close'].pct_change(5)
    volume_change = df['volume'].pct_change(5)
    df['volume_price_trend'] = price_change * volume_change

    # Momentum indicators
    df['rsi'] = ta.rsi(df['close'], length=14) / 100.0

    price_high_20 = df['close'].rolling(20).max()
    rsi_high_20 = df['rsi'].rolling(20).max()
    df['rsi_divergence'] = (df['close'] == price_high_20).astype(float) - (df['rsi'] == rsi_high_20).astype(float)

    stoch = ta.stoch(df['high'], df['low'], df['close'], k=14, d=3)
    df['stoch_k'] = stoch['STOCHk_14_3_3'] / 100.0
    df['stoch_d'] = stoch['STOCHd_14_3_3'] / 100.0

    # Volatility
    bb = ta.bbands(df['close'], length=20, std=2)
    df['bb_upper'] = bb['BBU_20_2.0_2.0']
    df['bb_lower'] = bb['BBL_20_2.0_2.0']
    df['bb_mid'] = bb['BBM_20_2.0_2.0']

    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-6)
    df['bb_position'] = df['bb_position'].clip(0, 1)

    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / (df['bb_mid'] + 1e-6)

    # Price action
    df['higher_high'] = (df['high'] > df['high'].shift(1)).astype(float)
    df['lower_low'] = (df['low'] < df['low'].shift(1)).astype(float)

    df['price_velocity'] = df['close'].pct_change(3)
    df['price_acceleration'] = df['price_velocity'].diff(2)

    # Mean reversion signals
    df['mean_reversion_strength'] = 1.0 - abs(df['vwap_slope']).clip(0, 1)
    df['price_extreme_score'] = abs(df['vwap_zscore'])

    df['reversion_probability'] = (
        (df['price_extreme_score'] > 1.0).astype(float) * 0.4 +
        (df['volume_surge'] > 1.2).astype(float) * 0.3 +
        (df['mean_reversion_strength'] > 0.7).astype(float) * 0.3
    )

    # Temporal features
    df['hour'] = df.index.hour
    df['minute'] = df.index.minute
    df['day_of_week'] = df.index.dayofweek / 4.0

    df['session_time'] = (
        np.where(df['hour'] < 11, 0,
        np.where(df['hour'] < 14, 1, 2))
    ).astype(float) / 2.0

    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)

    # Cleanup
    df.drop(columns=[
        'vwap', 'vwap_std', 'atr', 'vol_above', 'vol_below',
        'bb_upper', 'bb_lower', 'bb_mid', 'hour', 'minute'
    ], inplace=True, errors='ignore')

    # Fill NaNs
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.ffill(inplace=True)
    df.fillna(0, inplace=True)

    # Clip all features
    for col in df.columns:
        if col in ['open', 'high', 'low', 'close', 'volume']:
            continue
        df[col] = df[col].clip(-10, 10)

    # üîß FINAL VALIDATION: Check for any remaining NaN
    nan_count = df.isna().sum().sum()
    if nan_count > 0:
        print(f"  ‚ö†Ô∏è  WARNING: Found {nan_count:,} NaN values after feature creation")
        nan_cols = df.columns[df.isna().any()].tolist()
        print(f"      Columns with NaN: {nan_cols}")
        print(f"      Filling...")

        df.ffill(inplace=True)
        df.bfill(inplace=True)
        df.fillna(0, inplace=True)

        # Verify
        final_nan = df.isna().sum().sum()
        if final_nan > 0:
            raise ValueError(f"ERROR: Still have {final_nan:,} NaN after cleanup!")
        print(f"  ‚úÖ All NaN filled")
    else:
        print(f"  ‚úÖ No NaN detected")

    print(f"  ‚úÖ Created 33 base LSTM features")

    return df


# =========================================================
# REGIME-AWARE FEATURES
# =========================================================

def add_regime_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add regime-aware features for LSTM (helps model understand context)"""
    print(f"  üìä Adding regime-aware features...")

    # ADX (already calculated in calculate_market_regime)
    df['adx_norm'] = df['adx'] / 50.0
    df['adx_norm'] = df['adx_norm'].clip(0, 1)

    # Trend strength (already calculated)
    df['trend_strength_norm'] = df['trend_strength'].clip(0, 3) / 3.0

    # Ranging regime binary
    df['regime_ranging'] = df['is_ranging'].astype(float)

    # VWAP position in recent range
    range_high = df['high'].rolling(100).max()
    range_low = df['low'].rolling(100).min()

    # Need to recalculate VWAP for this
    df_temp = calculate_session_vwap(df[['open', 'high', 'low', 'close', 'volume']].copy())
    vwap_for_range = df_temp['vwap']

    df['vwap_range_position'] = (vwap_for_range - range_low) / (range_high - range_low + 1e-6)
    df['vwap_range_position'] = df['vwap_range_position'].clip(0, 1)

    # Time since regime change
    regime_change = df['is_ranging'].astype(int).diff().abs()
    df['bars_since_regime_change'] = 0

    counter = 0
    bars_list = []
    for val in regime_change:
        if val == 1:
            counter = 0
        else:
            counter += 1
        bars_list.append(counter)

    df['bars_since_regime_change'] = bars_list
    df['bars_since_regime_change'] = (df['bars_since_regime_change'] / 100.0).clip(0, 1)

    # Volatility regime
    atr = ta.atr(df['high'], df['low'], df['close'], length=14)
    atr_ma = atr.rolling(100).mean()
    df['volatility_regime'] = (atr / (atr_ma + 1e-6)).clip(0, 3) / 3.0

    # üîß CRITICAL: Fill NaN in all regime feature columns
    regime_feature_cols = [
        'adx_norm', 'trend_strength_norm', 'regime_ranging',
        'vwap_range_position', 'bars_since_regime_change', 'volatility_regime'
    ]

    for col in regime_feature_cols:
        if col in df.columns:
            df[col] = df[col].ffill().bfill().fillna(0.0)

    print(f"  ‚úÖ Added 6 regime-aware features (NaN filled)")

    return df


# =========================================================
# üÜï V2.1: RELAXED MARKET REGIME DETECTION
# =========================================================

def calculate_market_regime(df: pd.DataFrame) -> pd.DataFrame:
    """Identify ranging vs trending markets - RELAXED VERSION"""
    print(f"  üìä Calculating market regime (RELAXED)...")

    # ADX (trend strength)
    adx_data = ta.adx(df['high'], df['low'], df['close'], length=14)
    df['adx'] = adx_data['ADX_14']

    # Trend strength (EMA distance)
    ema_fast = df['close'].ewm(span=20, adjust=False).mean()
    ema_slow = df['close'].ewm(span=50, adjust=False).mean()

    atr = ta.atr(df['high'], df['low'], df['close'], length=14)
    df['trend_strength'] = abs(ema_fast - ema_slow) / (atr + 1e-6)

    # VWAP oscillation (price flips around VWAP)
    df_temp = calculate_session_vwap(df[['open', 'high', 'low', 'close', 'volume']].copy())
    above_vwap = (df['close'] > df_temp['vwap']).astype(int)
    vwap_flips = above_vwap.diff().abs().rolling(20).sum()

    # üÜï RELAXED ranging criteria (was ADX < 25, now ADX < 35)
    df['is_ranging'] = (
        (df['adx'] < 35) &  # RELAXED from 25
        (df['trend_strength'] < 2.0) &  # RELAXED from 1.5
        (vwap_flips > 3)  # RELAXED from 5
    )

    ranging_pct = df['is_ranging'].sum() / len(df) * 100
    print(f"  ‚úÖ Ranging regime: {ranging_pct:.1f}% of bars (RELAXED criteria)")

    # üîß CRITICAL: Fill NaN from ADX/trend calculations
    df['adx'] = df['adx'].bfill().fillna(25.0)  # Default to mid-range ADX
    df['trend_strength'] = df['trend_strength'].bfill().fillna(0.0)

    # is_ranging is bool, fill with True (assume ranging at start)
    df['is_ranging'].fillna(True, inplace=True)

    print(f"  üîß Filled NaN in regime columns")

    return df


# =========================================================
# üÜï V2.1: WIDER PERCENTILE THRESHOLDS
# =========================================================

def calculate_dynamic_thresholds(df: pd.DataFrame, window: int = 500) -> pd.DataFrame:
    """Calculate adaptive thresholds - WIDER RANGES for more signals"""
    print(f"  üìä Calculating dynamic thresholds (WIDER RANGES)...")

    # Work only with ranging bars
    ranging_mask = df['is_ranging']

    # Initialize
    df['long_threshold_a_plus'] = np.nan
    df['long_threshold_a'] = np.nan
    df['long_threshold_b_plus'] = np.nan
    df['long_threshold_c'] = np.nan  # NEW

    df['short_threshold_a_plus'] = np.nan
    df['short_threshold_a'] = np.nan
    df['short_threshold_b_plus'] = np.nan
    df['short_threshold_c'] = np.nan  # NEW

    # Calculate percentiles on ranging bars only
    zscore_ranging = df['vwap_zscore'].copy()
    zscore_ranging[~ranging_mask] = np.nan

    # üÜï WIDER percentile ranges (was 10/20/30, now 5/15/25/35)
    df['long_threshold_a_plus'] = zscore_ranging.rolling(window, min_periods=100).quantile(0.05)  # Was 0.10
    df['long_threshold_a'] = zscore_ranging.rolling(window, min_periods=100).quantile(0.15)  # Was 0.20
    df['long_threshold_b_plus'] = zscore_ranging.rolling(window, min_periods=100).quantile(0.25)  # Was 0.30
    df['long_threshold_c'] = zscore_ranging.rolling(window, min_periods=100).quantile(0.35)  # NEW

    df['short_threshold_a_plus'] = zscore_ranging.rolling(window, min_periods=100).quantile(0.95)  # Was 0.90
    df['short_threshold_a'] = zscore_ranging.rolling(window, min_periods=100).quantile(0.85)  # Was 0.80
    df['short_threshold_b_plus'] = zscore_ranging.rolling(window, min_periods=100).quantile(0.75)  # Was 0.70
    df['short_threshold_c'] = zscore_ranging.rolling(window, min_periods=100).quantile(0.65)  # NEW

    # Forward fill NaNs
    threshold_cols = [
        'long_threshold_a_plus', 'long_threshold_a', 'long_threshold_b_plus', 'long_threshold_c',
        'short_threshold_a_plus', 'short_threshold_a', 'short_threshold_b_plus', 'short_threshold_c'
    ]
    df[threshold_cols] = df[threshold_cols].ffill()

    # Fallback values
    df['long_threshold_a_plus'].fillna(-1.5, inplace=True)  # Wider
    df['long_threshold_a'].fillna(-1.0, inplace=True)
    df['long_threshold_b_plus'].fillna(-0.6, inplace=True)
    df['long_threshold_c'].fillna(-0.3, inplace=True)  # NEW

    df['short_threshold_a_plus'].fillna(1.5, inplace=True)  # Wider
    df['short_threshold_a'].fillna(1.0, inplace=True)
    df['short_threshold_b_plus'].fillna(0.6, inplace=True)
    df['short_threshold_c'].fillna(0.3, inplace=True)  # NEW

    print(f"  ‚úÖ Dynamic thresholds calculated (WIDER)")

    return df


# =========================================================
# SYMMETRIC LABEL BALANCING
# =========================================================

def ensure_symmetric_labels(long_gate, short_gate, target_ratio=1.0, tolerance=0.3):
    """Force BUY:SELL ratio to stay balanced"""
    long_count = long_gate.sum()
    short_count = short_gate.sum()

    if short_count == 0:
        return long_gate, short_gate

    ratio = long_count / short_count

    print(f"  ‚öñÔ∏è  Label balancing:")
    print(f"     Before: BUY={long_count:,}, SELL={short_count:,}, Ratio={ratio:.2f}:1")

    min_ratio = target_ratio * (1 - tolerance)
    max_ratio = target_ratio * (1 + tolerance)

    if ratio > max_ratio:
        # Too many longs
        np.random.seed(42)
        keep_count = int(short_count * max_ratio)
        long_indices = np.where(long_gate)[0]
        keep_indices = np.random.choice(long_indices, size=keep_count, replace=False)

        new_long_gate = np.zeros_like(long_gate)
        new_long_gate[keep_indices] = True
        long_gate = new_long_gate

    elif ratio < min_ratio:
        # Too many shorts
        np.random.seed(42)
        keep_count = int(long_count / min_ratio)
        short_indices = np.where(short_gate)[0]
        keep_indices = np.random.choice(short_indices, size=keep_count, replace=False)

        new_short_gate = np.zeros_like(short_gate)
        new_short_gate[keep_indices] = True
        short_gate = new_short_gate

    final_ratio = long_gate.sum() / (short_gate.sum() + 1e-6)
    print(f"     After:  BUY={long_gate.sum():,}, SELL={short_gate.sum():,}, Ratio={final_ratio:.2f}:1")

    return long_gate, short_gate


# =========================================================
# OUTCOME-BASED QUALITY SCORING
# =========================================================

def calculate_outcome_based_quality(
    labels, hit_1R, hit_2R, hit_3R, stopped,
    vwap_zscore, volume_surge
):
    """Quality based on actual outcomes, not arbitrary features"""
    quality = np.zeros(len(labels), dtype=np.float32)

    for i in range(len(labels)):
        if labels[i] == 0:
            continue

        # Base quality from outcome
        if hit_3R[i]:
            base_quality = 1.0
        elif hit_2R[i]:
            base_quality = 0.85
        elif hit_1R[i]:
            base_quality = 0.65
        else:
            base_quality = 0.0

        # Small bonus for strong setups
        setup_bonus = 0.0
        setup_bonus += min(abs(vwap_zscore[i]) / 5.0, 0.05)
        setup_bonus += min((volume_surge[i] - 0.5) / 10.0, 0.05)

        quality[i] = min(base_quality + setup_bonus, 1.0)

    return quality


# =========================================================
# üÜï V2.1: RELAXED LABELING FUNCTION
# =========================================================

def label_mean_reversion_v2_1(
    df: pd.DataFrame,
    ticker_name: str,
    horizon: int,
    params: dict,
    min_hold_bars: int = 5,
    enable_regime_filter: bool = True,
    enable_dynamic_thresholds: bool = True,
    enable_symmetric_balance: bool = True,
    target_ratio: float = 1.0,
) -> pd.DataFrame:
    """
    RELAXED MEAN REVERSION LABELING V2.1

    Changes from V2.0:
    1. RELAXED regime filtering (ADX < 35 not 25)
    2. RELAXED quality gates (lower thresholds)
    3. WIDER percentile ranges (more candidates)
    4. ADDED C-tier for additional signals
    5. Regime filter only applied to A+ tier
    """

    print(f"\n{'='*70}")
    print(f"[{ticker_name}] üéØ RELAXED MEAN REVERSION LABELING V2.1")
    print(f"{'='*70}")
    print(f"Horizon: {horizon} bars ({horizon*3} minutes)")
    print(f"Min hold: {min_hold_bars} bars ({min_hold_bars*3} minutes)")
    print(f"\nüîß V2.1 Improvements:")
    print(f"  - RELAXED regime (ADX < 35, was 25)")
    print(f"  - RELAXED quality gates")
    print(f"  - WIDER percentiles (5-35%, was 10-30%)")
    print(f"  - ADDED C-tier for more signals")
    print(f"  - TARGET: 25-35% signal rate")

    print(f"\nüìà LONG Parameters:")
    print(f"  Stop: {params['long_stop_pts']}pts + {params['long_wiggle']}pt wiggle")
    print(f"  Targets: {params['long_target_1R']}/{params['long_target_2R']}/{params['long_target_3R']}pts")

    print(f"\nüìâ SHORT Parameters:")
    print(f"  Stop: {params['short_stop_pts']}pts + {params['short_wiggle']}pt wiggle")
    print(f"  Targets: {params['short_target_1R']}/{params['short_target_2R']}/{params['short_target_3R']}pts")

    # STEP 1: Regime detection (RELAXED)
    if enable_regime_filter:
        df = calculate_market_regime(df)
    else:
        df['is_ranging'] = True
        df['adx'] = 0.0
        df['trend_strength'] = 0.0

    # STEP 2: Dynamic thresholds (WIDER)
    if enable_dynamic_thresholds:
        df = calculate_dynamic_thresholds(df, window=500)
    else:
        df['long_threshold_a_plus'] = -1.5
        df['long_threshold_a'] = -1.0
        df['long_threshold_b_plus'] = -0.6
        df['long_threshold_c'] = -0.3
        df['short_threshold_a_plus'] = 1.5
        df['short_threshold_a'] = 1.0
        df['short_threshold_b_plus'] = 0.6
        df['short_threshold_c'] = 0.3

    # STEP 3: First-touch labeling
    print(f"\n  ‚è≥ Running first-touch labeling (min hold = {min_hold_bars} bars)...")

    close_vals = df['close'].values.astype(np.float64)
    high_vals = df['high'].values.astype(np.float64)
    low_vals = df['low'].values.astype(np.float64)

    labels_length = len(df) - horizon - 1
    close_entry = close_vals[0:labels_length]

    # Calculate levels
    long_stop_levels = close_entry - params['long_stop_pts'] - params['long_wiggle']
    long_target_1R = close_entry + params['long_target_1R']
    long_target_2R = close_entry + params['long_target_2R']
    long_target_3R = close_entry + params['long_target_3R']

    short_stop_levels = close_entry + params['short_stop_pts'] + params['short_wiggle']
    short_target_1R = close_entry - params['short_target_1R']
    short_target_2R = close_entry - params['short_target_2R']
    short_target_3R = close_entry - params['short_target_3R']

    # Run labeling
    (long_hit_1R, long_hit_2R, long_hit_3R, long_stopped,
     short_hit_1R, short_hit_2R, short_hit_3R, short_stopped) = first_touch_labeling_v2(
        high_vals, low_vals, close_vals,
        long_stop_levels, long_target_1R, long_target_2R, long_target_3R,
        short_stop_levels, short_target_1R, short_target_2R, short_target_3R,
        horizon, labels_length, min_hold_bars
    )

    # STEP 4: RELAXED quality gates
    print(f"\n  üìä Applying RELAXED quality gates...")

    vwap_zscore = df['vwap_zscore'].values[0:labels_length]
    volume_surge = df['volume_surge'].values[0:labels_length]
    price_extreme = df['price_extreme_score'].values[0:labels_length]
    is_ranging = df['is_ranging'].values[0:labels_length]

    # Get dynamic thresholds
    long_thresh_a_plus = df['long_threshold_a_plus'].values[0:labels_length]
    long_thresh_a = df['long_threshold_a'].values[0:labels_length]
    long_thresh_b_plus = df['long_threshold_b_plus'].values[0:labels_length]
    long_thresh_c = df['long_threshold_c'].values[0:labels_length]  # NEW

    short_thresh_a_plus = df['short_threshold_a_plus'].values[0:labels_length]
    short_thresh_a = df['short_threshold_a'].values[0:labels_length]
    short_thresh_b_plus = df['short_threshold_b_plus'].values[0:labels_length]
    short_thresh_c = df['short_threshold_c'].values[0:labels_length]  # NEW

    # üÜï RELAXED A+ Tier (regime filter ONLY on A+)
    long_gate_a_plus = (
        (vwap_zscore < long_thresh_a_plus) &
        (volume_surge > 0.8) &  # RELAXED from 1.0
        (price_extreme > 0.8) &  # RELAXED from 1.0
        is_ranging  # Only A+ requires ranging
    )
    short_gate_a_plus = (
        (vwap_zscore > short_thresh_a_plus) &
        (volume_surge > 0.8) &  # RELAXED from 1.0
        (price_extreme > 0.8) &  # RELAXED from 1.0
        is_ranging  # Only A+ requires ranging
    )

    # üÜï RELAXED A Tier (NO regime filter)
    long_gate_a = (
        (vwap_zscore < long_thresh_a) &
        (volume_surge > 0.5) &  # RELAXED from 0.8
        (price_extreme > 0.5) &  # RELAXED from 0.75
        ~long_gate_a_plus
    )
    short_gate_a = (
        (vwap_zscore > short_thresh_a) &
        (volume_surge > 0.5) &  # RELAXED from 0.8
        (price_extreme > 0.5) &  # RELAXED from 0.75
        ~short_gate_a_plus
    )

    # üÜï RELAXED B+ Tier (NO regime filter)
    long_gate_b_plus = (
        (vwap_zscore < long_thresh_b_plus) &
        (volume_surge > 0.3) &  # RELAXED from 0.6
        (price_extreme > 0.3) &  # RELAXED from 0.5
        ~long_gate_a_plus &
        ~long_gate_a
    )
    short_gate_b_plus = (
        (vwap_zscore > short_thresh_b_plus) &
        (volume_surge > 0.3) &  # RELAXED from 0.6
        (price_extreme > 0.3) &  # RELAXED from 0.5
        ~short_gate_a_plus &
        ~short_gate_a
    )

    # üÜï NEW C Tier (NO regime filter, minimal requirements)
    long_gate_c = (
        (vwap_zscore < long_thresh_c) &
        (volume_surge > 0.0) &  # No minimum
        (price_extreme > 0.0) &  # No minimum
        ~long_gate_a_plus &
        ~long_gate_a &
        ~long_gate_b_plus
    )
    short_gate_c = (
        (vwap_zscore > short_thresh_c) &
        (volume_surge > 0.0) &  # No minimum
        (price_extreme > 0.0) &  # No minimum
        ~short_gate_a_plus &
        ~short_gate_a &
        ~short_gate_b_plus
    )

    # Combine tiers
    long_gate = long_gate_a_plus | long_gate_a | long_gate_b_plus | long_gate_c
    short_gate = short_gate_a_plus | short_gate_a | short_gate_b_plus | short_gate_c

    # Apply gates
    long_hit_1R_gated = long_hit_1R & long_gate
    short_hit_1R_gated = short_hit_1R & short_gate

    # STEP 5: Symmetric balancing
    if enable_symmetric_balance:
        long_hit_1R_gated, short_hit_1R_gated = ensure_symmetric_labels(
            long_hit_1R_gated, short_hit_1R_gated,
            target_ratio=target_ratio, tolerance=0.3
        )

    # STEP 6: Create labels
    labels = np.zeros(labels_length, dtype=np.int8)
    labels[long_hit_1R_gated] = 1
    labels[short_hit_1R_gated] = 2

    conflicts = long_hit_1R_gated & short_hit_1R_gated
    if conflicts.sum() > 0:
        print(f"  ‚ö†Ô∏è  {conflicts.sum()} conflicts - set to HOLD")
        labels[conflicts] = 0

    # STEP 7: Outcome-based quality
    print(f"\n  üéØ Calculating outcome-based quality scores...")

    long_2R_for_quality = long_hit_2R & long_hit_1R_gated
    long_3R_for_quality = long_hit_3R & long_hit_1R_gated
    long_stopped_for_quality = long_stopped & ~long_hit_1R_gated

    short_2R_for_quality = short_hit_2R & short_hit_1R_gated
    short_3R_for_quality = short_hit_3R & short_hit_1R_gated
    short_stopped_for_quality = short_stopped & ~short_hit_1R_gated

    hit_1R_combined = long_hit_1R_gated | short_hit_1R_gated
    hit_2R_combined = long_2R_for_quality | short_2R_for_quality
    hit_3R_combined = long_3R_for_quality | short_3R_for_quality
    stopped_combined = long_stopped_for_quality | short_stopped_for_quality

    setup_quality = calculate_outcome_based_quality(
        labels, hit_1R_combined, hit_2R_combined, hit_3R_combined,
        stopped_combined, vwap_zscore, volume_surge
    )

    # üÜï STEP 7.5: Assign quality TIERS for multi-task learning
    # This lets the LSTM learn what makes A+ vs C trades!
    print(f"\n  üéì Assigning quality tiers for multi-task learning...")

    quality_tier = np.zeros(labels_length, dtype=np.int8)  # 0 = no signal

    # For LONG signals
    long_a_plus_mask = long_gate_a_plus & long_hit_1R_gated
    long_a_mask = long_gate_a & long_hit_1R_gated
    long_b_plus_mask = long_gate_b_plus & long_hit_1R_gated
    long_c_mask = long_gate_c & long_hit_1R_gated

    quality_tier[long_a_plus_mask] = 4  # A+ = 4
    quality_tier[long_a_mask] = 3       # A = 3
    quality_tier[long_b_plus_mask] = 2  # B+ = 2
    quality_tier[long_c_mask] = 1       # C = 1

    # For SHORT signals
    short_a_plus_mask = short_gate_a_plus & short_hit_1R_gated
    short_a_mask = short_gate_a & short_hit_1R_gated
    short_b_plus_mask = short_gate_b_plus & short_hit_1R_gated
    short_c_mask = short_gate_c & short_hit_1R_gated

    quality_tier[short_a_plus_mask] = 4  # A+ = 4
    quality_tier[short_a_mask] = 3       # A = 3
    quality_tier[short_b_plus_mask] = 2  # B+ = 2
    quality_tier[short_c_mask] = 1       # C = 1

    # HOLD remains 0

    print(f"  ‚úÖ Tier distribution:")
    print(f"     A+ (4): {(quality_tier == 4).sum():,} signals")
    print(f"     A  (3): {(quality_tier == 3).sum():,} signals")
    print(f"     B+ (2): {(quality_tier == 2).sum():,} signals")
    print(f"     C  (1): {(quality_tier == 1).sum():,} signals")
    print(f"     HOLD (0): {(quality_tier == 0).sum():,} bars")

    # STEP 8: Store results
    df[f'label_{horizon}bar'] = pd.Series(labels, index=df.index[0:labels_length])
    df[f'setup_quality_{horizon}bar'] = pd.Series(setup_quality, index=df.index[0:labels_length])
    df[f'quality_tier_{horizon}bar'] = pd.Series(quality_tier, index=df.index[0:labels_length])

    df[f'hit_1R_{horizon}bar'] = pd.Series(
        np.where(long_hit_1R_gated, 1, np.where(short_hit_1R_gated, -1, 0)),
        index=df.index[0:labels_length]
    )
    df[f'hit_2R_{horizon}bar'] = pd.Series(
        np.where(long_2R_for_quality, 1, np.where(short_2R_for_quality, -1, 0)),
        index=df.index[0:labels_length]
    )
    df[f'hit_3R_{horizon}bar'] = pd.Series(
        np.where(long_3R_for_quality, 1, np.where(short_3R_for_quality, -1, 0)),
        index=df.index[0:labels_length]
    )

    df[f'long_stopped_{horizon}bar'] = pd.Series(long_stopped.astype(np.int8), index=df.index[0:labels_length])
    df[f'short_stopped_{horizon}bar'] = pd.Series(short_stopped.astype(np.int8), index=df.index[0:labels_length])

    # STEP 9: Statistics
    print(f"\n{'='*70}")
    print(f"üìä LABELING RESULTS (RELAXED)")
    print(f"{'='*70}")

    # Tier breakdown
    long_a_plus_count = (long_gate_a_plus & long_hit_1R_gated).sum()
    long_a_count = (long_gate_a & long_hit_1R_gated).sum()
    long_b_plus_count = (long_gate_b_plus & long_hit_1R_gated).sum()
    long_c_count = (long_gate_c & long_hit_1R_gated).sum()  # NEW

    short_a_plus_count = (short_gate_a_plus & short_hit_1R_gated).sum()
    short_a_count = (short_gate_a & short_hit_1R_gated).sum()
    short_b_plus_count = (short_gate_b_plus & short_hit_1R_gated).sum()
    short_c_count = (short_gate_c & short_hit_1R_gated).sum()  # NEW

    print(f"\nüìà BUY SIGNALS BY TIER:")
    print(f"  A+ : {long_a_plus_count:,} ({long_a_plus_count/labels_length*100:.1f}%)")
    print(f"  A  : {long_a_count:,} ({long_a_count/labels_length*100:.1f}%)")
    print(f"  B+ : {long_b_plus_count:,} ({long_b_plus_count/labels_length*100:.1f}%)")
    print(f"  C  : {long_c_count:,} ({long_c_count/labels_length*100:.1f}%)")  # NEW

    print(f"\nüìâ SELL SIGNALS BY TIER:")
    print(f"  A+ : {short_a_plus_count:,} ({short_a_plus_count/labels_length*100:.1f}%)")
    print(f"  A  : {short_a_count:,} ({short_a_count/labels_length*100:.1f}%)")
    print(f"  B+ : {short_b_plus_count:,} ({short_b_plus_count/labels_length*100:.1f}%)")
    print(f"  C  : {short_c_count:,} ({short_c_count/labels_length*100:.1f}%)")  # NEW

    counts = Counter(labels)
    print(f"\nüìä OVERALL DISTRIBUTION:")
    print(f"  HOLD: {counts[0]:,} ({counts[0]/labels_length*100:.1f}%)")
    print(f"  BUY:  {counts[1]:,} ({counts[1]/labels_length*100:.1f}%)")
    print(f"  SELL: {counts[2]:,} ({counts[2]/labels_length*100:.1f}%)")
    print(f"  TOTAL SIGNALS: {(counts[1] + counts[2]):,} ({(counts[1] + counts[2])/labels_length*100:.1f}%)")  # NEW

    if counts[1] > 0 and counts[2] > 0:
        ratio = counts[1] / counts[2]
        print(f"  BUY:SELL Ratio: {ratio:.2f}:1")

    if counts[1] > 0:
        buy_2R_rate = long_2R_for_quality.sum() / counts[1] * 100
        buy_3R_rate = long_3R_for_quality.sum() / counts[1] * 100
        buy_quality = setup_quality[labels == 1].mean()

        print(f"\nüìà BUY PERFORMANCE:")
        print(f"  Signals: {counts[1]:,}")
        print(f"  Hit 2R: {long_2R_for_quality.sum():,} ({buy_2R_rate:.1f}%)")
        print(f"  Hit 3R: {long_3R_for_quality.sum():,} ({buy_3R_rate:.1f}%)")
        print(f"  Quality: {buy_quality:.3f}")

    if counts[2] > 0:
        sell_2R_rate = short_2R_for_quality.sum() / counts[2] * 100
        sell_3R_rate = short_3R_for_quality.sum() / counts[2] * 100
        sell_quality = setup_quality[labels == 2].mean()

        print(f"\nüìâ SELL PERFORMANCE:")
        print(f"  Signals: {counts[2]:,}")
        print(f"  Hit 2R: {short_2R_for_quality.sum():,} ({sell_2R_rate:.1f}%)")
        print(f"  Hit 3R: {short_3R_for_quality.sum():,} ({sell_3R_rate:.1f}%)")
        print(f"  Quality: {sell_quality:.3f}")

    print(f"\nüõë STOP-OUT RATES:")
    print(f"  Long:  {long_stopped.sum():,} ({long_stopped.sum()/labels_length*100:.1f}%)")
    print(f"  Short: {short_stopped.sum():,} ({short_stopped.sum()/labels_length*100:.1f}%)")

    # Fill NaNs
    fill_cols = [c for c in df.columns if c.endswith(f'_{horizon}bar')]
    df[fill_cols] = df[fill_cols].fillna(0)

    print(f"\n{'='*70}")

    return df


# =========================================================
# SEQUENCE CREATION FOR LSTM
# =========================================================

def create_sequences(df: pd.DataFrame, seq_length: int, feature_cols: list) -> dict:
    """Create sequences for LSTM training with quality tier labels"""
    print(f"\n  üì¶ Creating sequences (length={seq_length})...")

    # üîß CRITICAL: Validate no NaN in feature columns
    feature_data_df = df[feature_cols]
    nan_cols = feature_data_df.columns[feature_data_df.isna().any()].tolist()

    if nan_cols:
        print(f"  ‚ö†Ô∏è  WARNING: Found NaN in features: {nan_cols}")
        print(f"      Filling with forward/backward fill...")

        for col in nan_cols:
            df[col] = df[col].ffill().bfill().fillna(0.0)

        # Re-check
        nan_count = df[feature_cols].isna().sum().sum()
        if nan_count > 0:
            print(f"  ‚ùå ERROR: Still have {nan_count:,} NaN after fill!")
            raise ValueError("Cannot create sequences with NaN values")
        else:
            print(f"  ‚úÖ All NaN filled")

    feature_data = df[feature_cols].values

    label_col = f'label_{PRIMARY_HORIZON}bar'
    quality_col = f'setup_quality_{PRIMARY_HORIZON}bar'
    tier_col = f'quality_tier_{PRIMARY_HORIZON}bar'

    labels = df[label_col].values
    quality = df[quality_col].values
    tiers = df[tier_col].values

    n_samples = len(df) - seq_length

    X = np.zeros((n_samples, seq_length, len(feature_cols)), dtype=np.float32)
    y = np.zeros(n_samples, dtype=np.int8)
    q = np.zeros(n_samples, dtype=np.float32)
    t = np.zeros(n_samples, dtype=np.int8)  # tier labels

    timestamps = []
    close_prices = []

    valid_count = 0

    for i in range(n_samples):
        seq_idx = i + seq_length

        if pd.isna(labels[seq_idx]) or labels[seq_idx] < 0:
            continue

        X[valid_count] = feature_data[i:seq_idx]
        y[valid_count] = labels[seq_idx]
        q[valid_count] = quality[seq_idx]
        t[valid_count] = tiers[seq_idx]

        timestamps.append(df.index[seq_idx])
        close_prices.append(df['close'].iloc[seq_idx])

        valid_count += 1

    X = X[:valid_count]
    y = y[:valid_count]
    q = q[:valid_count]
    t = t[:valid_count]

    print(f"  ‚úÖ Created {valid_count:,} sequences")
    print(f"     Shape: {X.shape}")
    print(f"     Labels: {Counter(y)}")
    print(f"     Tiers: {Counter(t)}")

    return {
        'X': X,
        'y': y,
        'quality': q,
        'tier': t,
        'timestamps': timestamps,
        'close_prices': close_prices,
        'feature_cols': feature_cols,
    }


# =========================================================
# MAIN PROCESSING
# =========================================================

def process_ticker(ticker_name: str) -> None:
    """Process one ticker with V2.1 RELAXED improvements"""

    print(f"\n{'='*70}")
    print(f"[{ticker_name}] üöÄ PROCESSING WITH V2.1 RELAXED LABELING")
    print(f"{'='*70}")

    try:
        train_path = os.path.join(CACHE_DIR, f"{ticker_name}_TRAIN_VAL.npz")
        test_path = os.path.join(CACHE_DIR, f"{ticker_name}_TEST.npz")

        if os.path.exists(train_path) and os.path.exists(test_path):
            print(f"[{ticker_name}] ‚ö†Ô∏è  CACHE EXISTS - DELETE TO REGENERATE")
            return

        print(f"[{ticker_name}] ‚è≥ Loading data...")
        df = load_data(RAW_FILE_PATHS[ticker_name], ticker_name)

        if df is None:
            return

        split_dt = pd.Timestamp(SPLIT_DATE, tz='America/New_York')

        print(f"\n[{ticker_name}] üîí Date-based split at {SPLIT_DATE}...")

        df_train = df[df.index < split_dt].copy()
        df_test = df[df.index >= split_dt].copy()

        print(f"  Train: {len(df_train):,} bars")
        print(f"         {df_train.index[0].date()} to {df_train.index[-1].date()}")
        print(f"  Test:  {len(df_test):,} bars")
        print(f"         {df_test.index[0].date()} to {df_test.index[-1].date()}")

        train_days = (df_train.index[-1] - df_train.index[0]).days
        test_days = (df_test.index[-1] - df_test.index[0]).days
        print(f"\n  Train: {train_days} days ({train_days/252:.1f} years)")
        print(f"  Test:  {test_days} days ({test_days/252:.1f} years)")

        # Create base LSTM features
        print(f"\n[{ticker_name}] ‚è≥ Creating TRAIN base features...")
        df_train = create_lstm_features(df_train, ticker_name)

        print(f"\n[{ticker_name}] ‚è≥ Creating TEST base features...")
        df_test = create_lstm_features(df_test, ticker_name)

        params = MARKET_PARAMS[ticker_name]

        # üÜï V2.1: RELAXED labeling
        print(f"\n[{ticker_name}] üè∑Ô∏è  Labeling TRAIN (V2.1 RELAXED)...")
        df_train = label_mean_reversion_v2_1(
            df_train,
            f"{ticker_name}_TRAIN",
            PRIMARY_HORIZON,
            params,
            min_hold_bars=5,
            enable_regime_filter=True,
            enable_dynamic_thresholds=True,
            enable_symmetric_balance=True,
            target_ratio=1.0,
        )

        print(f"\n[{ticker_name}] üè∑Ô∏è  Labeling TEST (V2.1 RELAXED)...")
        df_test = label_mean_reversion_v2_1(
            df_test,
            f"{ticker_name}_TEST",
            PRIMARY_HORIZON,
            params,
            min_hold_bars=5,
            enable_regime_filter=True,
            enable_dynamic_thresholds=True,
            enable_symmetric_balance=True,
            target_ratio=1.0,
        )

        # Add regime features AFTER labeling
        print(f"\n[{ticker_name}] ‚è≥ Adding TRAIN regime features...")
        df_train = add_regime_features(df_train)

        print(f"\n[{ticker_name}] ‚è≥ Adding TEST regime features...")
        df_test = add_regime_features(df_test)

        # Create sequences
        print(f"\n[{ticker_name}] üì¶ Creating TRAIN sequences...")
        train_sequences = create_sequences(df_train, SEQUENCE_LENGTH, FEATURE_COLS)

        print(f"\n[{ticker_name}] üì¶ Creating TEST sequences...")
        test_sequences = create_sequences(df_test, SEQUENCE_LENGTH, FEATURE_COLS)

        print(f"\n[{ticker_name}] üíæ Saving...")

        np.savez_compressed(
            train_path,
            X=train_sequences['X'],
            y=train_sequences['y'],
            quality=train_sequences['quality'],
            tier=train_sequences['tier'],
            timestamps=np.array(train_sequences['timestamps'], dtype='datetime64[ns]'),
            close_prices=np.array(train_sequences['close_prices']),
            feature_cols=train_sequences['feature_cols'],
        )

        np.savez_compressed(
            test_path,
            X=test_sequences['X'],
            y=test_sequences['y'],
            quality=test_sequences['quality'],
            tier=test_sequences['tier'],
            timestamps=np.array(test_sequences['timestamps'], dtype='datetime64[ns]'),
            close_prices=np.array(test_sequences['close_prices']),
            feature_cols=test_sequences['feature_cols'],
        )

        print(f"\n[{ticker_name}] ‚úÖ COMPLETE WITH V2.1 RELAXED LABELING!")
        print(f"  Train sequences: {len(train_sequences['X']):,}")
        print(f"  Test sequences:  {len(test_sequences['X']):,}")
        print(f"  Sequence shape:  {train_sequences['X'].shape}")

    except Exception as e:
        print(f"[{ticker_name}] üí• ERROR: {e}")
        import traceback
        traceback.print_exc()


def main():
    """Main execution"""

    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)

    print("="*70)
    print("üöÄ VWAP MEAN REVERSION V2.1 - RELAXED LABELING SYSTEM")
    print("="*70)
    print(f"\nüìÖ SPLIT: Train < {SPLIT_DATE} | Test >= {SPLIT_DATE}")
    print(f"\nüìä LSTM CONFIG:")
    print(f"  - Sequence length: {SEQUENCE_LENGTH} bars ({SEQUENCE_LENGTH*3} min)")
    print(f"  - Prediction horizon: {PRIMARY_HORIZON} bars ({PRIMARY_HORIZON*3} min)")
    print(f"  - Features: {len(FEATURE_COLS)} (33 base + 6 regime)")
    print(f"\nüéØ STRATEGY:")
    print(f"  - Fade extremes from VWAP (buy low, sell high)")
    print(f"  - LSTM learns temporal patterns")
    print(f"  - Tiered quality system for filtering")
    print(f"\nüÜï V2.1 CHANGES FROM V2.0:")
    print(f"  ‚úÖ RELAXED regime (ADX < 35, was 25) - more bars qualify")
    print(f"  ‚úÖ RELAXED quality gates (lower thresholds)")
    print(f"  ‚úÖ WIDER percentiles (5-35%, was 10-30%) - more candidates")
    print(f"  ‚úÖ ADDED C-tier for additional signals")
    print(f"  ‚úÖ Regime filter ONLY on A+ tier (others work anywhere)")
    print(f"\n‚≠ê EXPECTED RESULTS:")
    print(f"  - BUY:SELL ratio: ~1.0:1 (balanced)")
    print(f"  - Total signals: 25-35% of bars (was 1-2%)")
    print(f"  - Win rates: 60-80% (varies by tier)")
    print(f"  - Can filter by quality tier during training")
    print("="*70)

    start_time = time.time()

    for ticker in ['ES', 'NQ', 'RTY', 'YM']:
        process_ticker(ticker)

    elapsed = time.time() - start_time
    print(f"\n‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes")
    print("\n" + "="*70)
    print("üéâ V2.1 RELAXED PROCESSING COMPLETE!")
    print("="*70)

if __name__ == '__main__':
    main()