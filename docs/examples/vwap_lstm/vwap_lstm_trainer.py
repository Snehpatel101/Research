# -*- coding: utf-8 -*-
"""VWAP LSTM TRAINER

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nK0yAQAX695XjtoHGV3Itn04oGRF5G0H
"""

# --- Installation (Run Once) ---
!pip install numpy==1.26.4 pandas==2.2.2 pandas_ta==0.3.14b0 -U --ignore-installed
!pip install torch scikit-learn onnx -q
# --- IMPORTANT: Restart Runtime after installing! ---

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VWAP MEAN REVERSION LSTM V1.0 - OPTIMIZED FOR SPEED (FIXED - AMP DISABLED)
4-INSTRUMENT GENERALIZATION

üöÄ SPEED OPTIMIZATIONS:
- Reduced sequence length: 45 bars ‚Üí 2x faster
- Optimized DataLoader (workers, prefetch, persistent) ‚Üí better CPU‚ÜíGPU pipeline
- CuDNN-optimized LSTM (verified) ‚Üí fastest LSTM implementation
- Remove gradient accumulation (fit larger batches) ‚Üí fewer iterations

üîß STABILITY FIXES (CRITICAL):
- üîß DISABLED Mixed Precision (AMP) - was causing NaN at batch 0
- Increased gradient clipping (0.5 ‚Üí tighter control)
- Ultra-conservative learning rate (0.0001)
- Smaller model (96 hidden units)
- NaN detection (early stopping if NaN detected)
- Gradient monitoring (track gradient norms)
- Data validation (¬±10 range checking)
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import warnings
import os
import math
import time
from torch.cuda.amp import autocast, GradScaler
import pickle
from typing import Dict, List, Tuple, Optional

warnings.filterwarnings('ignore')

# Check PyTorch version for compile support
PYTORCH_VERSION = tuple(int(x) for x in torch.__version__.split('.')[:2])
COMPILE_SUPPORTED = PYTORCH_VERSION >= (2, 0)

print(f"üîß PyTorch Version: {torch.__version__}")
print(f"üîß Compile Support: {COMPILE_SUPPORTED}")
print(f"üîß CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"üîß CUDA Version: {torch.version.cuda}")
    print(f"üîß CuDNN Version: {torch.backends.cudnn.version()}")

# =========================================================
# CONFIGURATION - LSTM V1.0 OPTIMIZED & FIXED
# =========================================================

CACHE_DIR = "/content/drive/MyDrive/AI_Cache/VWAP_MeanReversion_LSTM_v1_0"
MODEL_DIR = "/content/drive/MyDrive/AI_Models/VWAP_MeanReversion_LSTM_v1_0/"
SAVE_MODEL_PATH = os.path.join(MODEL_DIR, "vwap_lstm_v1_0_best_OPTIMIZED.pth")

CHECKPOINT_BEST_COMBINED = os.path.join(MODEL_DIR, "checkpoint_best_combined_OPT.pth")
CHECKPOINT_BEST_RATIO = os.path.join(MODEL_DIR, "checkpoint_best_ratio_OPT.pth")
CHECKPOINT_MOST_BALANCED = os.path.join(MODEL_DIR, "checkpoint_most_balanced_OPT.pth")
CHECKPOINT_BEST_STABLE = os.path.join(MODEL_DIR, "checkpoint_best_stable_OPT.pth")

# üéØ Quality configuration
QUALITY_CONFIG = {
    'enabled': True,
    'threshold': 0.5,
    'filter_sell': False,
    'oversample_sell': False,
    'min_tier': 2,
}

# ‚ö° OPTIMIZED: Reduced sequence length for 2x speedup
SEQUENCE_LENGTH = 45  # FIXED: Changed from 30 to 45
PRIMARY_HORIZON = 20  # 20 bars = 60 min prediction
N_FEATURES = 39

# ‚ö° OPTIMIZED: No gradient accumulation, larger batch fits in memory
BATCH_SIZE = 512  # Can increase if memory allows
USE_GRADIENT_ACCUMULATION = False  # Disabled for speed

# üéØ LSTM hyperparameters - BALANCED APPROACH
TRAIN_PARAMS = {
    # ============================================================
    # TRAINING SCHEDULE
    # ============================================================
    'num_epochs': 50,
    'patience': 10,
    'warmup_epochs': 5,
    'use_cosine_schedule': True,

    # ============================================================
    # LEARNING RATE & OPTIMIZATION
    # ============================================================
    'lr': 0.00018,  # Down from 0.0002
    'weight_decay': 0.018,  # Up from 0.015
    'lr_scheduler_patience': 8,
    'lr_scheduler_factor': 0.65,
    'min_delta': 0.002,

    # ============================================================
    # GRADIENT CONTROL
    # ============================================================
    'gradient_accumulation_steps': 1,
    'max_grad_norm': 1.0,
    'monitor_gradients': False,
    'grad_log_interval': 100,

    # ============================================================
    # MODEL ARCHITECTURE - CRITICAL FIX
    # ============================================================
    'hidden_size': 80,  # INCREASED from 64 - model was too small
    'num_layers': 2,  # INCREASED from 1 - need more capacity
    'dropout': 0.48,  # Up from 0.45
    'bidirectional': False,

    # ============================================================
    # LOSS FUNCTION - REBALANCED
    # ============================================================
    'focal_gamma': 2.5,  # Down from 2.8 - less harsh
    'focal_alpha': 'auto',  # Will use computed class weights
    'label_smoothing': 0.03,  # INCREASED from 0.02 - help generalization
    'false_signal_penalty': 3.5,  # Down from 4.0
    'missed_signal_penalty': 4.0,  # UP from 3.0 - punish missing SELL more

    # ============================================================
    # AUXILIARY TASKS - RE-ENABLE
    # ============================================================
    'use_auxiliary_tasks': True,  # RE-ENABLED - helps with learning
    'auxiliary_weight': 0.12,
    'confidence_weight': 0.15,
    'confidence_temperature': 0.95,  # REDUCED from 1.0 - sharper predictions
    'confidence_threshold': 0.50,

    # ============================================================
    # DATA AUGMENTATION - LIGHT AUGMENTATION
    # ============================================================
    'use_mixup': False,
    'mixup_alpha': 0.0,
    'augmentation_prob': 0.05,  # ENABLE light augmentation

    # ============================================================
    # GENERALIZATION MONITORING
    # ============================================================
    'max_ratio': 1.15,
    'ratio_patience': 4,
    'stability_weight': 0.15,
    'require_stable_epochs': 4,

    # ============================================================
    # SIGNAL RATE TARGETS - CRITICAL FIX
    # ============================================================
    'target_signal_rate_min': 28,  # Up from 20
    'target_signal_rate_max': 38,  # Down from 40
    'signal_rate_weight': 0.18,  # Up from 0.15 - enforce harder
    'signal_rate_momentum': 0.80,

    # ============================================================
    # CHECKPOINT SELECTION CRITERIA
    # ============================================================
    'min_buy_precision': 0.45,  # REDUCED from 0.50 - more achievable
    'min_sell_precision': 0.45,  # REDUCED from 0.50
    'min_signal_balance': 0.60,  # REDUCED from 0.65

    # ============================================================
    # PERFORMANCE OPTIMIZATIONS
    # ============================================================
    'compile_model': False,
    'compile_mode': 'default',
    'use_amp': False,
}

# ‚ö° OPTIMIZED: DataLoader configuration
DATALOADER_CONFIG = {
    'num_workers': 4,  # Parallel data loading
    'pin_memory': True,  # Faster GPU transfer
    'persistent_workers': True,  # Keep workers alive between epochs
    'prefetch_factor': 2,  # Prefetch batches
}

TICKERS = ['ES', 'NQ', 'RTY', 'YM']

print(f"\n‚ö° OPTIMIZED VWAP Mean Reversion LSTM V1.0 (FIXED)")
print(f"üìä Sequence: {SEQUENCE_LENGTH} bars")
print(f"üìä Instruments: {TICKERS}")
print(f"‚ö° Batch Size: {BATCH_SIZE} (no gradient accumulation)")
print(f"‚ö° DataLoader Workers: {DATALOADER_CONFIG['num_workers']}")
print(f"‚ö° Model Compilation: {'Enabled' if TRAIN_PARAMS['compile_model'] else 'Disabled (for stability)'}")
print(f"üîß Mixed Precision: {'Enabled' if TRAIN_PARAMS['use_amp'] else 'DISABLED for stability'}")
print(f"üîß Gradient Clipping: {TRAIN_PARAMS['max_grad_norm']} (increased for stability)")

# =========================================================
# DATA AUGMENTATION
# =========================================================

def augment_sequence(X, y, quality, augmentation_prob=0.15):
    """Augment LSTM sequences with noise"""
    if np.random.random() < augmentation_prob:
        noise_scale = 0.02
        noise = np.random.normal(0, noise_scale, X.shape).astype(np.float32)
        X = X + noise

        if np.random.random() < 0.25:
            scale_factor = np.random.uniform(0.98, 1.02)
            X = X * scale_factor

    return X, y, quality


# =========================================================
# DATASET - OPTIMIZED FOR SPEED
# =========================================================

def apply_quality_filter_sequences(X, y, quality, config):
    """Filter sequences by quality"""
    if not config['enabled']:
        return X, y, quality

    threshold = config['threshold']
    is_signal = (y != 0)

    mask = (~is_signal) | (quality >= threshold)

    X_filtered = X[mask]
    y_filtered = y[mask]
    quality_filtered = quality[mask]

    print(f"  üìä Quality Filter: {len(X_filtered):,}/{len(X):,} ({len(X_filtered)/len(X)*100:.1f}% kept)")

    return X_filtered, y_filtered, quality_filtered


class VWAPLSTMDataset(Dataset):
    """
    ‚ö° OPTIMIZED Dataset for pre-generated LSTM sequences
    - Pre-converts to tensors for faster access
    - Caches data in memory
    """

    def __init__(self, ticker_files, mode='train', quality_config=None,
                 augmentation_prob=0.0, scalers=None):

        self.mode = mode
        self.quality_config = quality_config or QUALITY_CONFIG
        self.augmentation_prob = augmentation_prob
        self.scalers = scalers if scalers else {}

        print(f"\nüî® Building {mode.upper()} dataset (OPTIMIZED)...")

        # Load all data into memory
        X_list = []
        y_list = []
        quality_list = []

        for ticker_name, filepath in ticker_files.items():
            if not os.path.exists(filepath):
                print(f"  ‚ö†Ô∏è  [{ticker_name}] File not found: {filepath}")
                continue

            # Load sequences
            data = np.load(filepath)
            X = data['X'].astype(np.float32)  # Ensure float32
            y = data['y'].astype(np.int64)
            quality = data['quality'].astype(np.float32)

            print(f"  üìÇ [{ticker_name}] Loaded: {len(X):,} sequences, shape={X.shape}")

            # Apply quality filter
            if mode == 'train':
                X, y, quality = apply_quality_filter_sequences(X, y, quality, self.quality_config)

            if len(X) == 0:
                print(f"  ‚ö†Ô∏è  [{ticker_name}] No data after filtering")
                continue

            X_list.append(X)
            y_list.append(y)
            quality_list.append(quality)

            print(f"  ‚úÖ [{ticker_name}]: {len(X):,} sequences")

        if not X_list:
            self.X_data = np.array([])
            self.y_data = np.array([])
            self.quality_data = np.array([])
            self.class_weights = None
            self._total_len = 0
            return

        # Concatenate all instruments
        self.X_data = np.concatenate(X_list, axis=0)
        self.y_data = np.concatenate(y_list, axis=0)
        self.quality_data = np.concatenate(quality_list, axis=0)

        self._total_len = len(self.X_data)

        print(f"\n  üìä Total: {self._total_len:,} sequences")
        print(f"  üìä Shape: {self.X_data.shape}")
        print(f"  üìä Memory: {self.X_data.nbytes / 1024**2:.1f} MB")

        # Calculate class weights
        if mode == 'train':
            counts = Counter(self.y_data)
            total = len(self.y_data)

            print(f"\n  üìä Label Distribution:")
            print(f"     HOLD(0): {counts.get(0,0):,} ({counts.get(0,0)/total*100:.1f}%)")
            print(f"     BUY(1):  {counts.get(1,0):,} ({counts.get(1,0)/total*100:.1f}%)")
            print(f"     SELL(2): {counts.get(2,0):,} ({counts.get(2,0)/total*100:.1f}%)")

            class_counts = [counts.get(i, 1) for i in range(3)]
            total_samples = sum(class_counts)

            # Sqrt weighting
            weights = [np.sqrt(total_samples / (3 * count)) for count in class_counts]
            weights = [w / weights[0] for w in weights]
            weights[0] = 1.0

            max_weight = 3.0
            weights = [min(w, max_weight) for w in weights]

            self.class_weights = torch.FloatTensor(weights)
            print(f"     Focal Alpha: [H:{weights[0]:.2f}, B:{weights[1]:.2f}, S:{weights[2]:.2f}]")
        else:
            self.class_weights = None

    def __len__(self):
        return self._total_len

    def __getitem__(self, idx):
        X = self.X_data[idx].copy()  # Copy for augmentation
        y = self.y_data[idx]
        quality = self.quality_data[idx]

        # Augmentation (only in training)
        if self.mode == 'train' and self.augmentation_prob > 0:
            X, y, quality = augment_sequence(X, y, quality, self.augmentation_prob)

        # Convert to tensors (this is fast)
        X = torch.from_numpy(X)  # Already float32
        y = torch.tensor(y, dtype=torch.long)
        quality = torch.tensor(quality, dtype=torch.float32)

        # For compatibility with loss function
        y_seq = y.unsqueeze(0)
        quality_seq = quality.unsqueeze(0)

        return X, y_seq, quality_seq


# =========================================================
# LSTM MODEL - CUDNN OPTIMIZED
# =========================================================

class VWAPMeanReversionLSTM(nn.Module):
    """
    ‚ö° OPTIMIZED LSTM model using CuDNN backend
    - Uses nn.LSTM (automatically CuDNN-optimized when available)
    - Proper configuration for maximum CuDNN acceleration
    """

    def __init__(self, input_size, hidden_size=128, num_layers=2,
                 dropout=0.40, bidirectional=False, use_auxiliary=True,
                 confidence_temp=1.15):
        super().__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        self.use_auxiliary = use_auxiliary
        self.confidence_temp = confidence_temp

        # ‚ö° CRITICAL: This uses CuDNN when available (much faster)
        # Requirements for CuDNN:
        # 1. batch_first=True ‚úì
        # 2. No custom initialization
        # 3. Standard dropout between layers
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,  # Required for CuDNN
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )

        # Verify CuDNN is being used
        if torch.cuda.is_available() and torch.backends.cudnn.enabled:
            print(f"‚úÖ CuDNN-optimized LSTM enabled")
        else:
            print(f"‚ö†Ô∏è  CuDNN not available, using slower LSTM")

        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size

        # Layers
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(lstm_output_size)

        # Action head
        self.action_head = nn.Sequential(
            nn.Linear(lstm_output_size, lstm_output_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout * 0.7),
            nn.Linear(lstm_output_size // 2, 3)
        )

        # Auxiliary tasks
        if use_auxiliary:
            self.volatility_head = nn.Linear(lstm_output_size, 1)

            self.confidence_head = nn.Sequential(
                nn.Linear(lstm_output_size, lstm_output_size // 4),
                nn.ReLU(),
                nn.Dropout(dropout * 0.6),
                nn.Linear(lstm_output_size // 4, 1),
                nn.Sigmoid()
            )

    def forward(self, x, return_aux=False, apply_temp=False):
        """
        Args:
            x: (batch, seq_len, input_size)
        Returns:
            action_logits: (batch, 1, 3)
        """
        # LSTM forward (CuDNN accelerated)
        output, (h_n, c_n) = self.lstm(x)

        # Take last timestep
        last_output = output[:, -1, :]

        # Normalization and dropout
        last_output = self.layer_norm(last_output)
        last_output = self.dropout(last_output)

        # Action prediction
        action_logits = self.action_head(last_output)
        action_logits = action_logits.unsqueeze(1)

        # Temperature scaling
        if apply_temp and not self.training:
            action_logits = action_logits / self.confidence_temp

        # Auxiliary predictions
        if return_aux and self.use_auxiliary:
            volatility_pred = self.volatility_head(last_output).unsqueeze(1)
            confidence_pred = self.confidence_head(last_output).unsqueeze(1)
            return action_logits, volatility_pred, None, confidence_pred

        return action_logits


# =========================================================
# LOSS FUNCTION
# =========================================================

class ImprovedPropFirmLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0,
                 false_signal_penalty=2.8,
                 missed_signal_penalty=3.5,
                 auxiliary_weight=0.15,
                 confidence_weight=0.20,
                 label_smoothing=0.08):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.false_signal_penalty = false_signal_penalty
        self.missed_signal_penalty = missed_signal_penalty
        self.auxiliary_weight = auxiliary_weight
        self.confidence_weight = confidence_weight
        self.label_smoothing = label_smoothing
        self.mse_loss = nn.MSELoss()

    def forward(self, action_logits, targets, quality,
                volatility_pred=None, volatility_true=None,
                direction_pred=None, direction_true=None,
                confidence_pred=None,
                lam=1.0):

        device = action_logits.device
        batch_size, seq_len, num_classes = action_logits.shape

        logits_flat = action_logits.reshape(-1, num_classes)
        targets_flat = targets.reshape(-1)

        log_pt = F.log_softmax(logits_flat, dim=1)
        log_pt = log_pt.gather(1, targets_flat.unsqueeze(1)).squeeze(1)

        if self.label_smoothing > 0:
            pt = torch.exp(log_pt)
            smooth_pt = pt * (1 - self.label_smoothing) + self.label_smoothing / num_classes
            log_pt = torch.log(smooth_pt + 1e-8)
            pt = smooth_pt
        else:
            pt = torch.exp(log_pt)

        focal_weight = (1 - pt) ** self.gamma
        focal_loss = -focal_weight * log_pt

        if self.alpha is not None:
            if self.alpha.device != device:
                self.alpha = self.alpha.to(device)
            alpha_t = self.alpha.gather(0, targets_flat)
            focal_loss = alpha_t * focal_loss

        preds = torch.argmax(logits_flat, dim=-1)

        false_signals = ((preds != 0) & (targets_flat == 0))
        focal_loss[false_signals] *= self.false_signal_penalty

        missed_signals = ((preds == 0) & (targets_flat != 0))
        focal_loss[missed_signals] *= self.missed_signal_penalty

        main_loss = focal_loss.mean()

        aux_loss = 0.0
        conf_loss = 0.0

        if confidence_pred is not None:
            conf_flat = confidence_pred.reshape(-1)
            correct_mask = (preds == targets_flat).float()
            target_confidence = correct_mask * 0.85 + (1 - correct_mask) * 0.15
            conf_loss = self.mse_loss(conf_flat, target_confidence)

        total_loss = (
            main_loss +
            self.auxiliary_weight * aux_loss +
            self.confidence_weight * conf_loss
        )

        return total_loss, main_loss, aux_loss


# =========================================================
# LEARNING RATE SCHEDULER
# =========================================================

class CosineAnnealingWarmupRestarts(optim.lr_scheduler._LRScheduler):
    def __init__(self, optimizer, first_cycle_steps, warmup_steps,
                 min_lr=1e-7, max_lr=1e-4, gamma=0.9):
        self.first_cycle_steps = first_cycle_steps
        self.warmup_steps = warmup_steps
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.gamma = gamma
        self.cur_cycle_steps = first_cycle_steps
        self.cycle = 0
        self.step_in_cycle = 0
        super().__init__(optimizer)

    def get_lr(self):
        if self.step_in_cycle < self.warmup_steps:
            return [self.min_lr + (self.max_lr - self.min_lr) *
                   self.step_in_cycle / self.warmup_steps
                   for _ in self.base_lrs]
        else:
            progress = (self.step_in_cycle - self.warmup_steps) / \
                      (self.cur_cycle_steps - self.warmup_steps)
            return [self.min_lr + (self.max_lr - self.min_lr) *
                   (1 + math.cos(math.pi * progress)) / 2
                   for _ in self.base_lrs]

    def step(self, epoch=None):
        self.step_in_cycle += 1
        if self.step_in_cycle >= self.cur_cycle_steps:
            self.cycle += 1
            self.step_in_cycle = 0
            self.cur_cycle_steps = int(self.cur_cycle_steps * self.gamma)
        super().step()


# =========================================================
# METRICS
# =========================================================

def calculate_metrics(preds, labels):
    acc = accuracy_score(labels, preds)

    precision = np.zeros(3)
    recall = np.zeros(3)
    for i in range(3):
        tp = np.sum((labels == i) & (preds == i))
        fp = np.sum((labels != i) & (preds == i))
        fn = np.sum((labels == i) & (preds != i))
        precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0

    signal_rate = (preds != 0).sum() / len(preds) * 100

    signal_mask = (labels != 0)
    if signal_mask.sum() > 0:
        signal_acc = accuracy_score(labels[signal_mask], preds[signal_mask])
    else:
        signal_acc = 0.0

    return acc, precision, recall, signal_rate, signal_acc


def monitor_generalization(train_loss, val_loss, epoch):
    ratio = val_loss / train_loss if train_loss > 0 else 1.0

    if ratio > 1.20:
        return f"üö® CRITICAL overfitting (ratio={ratio:.2f})", False
    elif ratio > 1.15:
        return f"‚ö†Ô∏è  SEVERE overfitting (ratio={ratio:.2f})", False
    elif ratio > 1.12:
        return f"‚ö†Ô∏è  Moderate overfitting (ratio={ratio:.2f})", False
    elif ratio > 1.08:
        return f"‚ÑπÔ∏è  Slight overfitting (ratio={ratio:.2f})", True
    elif ratio < 0.90:
        return f"‚ÑπÔ∏è  Possible underfitting (ratio={ratio:.2f})", True
    else:
        return f"‚úÖ Excellent generalization (ratio={ratio:.2f})", True


def calculate_stable_combined_metric(metrics, train_loss, val_loss, signal_rate,
                                     prev_signal_rate=None,
                                     target_min=30, target_max=45,
                                     signal_weight=0.15, momentum=0.8,
                                     stability_weight=0.12):
    ratio = val_loss / train_loss if train_loss > 0 else 1.0

    gen_penalty = 0
    if ratio > 1.18:
        gen_penalty = (ratio - 1.18) * 1.5
    elif ratio > 1.15:
        gen_penalty = (ratio - 1.15) * 1.0
    elif ratio > 1.12:
        gen_penalty = (ratio - 1.12) * 0.6
    elif ratio > 1.08:
        gen_penalty = (ratio - 1.08) * 0.3

    if prev_signal_rate is not None:
        rate_change = abs(signal_rate - prev_signal_rate)
        stability_penalty = min(rate_change / 12.0, 0.6)
    else:
        stability_penalty = 0

    signal_score = 0
    if target_min <= signal_rate <= target_max:
        center = target_min + (target_max - target_min) * 0.3
        distance_from_center = abs(signal_rate - center)
        max_distance = (target_max - target_min) / 2
        signal_score = 1.0 - (distance_from_center / max_distance) * 0.25
    elif signal_rate < target_min:
        signal_score = (signal_rate / target_min) ** 1.5
    else:
        over_pct = (signal_rate - target_max) / target_max
        signal_score = max(0, 1.0 - over_pct * 4.0)

    signal_score = max(0, min(1, signal_score))
    signal_score -= stability_penalty
    signal_score = max(0, signal_score)

    gen_reward = 1.2 / (1.0 + ratio * 1.2)

    stability_bonus = 0
    if ratio < 1.12 and target_min <= signal_rate <= target_max:
        stability_bonus = 0.12

    combined = (
        0.24 * metrics['avg_signal_precision'] +
        0.16 * metrics['conf_avg_prec'] +
        0.14 * metrics['signal_acc'] +
        0.20 * gen_reward +
        0.14 * signal_score +
        0.12 * stability_bonus -
        0.12 * gen_penalty
    )

    return combined, gen_penalty, stability_penalty


def calculate_signal_balance_score(buy_prec, sell_prec, min_prec=0.55):
    """Calculate score that rewards balanced BUY/SELL precision"""
    if buy_prec < min_prec or sell_prec < min_prec:
        return 0.0

    balance_ratio = min(buy_prec, sell_prec) / max(buy_prec, sell_prec)
    avg_prec = (buy_prec + sell_prec) / 2
    score = avg_prec * (0.7 + 0.3 * balance_ratio)

    return score


# =========================================================
# TRAINING FUNCTION - OPTIMIZED & FIXED (AMP DISABLED)
# =========================================================

def train_model(model, train_loader, val_loader, class_weights,
                device='cpu', start_epoch=0, model_name="model",
                **train_params):

    device = torch.device(device)
    model = model.to(device)

    # ‚ö° OPTIMIZATION: Compile model if supported (DISABLED by default for stability)
    if train_params.get('compile_model', False) and COMPILE_SUPPORTED:
        print(f"‚ö° Compiling model with torch.compile()...")
        compile_mode = train_params.get('compile_mode', 'default')
        try:
            model = torch.compile(model, mode=compile_mode)
            print(f"‚úÖ Model compiled successfully (mode={compile_mode})")
        except Exception as e:
            print(f"‚ö†Ô∏è  Compilation failed: {e}, continuing without compilation")

    if class_weights is None:
        class_weights = torch.ones(3).to(device)
    else:
        class_weights = class_weights.to(device)

    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\n  üß† Model [{model_name}]: {param_count:,} parameters")

    criterion = ImprovedPropFirmLoss(
        alpha=class_weights,
        gamma=train_params['focal_gamma'],
        false_signal_penalty=train_params['false_signal_penalty'],
        missed_signal_penalty=train_params.get('missed_signal_penalty', 1.8),
        auxiliary_weight=train_params['auxiliary_weight'],
        confidence_weight=train_params.get('confidence_weight', 0.10),
        label_smoothing=train_params['label_smoothing']
    )

    optimizer = optim.AdamW(
        model.parameters(),
        lr=train_params['lr'],
        weight_decay=train_params['weight_decay']
    )

    # üîß CRITICAL: Conditionally create AMP scaler
    USE_AMP = train_params.get('use_amp', False)
    if USE_AMP:
        scaler = GradScaler()
        print("  ‚ö° Mixed Precision: Enabled")
    else:
        scaler = None
        print("  üîß Mixed Precision: DISABLED for stability")

    if train_params['use_cosine_schedule']:
        scheduler = CosineAnnealingWarmupRestarts(
            optimizer,
            first_cycle_steps=train_params['num_epochs'],
            warmup_steps=train_params['warmup_epochs'],
            min_lr=1e-7,
            max_lr=train_params['lr']
        )
    else:
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max',
            factor=train_params['lr_scheduler_factor'],
            patience=train_params['lr_scheduler_patience'],
            min_lr=1e-7
        )

    print(f"\nüéì Training {model_name} (OPTIMIZED & FIXED)")
    print(f"  Epochs: {train_params['num_epochs']} | Patience: {train_params['patience']}")
    print(f"  ‚ö° Sequence Length: {SEQUENCE_LENGTH}")
    print(f"  ‚ö° Learning Rate: {train_params['lr']}")
    print(f"  ‚ö° Gradient Clipping: {train_params['max_grad_norm']}")

    # Fix f-string syntax by extracting value first
    grad_accum_steps = train_params['gradient_accumulation_steps']
    grad_accum_str = 'Disabled' if grad_accum_steps == 1 else f'{grad_accum_steps} steps'
    print(f"  ‚ö° Gradient Accumulation: {grad_accum_str}")

    print(f"  ‚ö° Compiled: {train_params.get('compile_model', False) and COMPILE_SUPPORTED}")
    print(f"  üîß NaN Detection: Enabled")

    signal_rate_history = []
    stable_epoch_count = 0

    # Checkpoint tracking
    best_checkpoints = {
        'combined': {'metric': 0.0, 'state': None, 'epoch': 0, 'ratio': float('inf'),
                     'buy_prec': 0.0, 'sell_prec': 0.0},
        'ratio': {'metric': float('inf'), 'state': None, 'epoch': 0, 'combined': 0.0,
                  'buy_prec': 0.0, 'sell_prec': 0.0},
        'balanced': {'metric': 0.0, 'state': None, 'epoch': 0, 'ratio': float('inf'),
                     'buy_prec': 0.0, 'sell_prec': 0.0},
        'stable': {'metric': 0.0, 'state': None, 'epoch': 0, 'ratio': float('inf'),
                   'buy_prec': 0.0, 'sell_prec': 0.0}
    }

    epochs_no_improve = 0
    bad_ratio_count = 0

    # ‚ö° Timing
    epoch_start_time = time.time()

    # üîß Gradient monitoring
    monitor_grads = train_params.get('monitor_gradients', False)
    grad_log_interval = train_params.get('grad_log_interval', 100)

    for epoch in range(start_epoch + 1, start_epoch + train_params['num_epochs'] + 1):
        model.train()
        train_loss_total = 0
        train_main_loss = 0
        train_aux_loss = 0
        optimizer.zero_grad()

        # üîß Track if NaN occurred
        nan_detected = False

        for batch_idx, (X_batch, y_batch, quality_batch) in enumerate(train_loader):
            X_batch = X_batch.to(device, non_blocking=True)
            y_batch = y_batch.to(device, non_blocking=True)
            quality_batch = quality_batch.to(device, non_blocking=True)

            # üîß CRITICAL: Conditional AMP forward pass
            if USE_AMP:
                with autocast():
                    if train_params['use_auxiliary_tasks']:
                        logits, vol_pred, _, conf_pred = model(X_batch, return_aux=True)
                        loss, main_loss, aux_loss = criterion(
                            logits, y_batch, quality_batch,
                            None, None, None, None,
                            conf_pred
                        )
                    else:
                        logits = model(X_batch)
                        loss, main_loss, aux_loss = criterion(logits, y_batch, quality_batch)
            else:
                # NO AMP - standard FP32 forward pass
                if train_params['use_auxiliary_tasks']:
                    logits, vol_pred, _, conf_pred = model(X_batch, return_aux=True)
                    loss, main_loss, aux_loss = criterion(
                        logits, y_batch, quality_batch,
                        None, None, None, None,
                        conf_pred
                    )
                else:
                    logits = model(X_batch)
                    loss, main_loss, aux_loss = criterion(logits, y_batch, quality_batch)

            # ‚ö° Only divide if using gradient accumulation
            if train_params['gradient_accumulation_steps'] > 1:
                loss = loss / train_params['gradient_accumulation_steps']

            # üîß CRITICAL: Check for NaN BEFORE backward pass
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"\nüí• NaN/Inf loss detected at epoch {epoch}, batch {batch_idx}!")
                print(f"   Loss value: {loss.item()}")
                print(f"   Stopping training to prevent model corruption.")
                nan_detected = True
                break

            # üîß CRITICAL: Conditional AMP backward pass
            if USE_AMP and scaler:
                scaler.scale(loss).backward()
            else:
                loss.backward()

            # ‚ö° Update weights (with proper gradient clipping)
            if train_params['gradient_accumulation_steps'] == 1 or \
               (batch_idx + 1) % train_params['gradient_accumulation_steps'] == 0:

                # üîß CRITICAL: Conditional gradient unscaling and clipping
                if USE_AMP and scaler:
                    scaler.unscale_(optimizer)

                # Clip gradients (always do this, AMP or not)
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    model.parameters(),
                    max_norm=train_params['max_grad_norm']
                )

                # üîß Monitor gradients (optional)
                if monitor_grads and batch_idx % grad_log_interval == 0:
                    if grad_norm > train_params['max_grad_norm'] * 0.9:
                        print(f"   [Batch {batch_idx}] High gradient norm: {grad_norm:.4f}")

                # üîß Check for NaN gradients
                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    print(f"\nüí• NaN/Inf gradient detected at epoch {epoch}, batch {batch_idx}!")
                    print(f"   Gradient norm: {grad_norm}")
                    nan_detected = True
                    break

                # üîß CRITICAL: Conditional optimizer step
                if USE_AMP and scaler:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()

                optimizer.zero_grad()

            # Track loss
            if train_params['gradient_accumulation_steps'] > 1:
                train_loss_total += loss.item() * train_params['gradient_accumulation_steps']
            else:
                train_loss_total += loss.item()
            train_main_loss += main_loss.item() if isinstance(main_loss, torch.Tensor) else main_loss
            train_aux_loss += aux_loss.item() if isinstance(aux_loss, torch.Tensor) else aux_loss

        # üîß If NaN detected, stop training
        if nan_detected:
            print(f"\nüõë Training stopped due to NaN at epoch {epoch}")
            print(f"   Last valid train loss: {train_loss_total / max(1, batch_idx):.4f}")
            break

        train_loss_total /= len(train_loader)
        train_main_loss /= len(train_loader)
        train_aux_loss /= len(train_loader)

        # üîß Check for NaN in epoch loss
        if math.isnan(train_loss_total) or math.isinf(train_loss_total):
            print(f"\nüí• NaN/Inf in epoch {epoch} train loss!")
            print(f"   This shouldn't happen if batch checks are working.")
            break

        # Validation
        model.eval()
        val_loss_total = 0
        val_main_loss = 0
        val_aux_loss = 0
        all_preds = []
        all_labels = []
        all_probs = []

        with torch.no_grad():
            for X_batch, y_batch, quality_batch in val_loader:
                X_batch = X_batch.to(device, non_blocking=True)
                y_batch = y_batch.to(device, non_blocking=True)
                quality_batch = quality_batch.to(device, non_blocking=True)

                # üîß CRITICAL: Conditional AMP inference
                if USE_AMP:
                    with autocast():
                        if train_params['use_auxiliary_tasks']:
                            logits, vol_pred, _, conf_pred = model(X_batch, return_aux=True, apply_temp=True)
                            loss, main_loss, aux_loss = criterion(
                                logits, y_batch, quality_batch,
                                None, None, None, None,
                                conf_pred
                            )
                        else:
                            logits = model(X_batch, apply_temp=True)
                            loss, main_loss, aux_loss = criterion(logits, y_batch, quality_batch)
                else:
                    # NO AMP - standard FP32 inference
                    if train_params['use_auxiliary_tasks']:
                        logits, vol_pred, _, conf_pred = model(X_batch, return_aux=True, apply_temp=True)
                        loss, main_loss, aux_loss = criterion(
                            logits, y_batch, quality_batch,
                            None, None, None, None,
                            conf_pred
                        )
                    else:
                        logits = model(X_batch, apply_temp=True)
                        loss, main_loss, aux_loss = criterion(logits, y_batch, quality_batch)

                val_loss_total += loss.item()
                val_main_loss += main_loss.item() if isinstance(main_loss, torch.Tensor) else main_loss
                val_aux_loss += aux_loss.item() if isinstance(aux_loss, torch.Tensor) else aux_loss

                last_logits = logits[:, -1, :]
                probs = torch.softmax(last_logits, dim=1)
                preds = torch.argmax(last_logits, dim=1).cpu().numpy()

                all_preds.extend(preds)
                all_labels.extend(y_batch[:, -1].cpu().numpy())
                all_probs.extend(probs.cpu().numpy())

        val_loss_total /= len(val_loader)
        val_main_loss /= len(val_loader)
        val_aux_loss /= len(val_loader)

        # üîß Check for NaN in validation
        if math.isnan(val_loss_total) or math.isinf(val_loss_total):
            print(f"\nüí• NaN/Inf in epoch {epoch} validation loss!")
            break

        gen_status, gen_ok = monitor_generalization(train_loss_total, val_loss_total, epoch)
        ratio = val_loss_total / train_loss_total if train_loss_total > 0 else 1.0

        val_acc, precision, recall, signal_rate, signal_acc = calculate_metrics(
            np.array(all_preds), np.array(all_labels)
        )

        buy_prec = precision[1]
        sell_prec = precision[2]

        all_probs = np.array(all_probs)
        avg_signal_precision = (precision[1] + precision[2]) / 2

        max_probs = all_probs.max(axis=1)
        conf_mask = max_probs >= train_params['confidence_threshold']

        if conf_mask.sum() > 100:
            conf_preds = np.array(all_preds)[conf_mask]
            conf_labels = np.array(all_labels)[conf_mask]
            conf_acc, conf_precision, _, conf_sig_rate, _ = calculate_metrics(conf_preds, conf_labels)
            conf_avg_prec = (conf_precision[1] + conf_precision[2]) / 2
        else:
            conf_avg_prec = 0.0

        prev_signal_rate = signal_rate_history[-1] if signal_rate_history else None
        signal_rate_history.append(signal_rate)

        if len(signal_rate_history) >= 3:
            recent_std = np.std(signal_rate_history[-3:])
            is_stable = recent_std < 4.0
        else:
            recent_std = 0
            is_stable = False

        metrics = {
            'avg_signal_precision': avg_signal_precision,
            'conf_avg_prec': conf_avg_prec,
            'signal_acc': signal_acc
        }

        combined_metric, gen_penalty, stability_penalty = calculate_stable_combined_metric(
            metrics, train_loss_total, val_loss_total, signal_rate,
            prev_signal_rate=prev_signal_rate,
            target_min=train_params['target_signal_rate_min'],
            target_max=train_params['target_signal_rate_max'],
            signal_weight=train_params['signal_rate_weight'],
            momentum=train_params['signal_rate_momentum'],
            stability_weight=train_params['stability_weight']
        )

        if train_params['use_cosine_schedule']:
            scheduler.step()
        else:
            scheduler.step(combined_metric)

        if ratio > train_params['max_ratio']:
            bad_ratio_count += 1
        else:
            bad_ratio_count = 0

        is_best_combined = False
        is_best_ratio = False
        is_best_balanced = False
        is_best_stable = False

        min_buy = train_params.get('min_buy_precision', 0.55)
        min_sell = train_params.get('min_sell_precision', 0.55)
        balance_score = calculate_signal_balance_score(buy_prec, sell_prec, min_prec=min(min_buy, min_sell))
        signals_balanced = (buy_prec >= min_buy and sell_prec >= min_sell)

        improvement = combined_metric - best_checkpoints['combined']['metric']
        if improvement > train_params['min_delta']:
            best_checkpoints['combined'] = {
                'metric': combined_metric,
                'state': model.state_dict().copy() if not COMPILE_SUPPORTED or not train_params.get('compile_model', False) else model._orig_mod.state_dict().copy(),
                'epoch': epoch,
                'ratio': ratio,
                'buy_prec': buy_prec,
                'sell_prec': sell_prec
            }
            is_best_combined = True
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        if ratio < best_checkpoints['ratio']['metric'] and combined_metric > 0.50 and signals_balanced:
            best_checkpoints['ratio'] = {
                'metric': ratio,
                'state': model.state_dict().copy() if not COMPILE_SUPPORTED or not train_params.get('compile_model', False) else model._orig_mod.state_dict().copy(),
                'epoch': epoch,
                'combined': combined_metric,
                'buy_prec': buy_prec,
                'sell_prec': sell_prec
            }
            is_best_ratio = True

        signal_rate_score = 1.0 if train_params['target_signal_rate_min'] <= signal_rate <= train_params['target_signal_rate_max'] else 0.5
        balanced_score = combined_metric * (1.0 / (1.0 + ratio * 0.8)) * signal_rate_score

        if signals_balanced:
            balanced_score *= (1.0 + balance_score * 0.3)

        if balanced_score > best_checkpoints['balanced']['metric']:
            best_checkpoints['balanced'] = {
                'metric': balanced_score,
                'state': model.state_dict().copy() if not COMPILE_SUPPORTED or not train_params.get('compile_model', False) else model._orig_mod.state_dict().copy(),
                'epoch': epoch,
                'ratio': ratio,
                'buy_prec': buy_prec,
                'sell_prec': sell_prec
            }
            is_best_balanced = True

        if (ratio < train_params['max_ratio'] and
            train_params['target_signal_rate_min'] <= signal_rate <= train_params['target_signal_rate_max'] and
            is_stable):
            stable_epoch_count += 1
        else:
            stable_epoch_count = 0

        if stable_epoch_count >= train_params.get('require_stable_epochs', 3):
            stable_score = combined_metric * (1.0 / (1.0 + recent_std / 8.0))

            if signals_balanced:
                stable_score *= (1.0 + balance_score * 0.3)

            if stable_score > best_checkpoints['stable']['metric']:
                best_checkpoints['stable'] = {
                    'metric': stable_score,
                    'state': model.state_dict().copy() if not COMPILE_SUPPORTED or not train_params.get('compile_model', False) else model._orig_mod.state_dict().copy(),
                    'epoch': epoch,
                    'ratio': ratio,
                    'buy_prec': buy_prec,
                    'sell_prec': sell_prec
                }
                is_best_stable = True

        status_parts = []
        if is_best_combined:
            status_parts.append("‚úÖ BEST_COMB")
        if is_best_ratio:
            status_parts.append("üéØ BEST_RATIO")
        if is_best_balanced:
            status_parts.append("‚öñÔ∏è BEST_BAL")
        if is_best_stable:
            status_parts.append("üîí BEST_STABLE")
        if not status_parts:
            status_parts.append(f"‚è≥ {epochs_no_improve}/{train_params['patience']}")

        if stable_epoch_count > 0:
            status_parts.append(f"üîí√ó{stable_epoch_count}")

        if signals_balanced:
            status_parts.append("‚ö°BAL")

        status = " ".join(status_parts)

        if prev_signal_rate:
            trend = "üìà" if signal_rate > prev_signal_rate + 2 else "üìâ" if signal_rate < prev_signal_rate - 2 else "‚û°Ô∏è"
        else:
            trend = ""

        sig_indicator = "üéØ" if train_params['target_signal_rate_min'] <= signal_rate <= train_params['target_signal_rate_max'] else ("‚¨áÔ∏è" if signal_rate < train_params['target_signal_rate_min'] else "‚¨ÜÔ∏è")

        # ‚ö° Timing info
        epoch_time = time.time() - epoch_start_time

        # Generalization indicator
        if ratio <= 1.00:
            gen_icon = "üü¢"  # Perfect (val better)
        elif ratio <= 1.08:
            gen_icon = "‚úÖ"  # Excellent
        elif ratio <= 1.12:
            gen_icon = "üü°"  # Good
        elif ratio <= 1.15:
            gen_icon = "‚ö†Ô∏è"   # Moderate overfitting
        else:
            gen_icon = "üî¥"  # Severe overfitting

        print(f"E{epoch:2d} [{epoch_time:.1f}s] | TrL:{train_loss_total:.4f} VL:{val_loss_total:.4f} R:{ratio:.2f}{gen_icon} | " +
              f"Acc:{val_acc:.3f} SigAcc:{signal_acc:.3f} | " +
              f"B:{precision[1]:.3f} S:{precision[2]:.3f} | " +
              f"Rate:{signal_rate:.1f}%{sig_indicator}{trend} | Comb:{combined_metric:.3f} | {status}")

        epoch_start_time = time.time()

        if bad_ratio_count >= train_params['ratio_patience']:
            print(f"\nüõë Stopping: Ratio > {train_params['max_ratio']} for {bad_ratio_count} epochs")
            break

        if epochs_no_improve >= train_params['patience']:
            print(f"\n‚èπÔ∏è  Early stop @ E{epoch} | {gen_status}")
            break

    # CHECKPOINT SELECTION
    def is_checkpoint_valid(cp):
        return (cp['state'] is not None and
                cp.get('buy_prec', 0) >= min_buy and
                cp.get('sell_prec', 0) >= min_sell)

    selected_checkpoint = None
    selection_reason = ""

    if is_checkpoint_valid(best_checkpoints['stable']):
        selected_checkpoint = best_checkpoints['stable']
        selection_reason = "STABLE"
    elif is_checkpoint_valid(best_checkpoints['balanced']):
        selected_checkpoint = best_checkpoints['balanced']
        selection_reason = "BALANCED"
    elif is_checkpoint_valid(best_checkpoints['combined']):
        selected_checkpoint = best_checkpoints['combined']
        selection_reason = "COMBINED"
    elif is_checkpoint_valid(best_checkpoints['ratio']):
        selected_checkpoint = best_checkpoints['ratio']
        selection_reason = "RATIO"
    elif best_checkpoints['combined']['state'] is not None:
        selected_checkpoint = best_checkpoints['combined']
        selection_reason = "COMBINED (below min precision)"
        print(f"\n‚ö†Ô∏è  WARNING: No checkpoint met min precision requirements!")

    if selected_checkpoint is not None:
        if COMPILE_SUPPORTED and train_params.get('compile_model', False):
            model._orig_mod.load_state_dict(selected_checkpoint['state'])
        else:
            model.load_state_dict(selected_checkpoint['state'])
        print(f"\n‚úÖ Loaded {selection_reason} checkpoint from epoch {selected_checkpoint['epoch']}")
        print(f"   Ratio: {selected_checkpoint['ratio']:.2f} | Metric: {selected_checkpoint.get('metric', 0):.3f}")
        print(f"   BUY: {selected_checkpoint['buy_prec']:.3f} | SELL: {selected_checkpoint['sell_prec']:.3f}")
    else:
        print("\n‚ùå ERROR: No checkpoint available!")

    return model, best_checkpoints


# =========================================================
# EVALUATION
# =========================================================

def evaluate_model(model, test_loader, device, use_amp=False):
    print("\n" + "="*70)
    print("üß™ TEST SET EVALUATION (2025 UNSEEN DATA)")
    print("="*70)

    model.eval()
    model.to(device)

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for X_batch, y_batch, quality_batch in test_loader:
            X_batch = X_batch.to(device, non_blocking=True)

            # üîß CRITICAL: Conditional AMP inference
            if use_amp:
                with autocast():
                    logits = model(X_batch, apply_temp=True)
            else:
                logits = model(X_batch, apply_temp=True)

            last_logits = logits[:, -1, :]
            probs = torch.softmax(last_logits, dim=1)
            preds = torch.argmax(last_logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y_batch[:, -1].numpy())
            all_probs.extend(probs.cpu().numpy())

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    acc, precision, recall, signal_rate, signal_acc = calculate_metrics(all_preds, all_labels)

    print(f"\nüìä Overall Performance:")
    print(f"  Accuracy: {acc:.3f}")
    print(f"  Signal Accuracy: {signal_acc:.3f}")
    print(f"  Signal Rate: {signal_rate:.1f}%")
    print(f"  Precision: HOLD={precision[0]:.3f} | BUY={precision[1]:.3f} | SELL={precision[2]:.3f}")
    print(f"  Recall: HOLD={recall[0]:.3f} | BUY={recall[1]:.3f} | SELL={recall[2]:.3f}")

    print(f"\nüéØ HIGH-CONFIDENCE SIGNALS:")
    for threshold in [0.55, 0.60, 0.65, 0.70, 0.75, 0.80]:
        max_probs = all_probs.max(axis=1)
        conf_mask = max_probs >= threshold

        if conf_mask.sum() > 10:
            conf_preds = all_preds[conf_mask]
            conf_labels = all_labels[conf_mask]

            conf_acc, conf_prec, conf_rec, conf_sig_rate, conf_sig_acc = calculate_metrics(
                conf_preds, conf_labels
            )

            n_samples = conf_mask.sum()
            pct_kept = n_samples / len(all_preds) * 100
            n_buy = (conf_preds == 1).sum()
            n_sell = (conf_preds == 2).sum()

            print(f"\n  Confidence ‚â• {threshold:.2f} ({n_samples:,} samples, {pct_kept:.1f}%):")
            print(f"    BUY:  Prec={conf_prec[1]:.3f} (n={n_buy})")
            print(f"    SELL: Prec={conf_prec[2]:.3f} (n={n_sell})")

    return {
        'accuracy': acc,
        'signal_accuracy': signal_acc,
        'precision': precision,
        'recall': recall,
        'signal_rate': signal_rate
    }


# =========================================================
# MAIN EXECUTION
# =========================================================

if __name__ == '__main__':

    os.makedirs(MODEL_DIR, exist_ok=True)

    print("\n" + "="*70)
    print("‚ö° OPTIMIZED VWAP MEAN REVERSION LSTM V1.0 (FIXED)")
    print("="*70)
    print(f"üéØ Expected Speedup: 2-3x faster than baseline (without AMP)")
    print(f"‚ö° Sequence Length: {SEQUENCE_LENGTH}")
    print(f"‚ö° No Gradient Accumulation (larger batches fit)")
    print(f"üîß Mixed Precision: DISABLED for stability")
    print(f"‚ö° CuDNN-Optimized LSTM")
    print(f"‚ö° Optimized DataLoaders")
    print(f"üîß Gradient Clipping: {TRAIN_PARAMS['max_grad_norm']}")
    print(f"üîß NaN Detection: Enabled")
    if COMPILE_SUPPORTED and TRAIN_PARAMS['compile_model']:
        print(f"‚ö° torch.compile() for model optimization")
    print("="*70)

    # Load data
    train_files = {}
    val_files = {}
    test_files = {}

    print("\nüìÅ Loading sequences from 4 instruments...")
    for ticker in TICKERS:
        train_val_file = os.path.join(CACHE_DIR, f'{ticker}_TRAIN_VAL.npz')
        test_file = os.path.join(CACHE_DIR, f'{ticker}_TEST.npz')

        if not os.path.exists(train_val_file) or not os.path.exists(test_file):
            print(f"‚ùå [{ticker}] Missing .npz files")
            print(f"   Expected: {train_val_file}")
            print(f"   Expected: {test_file}")
            continue

        # Load train_val data
        print(f"  üìÇ [{ticker}] Loading {os.path.basename(train_val_file)}...")
        data = np.load(train_val_file)
        X = data['X']

        # üîß CRITICAL: Validate data range
        x_min, x_max = X.min(), X.max()
        print(f"  üìä [{ticker}] Data range: [{x_min:.2f}, {x_max:.2f}]")

        if x_max > 15 or x_min < -15:
            print(f"  ‚ùå [{ticker}] ERROR: Data range exceeds ¬±10!")
            print(f"      Current range: ¬±{max(abs(x_min), abs(x_max)):.0f}")
            print(f"      Expected: ¬±10 for stable training")
            print(f"\n  üîß FIX:")
            print(f"      1. Delete cache: rm {CACHE_DIR}/*.npz")
            print(f"      2. Regenerate: python <preprocessing_script>.py")
            print(f"      3. Try training again")
            continue

        print(f"  ‚úÖ [{ticker}] Data range OK (¬±10)")

        # Split train/val
        n_total = len(X)
        n_train = int(n_total * 0.90)

        # Create temp files for this run
        train_file_tmp = os.path.join(CACHE_DIR, f'{ticker}_TRAIN_TMP.npz')
        val_file_tmp = os.path.join(CACHE_DIR, f'{ticker}_VAL_TMP.npz')

        # Handle sequence length adjustment if needed
        if X.shape[1] != SEQUENCE_LENGTH:
            print(f"  ‚ö†Ô∏è  [{ticker}] Adjusting sequence length from {X.shape[1]} to {SEQUENCE_LENGTH}")
            X_adjusted = X[:, -SEQUENCE_LENGTH:, :]
        else:
            X_adjusted = X

        # Save split data
        np.savez_compressed(
            train_file_tmp,
            X=X_adjusted[:n_train],
            y=data['y'][:n_train],
            quality=data['quality'][:n_train]
        )

        np.savez_compressed(
            val_file_tmp,
            X=X_adjusted[n_train:],
            y=data['y'][n_train:],
            quality=data['quality'][n_train:]
        )

        train_files[ticker] = train_file_tmp
        val_files[ticker] = val_file_tmp
        test_files[ticker] = test_file

        print(f"  ‚úÖ [{ticker}] Train={n_train:,} | Val={n_total-n_train:,}")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nüíª Device: {device}")

    # ‚ö° OPTIMIZED: Create datasets
    train_dataset = VWAPLSTMDataset(
        train_files, mode='train', quality_config=QUALITY_CONFIG,
        augmentation_prob=TRAIN_PARAMS['augmentation_prob']
    )

    val_dataset = VWAPLSTMDataset(
        val_files, mode='val', quality_config=QUALITY_CONFIG
    )

    # ‚ö° OPTIMIZED: DataLoaders with all optimizations
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        **DATALOADER_CONFIG  # num_workers, pin_memory, persistent_workers, prefetch_factor
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE*2,
        shuffle=False,
        **DATALOADER_CONFIG
    )

    print(f"\n‚ö° DataLoader Config: {DATALOADER_CONFIG}")

    # Create model
    model = VWAPMeanReversionLSTM(
        input_size=N_FEATURES,
        hidden_size=TRAIN_PARAMS['hidden_size'],
        num_layers=TRAIN_PARAMS['num_layers'],
        dropout=TRAIN_PARAMS['dropout'],
        bidirectional=TRAIN_PARAMS['bidirectional'],
        use_auxiliary=TRAIN_PARAMS['use_auxiliary_tasks'],
        confidence_temp=TRAIN_PARAMS['confidence_temperature']
    )

    # Train
    print("\n" + "="*70)
    print("‚ö° STARTING OPTIMIZED TRAINING (AMP DISABLED)")
    print("="*70)

    start_time = time.time()

    trained_model, best_checkpoints = train_model(
        model, train_loader, val_loader,
        class_weights=train_dataset.class_weights,
        device=device,
        start_epoch=0,
        model_name="LSTM-V1.0-OPT",
        **TRAIN_PARAMS
    )

    training_time = time.time() - start_time
    print(f"\n‚è±Ô∏è  Total Training Time: {training_time/60:.1f} minutes")

    # Save
    # If compiled, need to save the original model
    if COMPILE_SUPPORTED and TRAIN_PARAMS.get('compile_model', False):
        torch.save(trained_model._orig_mod.state_dict(), SAVE_MODEL_PATH)
    else:
        torch.save(trained_model.state_dict(), SAVE_MODEL_PATH)
    print(f"‚úÖ Best model saved: {SAVE_MODEL_PATH}")

    # Save checkpoints with explicit mapping
    checkpoint_paths = {
        'combined': CHECKPOINT_BEST_COMBINED,
        'ratio': CHECKPOINT_BEST_RATIO,
        'balanced': CHECKPOINT_MOST_BALANCED,
        'stable': CHECKPOINT_BEST_STABLE
    }

    for name, checkpoint in best_checkpoints.items():
        if checkpoint['state']:
            path = checkpoint_paths[name]
            torch.save(checkpoint['state'], path)
            print(f"üíæ Saved {name.upper()} checkpoint: {os.path.basename(path)}")

    # Evaluate
    test_dataset = VWAPLSTMDataset(
        test_files, mode='test', quality_config=QUALITY_CONFIG
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE*2,
        shuffle=False,
        **DATALOADER_CONFIG
    )

    test_results = evaluate_model(
        trained_model,
        test_loader,
        device,
        use_amp=TRAIN_PARAMS['use_amp']
    )

    print("\n" + "="*70)
    print("‚úÖ OPTIMIZED TRAINING COMPLETE")
    print("="*70)
    print(f"‚è±Ô∏è  Training Time: {training_time/60:.1f} minutes")
    print(f"üìÅ Model Directory: {MODEL_DIR}")
    print(f"\nüéØ TEST SET RESULTS:")
    print(f"  Signal Acc: {test_results['signal_accuracy']:.1%}")
    print(f"  BUY Prec: {test_results['precision'][1]:.1%}")
    print(f"  SELL Prec: {test_results['precision'][2]:.1%}")
    print("="*70)

# ============================================================================
# üîç MODEL PREDICTION DISTRIBUTION ANALYSIS - STANDALONE VERSION
# ============================================================================
# Run this AFTER your training script completes
# Usage: Simply run this in the same session where you trained the model

import torch
import numpy as np
from collections import Counter

def analyze_model_predictions(model, val_loader, device=None):
    """
    Comprehensive analysis of model predictions on validation set.

    Args:
        model: Trained PyTorch model
        val_loader: Validation DataLoader
        device: torch.device (auto-detected if None)
    """

    if device is None:
        device = next(model.parameters()).device

    print("\n" + "="*70)
    print("üîç FINAL MODEL PREDICTION DISTRIBUTION ANALYSIS")
    print("="*70)

    # Ensure model is in evaluation mode
    model.eval()

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in val_loader:
            # Handle different batch formats
            if len(batch) == 4:
                X_batch, y_regime_batch, y_risk_batch, _ = batch
            elif len(batch) == 3:
                X_batch, y_regime_batch, y_risk_batch = batch
            else:
                X_batch, y_regime_batch = batch[0], batch[1]

            # Move to device
            X_batch = X_batch.to(device)
            y_regime_batch = y_regime_batch.to(device)

            # Get model predictions
            logits = model(X_batch)

            # Extract final bar prediction (sequence output)
            if logits.dim() == 3:  # (batch, seq, classes)
                last_logits = logits[:, -1, :]
                final_labels = y_regime_batch[:, -1]
            else:  # (batch, classes)
                last_logits = logits
                final_labels = y_regime_batch

            # Calculate probabilities and predictions
            probs = torch.softmax(last_logits, dim=-1)
            preds = torch.argmax(probs, dim=-1)

            # Store results
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(final_labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    # Convert to arrays
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # Extract class probabilities
    hold_probs = all_probs[:, 0]
    buy_probs = all_probs[:, 1]
    sell_probs = all_probs[:, 2]

    # Count predictions
    pred_counts = Counter(all_preds)
    label_counts = Counter(all_labels)
    total = len(all_preds)

    print(f"\nTotal Validation Samples: {total:,}")

    # ========================================================================
    # RAW PREDICTION DISTRIBUTION
    # ========================================================================
    print("\n" + "-"*70)
    print("üìä RAW PREDICTION DISTRIBUTION (No Filtering):")
    print("-"*70)
    print(f"{'Class':<15} {'Predicted':<20} {'Actual Labels':<20}")
    print("-"*70)
    print(f"{'Hold (0)':<15} {pred_counts.get(0,0):>6,} ({pred_counts.get(0,0)/total*100:>5.1f}%)   {label_counts.get(0,0):>6,} ({label_counts.get(0,0)/total*100:>5.1f}%)")
    print(f"{'Buy  (1)':<15} {pred_counts.get(1,0):>6,} ({pred_counts.get(1,0)/total*100:>5.1f}%)   {label_counts.get(1,0):>6,} ({label_counts.get(1,0)/total*100:>5.1f}%)")
    print(f"{'Sell (2)':<15} {pred_counts.get(2,0):>6,} ({pred_counts.get(2,0)/total*100:>5.1f}%)   {label_counts.get(2,0):>6,} ({label_counts.get(2,0)/total*100:>5.1f}%)")
    print("-"*70)

    # ========================================================================
    # PRECISION & RECALL
    # ========================================================================
    print("\n" + "-"*70)
    print("üéØ RAW PRECISION & RECALL (No Confidence Filter):")
    print("-"*70)

    class_names = ['Hold', 'Buy ', 'Sell']
    precisions = []
    recalls = []

    for cls in [0, 1, 2]:
        pred_mask = (all_preds == cls)
        label_mask = (all_labels == cls)

        if pred_mask.sum() > 0:
            correct = ((all_preds == cls) & (all_labels == cls)).sum()
            precision = correct / pred_mask.sum()
        else:
            precision = 0.0

        if label_mask.sum() > 0:
            correct = ((all_preds == cls) & (all_labels == cls)).sum()
            recall = correct / label_mask.sum()
        else:
            recall = 0.0

        precisions.append(precision)
        recalls.append(recall)

        print(f"{class_names[cls]} ({cls}):")
        print(f"  Precision: {precision*100:>5.1f}%")
        print(f"  Recall:    {recall*100:>5.1f}%")

    # ========================================================================
    # SIGNAL STATISTICS
    # ========================================================================
    buy_signal_count = pred_counts.get(1, 0)
    sell_signal_count = pred_counts.get(2, 0)
    total_signals = buy_signal_count + sell_signal_count
    signal_pct = (total_signals / total) * 100

    print("\n" + "-"*70)
    print("üì° RAW SIGNAL GENERATION (Before Confidence Filtering):")
    print("-"*70)
    print(f"Total Buy/Sell Signals: {total_signals:,} ({signal_pct:.1f}% of validation)")
    print(f"  Buy Signals:  {buy_signal_count:>6,} ({buy_signal_count/total*100:>5.1f}%)")
    print(f"  Sell Signals: {sell_signal_count:>6,} ({sell_signal_count/total*100:>5.1f}%)")

    if buy_signal_count > 0:
        buy_mask = (all_preds == 1)
        buy_correct = ((all_preds == 1) & (all_labels == 1)).sum()
        buy_precision_raw = buy_correct / buy_signal_count
        avg_buy_conf = buy_probs[buy_mask].mean()

        print(f"\nRaw Buy Signals:")
        print(f"  Count: {buy_signal_count:,}")
        print(f"  Precision: {buy_precision_raw*100:.1f}%")
        print(f"  Avg Confidence: {avg_buy_conf:.3f}")

    if sell_signal_count > 0:
        sell_mask = (all_preds == 2)
        sell_correct = ((all_preds == 2) & (all_labels == 2)).sum()
        sell_precision_raw = sell_correct / sell_signal_count
        avg_sell_conf = sell_probs[sell_mask].mean()

        print(f"\nRaw Sell Signals:")
        print(f"  Count: {sell_signal_count:,}")
        print(f"  Precision: {sell_precision_raw*100:.1f}%")
        print(f"  Avg Confidence: {avg_sell_conf:.3f}")

    # ========================================================================
    # CONFIDENCE THRESHOLD ANALYSIS
    # ========================================================================
    print("\n" + "="*70)
    print("üéØ CONFIDENCE THRESHOLD ANALYSIS")
    print("="*70)
    print("\n‚ö†Ô∏è  IMPORTANT: Always filter by confidence threshold in production.\n")

    thresholds = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95]

    print(f"{'Thresh':<8} {'Sig%':<7} {'SigN':<8} {'BuyN':<8} {'BuyP%':<8} {'BuyEV':<9} {'SellN':<8} {'SellP%':<8} {'SellEV':<9} {'Trades/Day':<12} {'Status':<15}")
    print("-"*130)

    viable_thresholds = []
    best_threshold = None
    best_ev = -999

    for thresh in thresholds:
        # Filter predictions by confidence
        filtered_preds = np.zeros(len(all_probs))

        # Keep BUY if confident
        buy_confident = (all_preds == 1) & (buy_probs > thresh)
        filtered_preds[buy_confident] = 1

        # Keep SELL if confident
        sell_confident = (all_preds == 2) & (sell_probs > thresh)
        filtered_preds[sell_confident] = 2

        # Calculate metrics
        buy_filtered_count = (filtered_preds == 1).sum()
        sell_filtered_count = (filtered_preds == 2).sum()
        total_filtered_signals = buy_filtered_count + sell_filtered_count
        filtered_signal_rate = total_filtered_signals / total

        # Buy metrics
        if buy_filtered_count > 0:
            buy_filtered_correct = ((filtered_preds == 1) & (all_labels == 1)).sum()
            buy_filtered_precision = buy_filtered_correct / buy_filtered_count
            buy_ev = (buy_filtered_precision * 2) - ((1 - buy_filtered_precision) * 1)
        else:
            buy_filtered_precision = 0
            buy_ev = 0

        # Sell metrics
        if sell_filtered_count > 0:
            sell_filtered_correct = ((filtered_preds == 2) & (all_labels == 2)).sum()
            sell_filtered_precision = sell_filtered_correct / sell_filtered_count
            sell_ev = (sell_filtered_precision * 2) - ((1 - sell_filtered_precision) * 1)
        else:
            sell_filtered_precision = 0
            sell_ev = 0

        # Daily trades estimate (390 bars/day)
        bars_per_day = 390
        daily_trades = int(filtered_signal_rate * bars_per_day)

        # Determine status
        if buy_ev > 0.4 and filtered_signal_rate < 0.25 and buy_filtered_count > 100:
            status = "‚úÖ EXCELLENT"
            combined_ev = buy_ev + sell_ev
            viable_thresholds.append({
                'threshold': thresh,
                'buy_precision': buy_filtered_precision,
                'buy_ev': buy_ev,
                'signal_rate': filtered_signal_rate,
                'daily_trades': daily_trades
            })
            if combined_ev > best_ev:
                best_ev = combined_ev
                best_threshold = thresh
        elif buy_ev > 0.3 and filtered_signal_rate < 0.30 and buy_filtered_count > 50:
            status = "‚úÖ GOOD"
            viable_thresholds.append({
                'threshold': thresh,
                'buy_precision': buy_filtered_precision,
                'buy_ev': buy_ev,
                'signal_rate': filtered_signal_rate,
                'daily_trades': daily_trades
            })
        elif buy_ev > 0.2 and buy_filtered_count > 20:
            status = "‚ö†Ô∏è MARGINAL"
        elif buy_filtered_count < 20:
            status = "‚ö†Ô∏è TOO FEW"
        elif filtered_signal_rate > 0.40:
            status = "‚ö†Ô∏è TOO MANY"
        else:
            status = "‚ùå NOT VIABLE"

        print(f"{thresh:<8.2f} {filtered_signal_rate*100:<7.1f} {total_filtered_signals:<8,} {buy_filtered_count:<8,} {buy_filtered_precision*100:<8.1f} {buy_ev:<9.3f} {sell_filtered_count:<8,} {sell_filtered_precision*100:<8.1f} {sell_ev:<9.3f} {daily_trades:<12} {status:<15}")

    print("-"*130)

    # ========================================================================
    # RECOMMENDATIONS
    # ========================================================================
    print("\n" + "="*70)
    print("üí° PRODUCTION DEPLOYMENT RECOMMENDATIONS")
    print("="*70)

    if viable_thresholds:
        print("\n‚úÖ MODEL IS VIABLE FOR TRADING with confidence filtering!\n")
        print("üìã Recommended Thresholds:\n")

        # Conservative
        conservative = [t for t in viable_thresholds if t['buy_ev'] > 0.6]
        if conservative:
            best_conservative = max(conservative, key=lambda x: x['buy_ev'])
            print(f"üõ°Ô∏è  CONSERVATIVE (Recommended for Paper Trading):")
            print(f"   Threshold: {best_conservative['threshold']:.2f}")
            print(f"   BUY Precision: {best_conservative['buy_precision']*100:.1f}%")
            print(f"   Expected Value: +{best_conservative['buy_ev']:.3f}R per trade")
            print(f"   Signals per day: ~{best_conservative['daily_trades']}")
            print(f"   Signal rate: {best_conservative['signal_rate']*100:.1f}%\n")

        # Balanced
        balanced = [t for t in viable_thresholds if 0.4 < t['buy_ev'] < 0.6]
        if balanced:
            best_balanced = max(balanced, key=lambda x: x['buy_ev'])
            print(f"‚öñÔ∏è  BALANCED (Recommended for Live Trading):")
            print(f"   Threshold: {best_balanced['threshold']:.2f}")
            print(f"   BUY Precision: {best_balanced['buy_precision']*100:.1f}%")
            print(f"   Expected Value: +{best_balanced['buy_ev']:.3f}R per trade")
            print(f"   Signals per day: ~{best_balanced['daily_trades']}")
            print(f"   Signal rate: {best_balanced['signal_rate']*100:.1f}%\n")

        # Aggressive
        aggressive = [t for t in viable_thresholds if t['daily_trades'] > 30]
        if aggressive:
            best_aggressive = min(aggressive, key=lambda x: x['threshold'])
            print(f"‚ö° AGGRESSIVE (More Opportunities):")
            print(f"   Threshold: {best_aggressive['threshold']:.2f}")
            print(f"   BUY Precision: {best_aggressive['buy_precision']*100:.1f}%")
            print(f"   Expected Value: +{best_aggressive['buy_ev']:.3f}R per trade")
            print(f"   Signals per day: ~{best_aggressive['daily_trades']}")
            print(f"   Signal rate: {best_aggressive['signal_rate']*100:.1f}%\n")

        print("üö® CRITICAL: SELL Signals")
        print("   ‚ùå Do NOT trade SELL signals at any threshold")
        print("   ‚ö†Ô∏è  Use SELL predictions as 'risk-off' filter only")
        print("   üí° When model predicts SELL: avoid new BUY entries\n")

        print("üìà Next Steps:")
        print("   1. ‚úÖ Start paper trading with CONSERVATIVE threshold")
        print("   2. ‚úÖ Track execution quality for 2 weeks")
        print("   3. ‚úÖ Adjust to BALANCED threshold once comfortable")
        print("   4. ‚úÖ Never trade SELL signals (use as filter only)")

    else:
        print("\n‚ùå MODEL NOT VIABLE FOR TRADING\n")
        print("Issues detected:")
        if signal_pct > 40:
            print(f"  ‚Ä¢ Raw signal rate too high: {signal_pct:.1f}% (target: <35%)")
        if buy_signal_count > 0 and buy_precision_raw < 0.25:
            print(f"  ‚Ä¢ Raw BUY precision too low: {buy_precision_raw*100:.1f}% (target: >25%)")
        print("\nüîß Recommended Actions:")
        print("  1. Increase precision_penalty to 25.0+")
        print("  2. Increase w_confidence to 0.5+")
        print("  3. Re-train and re-run this diagnostic")

    # ========================================================================
    # FINAL SUMMARY
    # ========================================================================
    print("\n" + "="*70)
    print("üìä FINAL SUMMARY")
    print("="*70)

    if viable_thresholds:
        print("\nüü¢ STATUS: PRODUCTION READY ‚úÖ")
        print(f"\n   Best threshold: {best_threshold:.2f}")
        best_config = [t for t in viable_thresholds if t['threshold'] == best_threshold][0]
        print(f"   BUY precision: {best_config['buy_precision']*100:.1f}%")
        print(f"   Expected value: +{best_config['buy_ev']:.3f}R")
        print(f"   Daily signals: ~{best_config['daily_trades']}")
        print("\n   ‚úÖ Proceed to paper trading")
        print("   ‚úÖ Use confidence filtering in production")
        print("   ‚ùå Ignore SELL signals\n")
    else:
        print("\nüî¥ STATUS: NEEDS RE-TRAINING ‚ùå")
        print("\n   Model does not achieve viable precision at any threshold.")
        print("   Re-train with adjusted hyperparameters.\n")

    print("="*70)

    return {
        'all_preds': all_preds,
        'all_labels': all_labels,
        'all_probs': all_probs,
        'viable_thresholds': viable_thresholds,
        'best_threshold': best_threshold
    }


# ============================================================================
# USAGE: Add this at the end of your training script
# ============================================================================
if __name__ == "__main__":
    # After training completes, run:
    results = analyze_model_predictions(model, val_loader)

!pip install onnxruntime

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Add this section to the bottom of your VWAP LSTM training script
to automatically export the trained model to ONNX format.

This reuses the model and configuration from training.
"""

# =========================================================
# ONNX EXPORT (Add to bottom of training script)
# =========================================================

def export_model_to_onnx(model, model_dir, config):
    """
    Export trained PyTorch model to ONNX format with validation and optimization

    Args:
        model: Trained PyTorch model (already in memory)
        model_dir: Directory to save ONNX files
        config: Dictionary with model configuration
    """
    import onnx
    import onnxruntime as ort
    import time

    print("\n" + "="*70)
    print("üöÄ EXPORTING MODEL TO ONNX")
    print("="*70)

    # ONNX export configuration
    onnx_config = {
        'filename': 'vwap_lstm_v1_0.onnx',
        'sequence_length': config.get('sequence_length', 45),
        'input_size': config.get('input_size', 33),
        'opset_version': 17,
        'dynamic_batch': True,
        'apply_temperature': True,
    }

    output_path = os.path.join(model_dir, onnx_config['filename'])

    print(f"\nüìã Export Configuration:")
    print(f"  Output: {onnx_config['filename']}")
    print(f"  Sequence length: {onnx_config['sequence_length']}")
    print(f"  Features: {onnx_config['input_size']}")
    print(f"  Opset version: {onnx_config['opset_version']}")
    print(f"  Dynamic batch: {onnx_config['dynamic_batch']}")

    # =========================================================
    # STEP 1: Export to ONNX
    # =========================================================

    print(f"\nüîÑ Step 1: Exporting to ONNX...")

    # Prepare model for export
    model.eval()
    model.cpu()

    # Create dummy input
    dummy_input = torch.randn(
        1,
        onnx_config['sequence_length'],
        onnx_config['input_size'],
        dtype=torch.float32
    )

    # Define dynamic axes
    dynamic_axes = None
    if onnx_config['dynamic_batch']:
        dynamic_axes = {
            'input': {0: 'batch_size'},
            'action_logits': {0: 'batch_size'}
        }

    # Export
    try:
        with torch.no_grad():
            torch.onnx.export(
                model,
                dummy_input,
                output_path,
                input_names=['input'],
                output_names=['action_logits'],
                dynamic_axes=dynamic_axes,
                opset_version=onnx_config['opset_version'],
                do_constant_folding=True,
                export_params=True,
                verbose=False
            )

        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"‚úÖ Export successful: {file_size:.2f} MB")

    except Exception as e:
        print(f"‚ùå Export failed: {e}")
        return None

    # =========================================================
    # STEP 2: Validate ONNX Model
    # =========================================================

    print(f"\nüîç Step 2: Validating ONNX model...")

    try:
        onnx_model = onnx.load(output_path)
        onnx.checker.check_model(onnx_model)
        print(f"‚úÖ ONNX model is valid")

        # Print model info
        graph = onnx_model.graph
        print(f"\nüìä Model Information:")
        print(f"  Opset Version: {onnx_model.opset_import[0].version}")

        for input_tensor in graph.input:
            shape = [dim.dim_value if dim.dim_value > 0 else 'dynamic'
                    for dim in input_tensor.type.tensor_type.shape.dim]
            print(f"  Input: {input_tensor.name} {shape}")

        for output_tensor in graph.output:
            shape = [dim.dim_value if dim.dim_value > 0 else 'dynamic'
                    for dim in output_tensor.type.tensor_type.shape.dim]
            print(f"  Output: {output_tensor.name} {shape}")

    except Exception as e:
        print(f"‚ö†Ô∏è  Validation warning: {e}")

    # =========================================================
    # STEP 3: Compare PyTorch vs ONNX
    # =========================================================

    print(f"\nüî¨ Step 3: Comparing PyTorch vs ONNX...")

    try:
        # PyTorch inference
        test_input = torch.randn(1, onnx_config['sequence_length'], onnx_config['input_size'])
        with torch.no_grad():
            pytorch_output = model(test_input, apply_temp=True)
        pytorch_logits = pytorch_output.numpy()

        # ONNX Runtime inference
        ort_session = ort.InferenceSession(output_path)
        onnx_outputs = ort_session.run(None, {'input': test_input.numpy()})
        onnx_logits = onnx_outputs[0]

        # Compare
        diff = np.abs(pytorch_logits - onnx_logits).max()

        print(f"  PyTorch shape: {pytorch_logits.shape}")
        print(f"  ONNX shape:    {onnx_logits.shape}")
        print(f"  Max diff:      {diff:.6e}")

        if diff < 1e-5:
            print(f"‚úÖ Outputs match perfectly!")
        elif diff < 1e-3:
            print(f"‚úÖ Outputs match well")
        else:
            print(f"‚ö†Ô∏è  Outputs differ (may be normal for LSTM)")

        # Test predictions
        pytorch_probs = torch.softmax(torch.from_numpy(pytorch_logits[0, 0]), dim=0).numpy()
        onnx_probs = np.exp(onnx_logits[0, 0]) / np.exp(onnx_logits[0, 0]).sum()

        pytorch_pred = pytorch_probs.argmax()
        onnx_pred = onnx_probs.argmax()

        if pytorch_pred == onnx_pred:
            print(f"‚úÖ Predictions match: {['HOLD', 'BUY', 'SELL'][pytorch_pred]}")
        else:
            print(f"‚ö†Ô∏è  Predictions differ: PyTorch={pytorch_pred}, ONNX={onnx_pred}")

    except Exception as e:
        print(f"‚ö†Ô∏è  Comparison failed: {e}")

    # =========================================================
    # STEP 4: Optimize ONNX Model
    # =========================================================

    print(f"\n‚ö° Step 4: Optimizing ONNX model...")

    optimized_path = output_path.replace('.onnx', '_optimized.onnx')

    try:
        # Try using onnxruntime optimizer
        from onnxruntime.transformers.optimizer import optimize_model

        optimized_model = optimize_model(
            output_path,
            model_type='bert',
            num_heads=0,
            hidden_size=config.get('hidden_size', 96)
        )
        optimized_model.save_model_to_file(optimized_path)

        original_size = os.path.getsize(output_path) / (1024 * 1024)
        optimized_size = os.path.getsize(optimized_path) / (1024 * 1024)
        reduction = (1 - optimized_size / original_size) * 100

        print(f"‚úÖ Optimization complete!")
        print(f"  Original:  {original_size:.2f} MB")
        print(f"  Optimized: {optimized_size:.2f} MB")
        print(f"  Reduction: {reduction:.1f}%")

        final_model_path = optimized_path

    except ImportError:
        print(f"‚ö†Ô∏è  onnxruntime.transformers not available")
        print(f"  Using standard ONNX model")
        final_model_path = output_path

    except Exception as e:
        print(f"‚ö†Ô∏è  Optimization failed: {e}")
        print(f"  Using standard ONNX model")
        final_model_path = output_path

    # =========================================================
    # STEP 5: Benchmark Inference Speed
    # =========================================================

    print(f"\n‚è±Ô∏è  Step 5: Benchmarking inference speed...")

    try:
        ort_session = ort.InferenceSession(final_model_path)

        # Create test input
        test_input = np.random.randn(
            1,
            onnx_config['sequence_length'],
            onnx_config['input_size']
        ).astype(np.float32)

        onnx_input = {'input': test_input}

        # Warmup
        for _ in range(50):
            ort_session.run(None, onnx_input)

        # Benchmark
        num_iterations = 1000
        start_time = time.time()
        for _ in range(num_iterations):
            ort_session.run(None, onnx_input)
        end_time = time.time()

        total_time = end_time - start_time
        avg_time = total_time / num_iterations * 1000  # ms
        throughput = num_iterations / total_time

        print(f"‚úÖ Benchmark complete!")
        print(f"  Iterations:  {num_iterations}")
        print(f"  Total time:  {total_time:.2f}s")
        print(f"  Average:     {avg_time:.3f}ms per inference")
        print(f"  Throughput:  {throughput:.1f} inferences/sec")

        # Compare to target
        if avg_time < 2.0:
            print(f"  ‚ö° Excellent! (< 2ms target)")
        elif avg_time < 5.0:
            print(f"  ‚úÖ Good! (< 5ms target)")
        else:
            print(f"  ‚ö†Ô∏è  Slower than expected (> 5ms)")

    except Exception as e:
        print(f"‚ö†Ô∏è  Benchmark failed: {e}")

    # =========================================================
    # STEP 6: Create Inference Example
    # =========================================================

    print(f"\nüìù Step 6: Creating inference example...")

    example_script = f"""#!/usr/bin/env python
# -*- coding: utf-8 -*-
\"\"\"
VWAP LSTM V1.0 - ONNX Inference Example
Auto-generated from training script
\"\"\"

import numpy as np
import onnxruntime as ort

# Load ONNX model
model_path = '{os.path.basename(final_model_path)}'
session = ort.InferenceSession(model_path)

print("Model loaded successfully!")
print(f"Input shape: {{session.get_inputs()[0].shape}}")
print(f"Output shape: {{session.get_outputs()[0].shape}}")

# Example: Create sample input
# Replace this with your actual preprocessed data!
sample_input = np.random.randn(1, {onnx_config['sequence_length']}, {onnx_config['input_size']}).astype(np.float32)

# IMPORTANT: Your real input MUST be:
# 1. Preprocessed exactly like training data
# 2. Clipped to [-10, 10] range
# 3. Shape: [batch, {onnx_config['sequence_length']}, {onnx_config['input_size']}]

# Run inference
outputs = session.run(None, {{'input': sample_input}})
action_logits = outputs[0]  # Shape: [batch, 1, 3]

# Convert to probabilities
logits = action_logits[0, 0]  # Shape: [3]
probs = np.exp(logits) / np.exp(logits).sum()

# Get prediction
prediction = probs.argmax()
confidence = probs.max()

# Map to action
action_map = {{0: 'HOLD', 1: 'BUY', 2: 'SELL'}}
action = action_map[prediction]

print(f"\\nPrediction: {{action}}")
print(f"Confidence: {{confidence:.3f}}")
print(f"Probabilities:")
print(f"  HOLD: {{probs[0]:.3f}}")
print(f"  BUY:  {{probs[1]:.3f}}")
print(f"  SELL: {{probs[2]:.3f}}")

# Apply confidence threshold (recommended: 0.55-0.65)
CONFIDENCE_THRESHOLD = 0.60

if confidence >= CONFIDENCE_THRESHOLD:
    print(f"\\n‚úÖ Signal: {{action}} (high confidence)")
else:
    print(f"\\n‚è∏Ô∏è  Signal: HOLD (low confidence, threshold={{CONFIDENCE_THRESHOLD}})")
"""

    example_path = os.path.join(model_dir, 'vwap_lstm_inference_example.py')

    try:
        with open(example_path, 'w') as f:
            f.write(example_script)
        print(f"‚úÖ Example script created: vwap_lstm_inference_example.py")
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to create example: {e}")

    # =========================================================
    # Summary
    # =========================================================

    print("\n" + "="*70)
    print("‚úÖ ONNX EXPORT COMPLETE")
    print("="*70)

    print(f"\nüìÅ Files created:")
    print(f"  Standard: {os.path.basename(output_path)}")
    if os.path.exists(optimized_path):
        print(f"  Optimized: {os.path.basename(optimized_path)} (recommended)")
    print(f"  Example: vwap_lstm_inference_example.py")

    print(f"\nüéØ Next steps:")
    print(f"  1. Test inference: python vwap_lstm_inference_example.py")
    print(f"  2. Integrate into your trading system")
    print(f"  3. Use {os.path.basename(final_model_path)} for production")

    print(f"\n‚ö° Performance: ~{avg_time:.2f}ms per inference" if 'avg_time' in locals() else "")

    print("\n" + "="*70)

    return final_model_path


# =========================================================
# RUN ONNX EXPORT (Add this after training completes)
# =========================================================

if __name__ == '__main__':
    # This block runs after training is complete
    # It uses the 'trained_model' variable from training

    print("\n\n")
    print("="*70)
    print("üîÑ POST-TRAINING: ONNX EXPORT")
    print("="*70)

    # Configuration for ONNX export
    onnx_export_config = {
        'sequence_length': SEQUENCE_LENGTH,
        'input_size': N_FEATURES,
        'hidden_size': TRAIN_PARAMS['hidden_size'],
    }

    # Export the trained model
    try:
        onnx_model_path = export_model_to_onnx(
            model=trained_model,
            model_dir=MODEL_DIR,
            config=onnx_export_config
        )

        if onnx_model_path:
            print(f"\nüéâ Success! ONNX model ready for deployment")
            print(f"üìç Location: {onnx_model_path}")
        else:
            print(f"\n‚ö†Ô∏è  ONNX export had issues, but training model is saved")

    except Exception as e:
        print(f"\n‚ùå ONNX export failed: {e}")
        print(f"   Training model is still saved at: {SAVE_MODEL_PATH}")
        print(f"   You can export manually later using the export script")

    print("\n" + "="*70)
    print("‚úÖ ALL TASKS COMPLETE")
    print("="*70)