{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Complete Pipeline\n",
    "\n",
    "**All-in-one notebook for training ML models on OHLCV data**\n",
    "\n",
    "## What This Notebook Does\n",
    "1. **Setup** - Clone repo, install dependencies, mount Drive\n",
    "2. **Load Data** - Load processed datasets from Drive\n",
    "3. **Train Models** - Train tabular models (XGBoost, LightGBM)\n",
    "4. **Evaluate** - Test set evaluation with trading metrics\n",
    "5. **Save** - Persist models to Google Drive\n",
    "\n",
    "## Prerequisites\n",
    "- Processed data in Google Drive at `ml_factory/data/processed/{SYMBOL}/`\n",
    "- Expected files: `train_scaled.parquet`, `val_scaled.parquet`, `test_scaled.parquet`\n",
    "\n",
    "## Expected Runtime\n",
    "- Model Training: 5-20 minutes per model\n",
    "- Total: ~30 minutes for 2 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Environment Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Snehpatel101/Research.git /content/research 2>/dev/null || echo \"Repo already exists\"\n",
    "%cd /content/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and mount Drive\n",
    "import sys\n",
    "sys.path.insert(0, '/content/research')\n",
    "\n",
    "from notebooks.colab_setup import setup_colab_environment\n",
    "\n",
    "env_info = setup_colab_environment(mount_drive=True, use_gpu=True)\n",
    "\n",
    "print(f\"\\nEnvironment ready!\")\n",
    "print(f\"  GPU: {env_info.get('gpu_available', False)}\")\n",
    "print(f\"  Drive: {env_info.get('drive_mounted', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THESE VALUES\n",
    "SYMBOL = \"MES\"      # Symbol (MES, MGC, etc.)\n",
    "HORIZON = 20        # Label horizon (5, 10, 15, 20)\n",
    "\n",
    "# Paths (adjust if your data is elsewhere)\n",
    "DATA_PATH = f\"/content/drive/MyDrive/ml_factory/data/processed/{SYMBOL}\"\n",
    "OUTPUT_DIR = \"/content/experiments\"\n",
    "MODELS_DIR = \"/content/drive/MyDrive/ml_factory/models\"\n",
    "\n",
    "print(f\"Config: {SYMBOL} @ horizon {HORIZON}\")\n",
    "print(f\"Data: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Verify Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check data files exist\n",
    "data_path = Path(DATA_PATH)\n",
    "required_files = [\"train_scaled.parquet\", \"val_scaled.parquet\", \"test_scaled.parquet\"]\n",
    "\n",
    "print(\"Checking data files...\")\n",
    "all_exist = True\n",
    "for f in required_files:\n",
    "    exists = (data_path / f).exists()\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  {f}: {status}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\nERROR: Missing data files!\")\n",
    "    print(f\"Please ensure processed data exists at: {DATA_PATH}\")\n",
    "    print(\"Run the data pipeline first or upload processed data to Drive.\")\n",
    "else:\n",
    "    print(\"\\nAll data files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect data\n",
    "from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "\n",
    "container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "    DATA_PATH,\n",
    "    horizon=HORIZON,\n",
    "    exclude_invalid_labels=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded: {container}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Train Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to train\n",
    "MODELS_TO_TRAIN = [\"xgboost\", \"lightgbm\"]\n",
    "\n",
    "print(f\"Will train: {MODELS_TO_TRAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.colab_setup import get_trainer_for_colab\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directories\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(MODELS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trained_models = {}\n",
    "results = {}\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        trainer, result = get_trainer_for_colab(\n",
    "            model_name=model_name,\n",
    "            horizon=HORIZON,\n",
    "            data_path=DATA_PATH,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "        )\n",
    "        \n",
    "        trained_models[model_name] = trainer\n",
    "        results[model_name] = result\n",
    "        \n",
    "        # Save to Drive\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = Path(MODELS_DIR) / f\"{model_name}_{SYMBOL}_h{HORIZON}_{timestamp}\"\n",
    "        trainer.model.save(model_path)\n",
    "        print(f\"Saved to: {model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        results[model_name] = {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"\\n{model_name}: FAILED - {result['error']}\")\n",
    "    else:\n",
    "        metrics = result.get('evaluation_metrics', {})\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Val Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "        print(f\"  Val F1: {metrics.get('macro_f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Test Set Evaluation (One-Shot)\n",
    "---\n",
    "\n",
    "**WARNING**: Test set evaluation is ONE-SHOT. Do not iterate on these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results are already computed by the trainer\n",
    "# Just display them\n",
    "\n",
    "print(\"TEST SET RESULTS (ONE-SHOT - DO NOT ITERATE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if \"error\" not in result:\n",
    "        test_metrics = result.get('test_metrics', {})\n",
    "        if test_metrics:\n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            print(f\"  Test Accuracy: {test_metrics.get('accuracy', 0):.4f}\")\n",
    "            print(f\"  Test F1: {test_metrics.get('macro_f1', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Save Summary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save summary to Drive\n",
    "summary = {\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"horizon\": HORIZON,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"models\": list(results.keys()),\n",
    "    \"results\": {k: str(v) for k, v in results.items()},\n",
    "}\n",
    "\n",
    "summary_path = Path(MODELS_DIR) / f\"summary_{SYMBOL}_h{HORIZON}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModels saved to: {MODELS_DIR}\")\n",
    "print(f\"Experiment outputs: {OUTPUT_DIR}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Train neural models (lstm, tcn, transformer)\")\n",
    "print(\"  2. Build ensemble with stacking\")\n",
    "print(\"  3. Run walk-forward validation\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
