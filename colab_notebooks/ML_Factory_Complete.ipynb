{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Complete Pipeline\n",
    "\n",
    "**Train ML models on your OHLCV data**\n",
    "\n",
    "## Instructions\n",
    "1. Run cells in order\n",
    "2. Configure your settings using the form fields on the right\n",
    "3. Models will be saved to your Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install & Setup\n",
    "!git clone https://github.com/Snehpatel101/Research.git /content/research 2>/dev/null || echo \"Repo exists\"\n",
    "%cd /content/research\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/research')\n",
    "\n",
    "from notebooks.colab_setup import setup_colab_environment\n",
    "env_info = setup_colab_environment(mount_drive=True, use_gpu=True)\n",
    "\n",
    "print(f\"\\nReady! GPU: {env_info.get('gpu_available', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configure Your Settings { run: \"auto\" }\n",
    "\n",
    "#@markdown ### Your Data\n",
    "SYMBOL = \"SL\" #@param {type:\"string\"}\n",
    "raw_data_filename = \"si_historical_2019_2024.parquet\" #@param {type:\"string\"}\n",
    "drive_data_folder = \"\" #@param {type:\"string\"}\n",
    "#@markdown *Leave drive_data_folder empty if file is in root of My Drive*\n",
    "\n",
    "#@markdown ### Training Settings\n",
    "HORIZON = 20 #@param [5, 10, 15, 20] {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Models to Train\n",
    "train_xgboost = True #@param {type:\"boolean\"}\n",
    "train_lightgbm = True #@param {type:\"boolean\"}\n",
    "train_catboost = False #@param {type:\"boolean\"}\n",
    "train_random_forest = False #@param {type:\"boolean\"}\n",
    "train_lstm = False #@param {type:\"boolean\"}\n",
    "train_tcn = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Build paths\n",
    "from pathlib import Path\n",
    "\n",
    "if drive_data_folder:\n",
    "    RAW_DATA_PATH = f\"/content/drive/MyDrive/{drive_data_folder}/{raw_data_filename}\"\n",
    "else:\n",
    "    RAW_DATA_PATH = f\"/content/drive/MyDrive/{raw_data_filename}\"\n",
    "\n",
    "PROCESSED_DIR = f\"/content/processed/{SYMBOL}\"\n",
    "MODELS_DIR = f\"/content/drive/MyDrive/ml_models/{SYMBOL}\"\n",
    "OUTPUT_DIR = \"/content/experiments\"\n",
    "\n",
    "# Build models list\n",
    "MODELS_TO_TRAIN = []\n",
    "if train_xgboost: MODELS_TO_TRAIN.append(\"xgboost\")\n",
    "if train_lightgbm: MODELS_TO_TRAIN.append(\"lightgbm\")\n",
    "if train_catboost: MODELS_TO_TRAIN.append(\"catboost\")\n",
    "if train_random_forest: MODELS_TO_TRAIN.append(\"random_forest\")\n",
    "if train_lstm: MODELS_TO_TRAIN.append(\"lstm\")\n",
    "if train_tcn: MODELS_TO_TRAIN.append(\"tcn\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"YOUR CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Symbol: {SYMBOL}\")\n",
    "print(f\"Raw data: {RAW_DATA_PATH}\")\n",
    "print(f\"Horizon: {HORIZON}\")\n",
    "print(f\"Models: {MODELS_TO_TRAIN}\")\n",
    "print(f\"Output: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Load & Check Your Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load Your Raw Data\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "raw_path = Path(RAW_DATA_PATH)\n",
    "\n",
    "if not raw_path.exists():\n",
    "    print(\"!\" * 50)\n",
    "    print(\"ERROR: Data file not found!\")\n",
    "    print(\"!\" * 50)\n",
    "    print(f\"\\nLooking for: {RAW_DATA_PATH}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  1. The filename is correct\")\n",
    "    print(\"  2. The folder path is correct\")\n",
    "    print(\"  3. Google Drive is mounted\")\n",
    "else:\n",
    "    print(f\"Loading {raw_path.name}...\")\n",
    "    df_raw = pd.read_parquet(raw_path)\n",
    "    \n",
    "    print(f\"\\nLoaded: {len(df_raw):,} rows\")\n",
    "    print(f\"Columns: {list(df_raw.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df_raw.head())\n",
    "    \n",
    "    print(f\"\\nDate range: {df_raw.index.min()} to {df_raw.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Prepare Data for Training\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "Path(PROCESSED_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Preparing data for training...\")\n",
    "print(\"This is a simplified pipeline for quick training.\")\n",
    "print()\n",
    "\n",
    "# Ensure we have the right columns\n",
    "required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Lowercase column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Check columns\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    print(f\"Missing columns: {missing}\")\n",
    "    print(f\"Available: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"Found OHLCV columns\")\n",
    "\n",
    "# Create basic features\n",
    "print(\"Creating features...\")\n",
    "df['return_1'] = df['close'].pct_change()\n",
    "df['return_5'] = df['close'].pct_change(5)\n",
    "df['return_10'] = df['close'].pct_change(10)\n",
    "df['return_20'] = df['close'].pct_change(20)\n",
    "\n",
    "df['volatility_20'] = df['return_1'].rolling(20).std()\n",
    "df['volatility_60'] = df['return_1'].rolling(60).std()\n",
    "\n",
    "df['sma_20'] = df['close'].rolling(20).mean()\n",
    "df['sma_60'] = df['close'].rolling(60).mean()\n",
    "df['price_sma20_ratio'] = df['close'] / df['sma_20']\n",
    "df['price_sma60_ratio'] = df['close'] / df['sma_60']\n",
    "\n",
    "df['volume_sma20'] = df['volume'].rolling(20).mean()\n",
    "df['volume_ratio'] = df['volume'] / df['volume_sma20']\n",
    "\n",
    "df['high_low_range'] = (df['high'] - df['low']) / df['close']\n",
    "df['close_open_range'] = (df['close'] - df['open']) / df['close']\n",
    "\n",
    "# RSI\n",
    "delta = df['close'].diff()\n",
    "gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "rs = gain / loss\n",
    "df['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Create labels (simple: up/down/neutral based on future returns)\n",
    "print(f\"Creating labels (horizon={HORIZON})...\")\n",
    "future_return = df['close'].shift(-HORIZON) / df['close'] - 1\n",
    "threshold = future_return.std() * 0.5\n",
    "\n",
    "df['label'] = 0  # neutral\n",
    "df.loc[future_return > threshold, 'label'] = 1   # long\n",
    "df.loc[future_return < -threshold, 'label'] = -1  # short\n",
    "\n",
    "# Drop NaN rows\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove last HORIZON rows (no label)\n",
    "df = df.iloc[:-HORIZON]\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts().sort_index())\n",
    "\n",
    "# Split data (70/15/15)\n",
    "print(\"\\nSplitting data...\")\n",
    "n = len(df)\n",
    "train_end = int(n * 0.7)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "train_df = df.iloc[:train_end]\n",
    "val_df = df.iloc[train_end:val_end]\n",
    "test_df = df.iloc[val_end:]\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Val: {len(val_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [\n",
    "    'return_1', 'return_5', 'return_10', 'return_20',\n",
    "    'volatility_20', 'volatility_60',\n",
    "    'price_sma20_ratio', 'price_sma60_ratio',\n",
    "    'volume_ratio', 'high_low_range', 'close_open_range', 'rsi_14'\n",
    "]\n",
    "\n",
    "# Scale features (train-only statistics)\n",
    "print(\"\\nScaling features...\")\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "train_features = scaler.fit_transform(train_df[feature_cols])\n",
    "val_features = scaler.transform(val_df[feature_cols])\n",
    "test_features = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "# Save as parquet\n",
    "print(\"\\nSaving processed data...\")\n",
    "\n",
    "train_scaled = pd.DataFrame(train_features, columns=feature_cols, index=train_df.index)\n",
    "train_scaled[f'label_h{HORIZON}'] = train_df['label'].values\n",
    "train_scaled.to_parquet(f\"{PROCESSED_DIR}/train_scaled.parquet\")\n",
    "\n",
    "val_scaled = pd.DataFrame(val_features, columns=feature_cols, index=val_df.index)\n",
    "val_scaled[f'label_h{HORIZON}'] = val_df['label'].values\n",
    "val_scaled.to_parquet(f\"{PROCESSED_DIR}/val_scaled.parquet\")\n",
    "\n",
    "test_scaled = pd.DataFrame(test_features, columns=feature_cols, index=test_df.index)\n",
    "test_scaled[f'label_h{HORIZON}'] = test_df['label'].values\n",
    "test_scaled.to_parquet(f\"{PROCESSED_DIR}/test_scaled.parquet\")\n",
    "\n",
    "print(f\"\\nData saved to: {PROCESSED_DIR}\")\n",
    "print(\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Train Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load Processed Data\n",
    "from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "\n",
    "print(f\"Loading processed data...\")\n",
    "container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "    PROCESSED_DIR,\n",
    "    horizon=HORIZON,\n",
    "    exclude_invalid_labels=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded: {container}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train Models\n",
    "from notebooks.colab_setup import get_trainer_for_colab\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(MODELS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not MODELS_TO_TRAIN:\n",
    "    print(\"No models selected! Go back and check at least one model.\")\n",
    "else:\n",
    "    trained_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    for i, model_name in enumerate(MODELS_TO_TRAIN):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[{i+1}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            trainer, result = get_trainer_for_colab(\n",
    "                model_name=model_name,\n",
    "                horizon=HORIZON,\n",
    "                data_path=PROCESSED_DIR,\n",
    "                output_dir=OUTPUT_DIR,\n",
    "            )\n",
    "            \n",
    "            trained_models[model_name] = trainer\n",
    "            results[model_name] = result\n",
    "            \n",
    "            # Save to Drive\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_path = Path(MODELS_DIR) / f\"{model_name}_{SYMBOL}_h{HORIZON}_{timestamp}\"\n",
    "            trainer.model.save(model_path)\n",
    "            print(f\"Saved: {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title View Results\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    if \"error\" in result:\n",
    "        print(f\"  FAILED: {result['error']}\")\n",
    "    else:\n",
    "        val_metrics = result.get('evaluation_metrics', {})\n",
    "        test_metrics = result.get('test_metrics', {})\n",
    "        \n",
    "        print(f\"  Validation Accuracy: {val_metrics.get('accuracy', 0):.4f}\")\n",
    "        print(f\"  Validation F1: {val_metrics.get('macro_f1', 0):.4f}\")\n",
    "        \n",
    "        if test_metrics:\n",
    "            print(f\"  Test Accuracy: {test_metrics.get('accuracy', 0):.4f}\")\n",
    "            print(f\"  Test F1: {test_metrics.get('macro_f1', 0):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Models saved to: {MODELS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
