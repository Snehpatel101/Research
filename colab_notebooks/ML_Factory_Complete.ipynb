{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Factory - Complete Pipeline\n",
    "\n",
    "**All-in-one notebook for training ML models on OHLCV data**\n",
    "\n",
    "## Instructions\n",
    "1. Run cells in order\n",
    "2. Use the form fields to configure settings (click the fields on the right)\n",
    "3. Models and data will be saved to your Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Clone Repository\n",
    "!git clone https://github.com/Snehpatel101/Research.git /content/research 2>/dev/null || echo \"Repo already exists\"\n",
    "%cd /content/research\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount Google Drive & Setup Environment\n",
    "import sys\n",
    "sys.path.insert(0, '/content/research')\n",
    "\n",
    "from notebooks.colab_setup import setup_colab_environment\n",
    "\n",
    "env_info = setup_colab_environment(mount_drive=True, use_gpu=True)\n",
    "\n",
    "print(f\"\\nEnvironment ready!\")\n",
    "print(f\"  GPU: {env_info.get('gpu_available', False)}\")\n",
    "print(f\"  Drive: {env_info.get('drive_mounted', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Configuration\n",
    "---\n",
    "\n",
    "**Click the form fields on the right to configure your settings!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configure Training Settings { run: \"auto\" }\n",
    "\n",
    "#@markdown ### Symbol & Data\n",
    "SYMBOL = \"MES\" #@param [\"MES\", \"MGC\", \"ES\", \"GC\", \"NQ\", \"CL\"] {allow-input: true}\n",
    "HORIZON = 20 #@param [5, 10, 15, 20] {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Models to Train\n",
    "train_xgboost = True #@param {type:\"boolean\"}\n",
    "train_lightgbm = True #@param {type:\"boolean\"}\n",
    "train_catboost = False #@param {type:\"boolean\"}\n",
    "train_random_forest = False #@param {type:\"boolean\"}\n",
    "train_lstm = False #@param {type:\"boolean\"}\n",
    "train_tcn = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### Paths (usually don't need to change)\n",
    "data_folder = \"ml_factory/data/processed\" #@param {type:\"string\"}\n",
    "models_folder = \"ml_factory/models\" #@param {type:\"string\"}\n",
    "\n",
    "# Build paths\n",
    "from pathlib import Path\n",
    "DATA_PATH = f\"/content/drive/MyDrive/{data_folder}/{SYMBOL}\"\n",
    "MODELS_DIR = f\"/content/drive/MyDrive/{models_folder}\"\n",
    "OUTPUT_DIR = \"/content/experiments\"\n",
    "\n",
    "# Build models list\n",
    "MODELS_TO_TRAIN = []\n",
    "if train_xgboost: MODELS_TO_TRAIN.append(\"xgboost\")\n",
    "if train_lightgbm: MODELS_TO_TRAIN.append(\"lightgbm\")\n",
    "if train_catboost: MODELS_TO_TRAIN.append(\"catboost\")\n",
    "if train_random_forest: MODELS_TO_TRAIN.append(\"random_forest\")\n",
    "if train_lstm: MODELS_TO_TRAIN.append(\"lstm\")\n",
    "if train_tcn: MODELS_TO_TRAIN.append(\"tcn\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Symbol: {SYMBOL}\")\n",
    "print(f\"Horizon: {HORIZON}\")\n",
    "print(f\"Models: {MODELS_TO_TRAIN}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Models path: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Verify Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check Data Files Exist\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(DATA_PATH)\n",
    "required_files = [\"train_scaled.parquet\", \"val_scaled.parquet\", \"test_scaled.parquet\"]\n",
    "\n",
    "print(\"Checking data files...\")\n",
    "print(f\"Looking in: {DATA_PATH}\")\n",
    "print()\n",
    "\n",
    "all_exist = True\n",
    "for f in required_files:\n",
    "    file_path = data_path / f\n",
    "    exists = file_path.exists()\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  {f}: {status}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\n\" + \"!\" * 50)\n",
    "    print(\"ERROR: Missing data files!\")\n",
    "    print(\"!\" * 50)\n",
    "    print(f\"\\nPlease upload your processed data to:\")\n",
    "    print(f\"  Google Drive > {data_folder}/{SYMBOL}/\")\n",
    "    print(f\"\\nRequired files:\")\n",
    "    for f in required_files:\n",
    "        print(f\"  - {f}\")\n",
    "else:\n",
    "    print(\"\\nAll data files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load Data\n",
    "from src.phase1.stages.datasets.container import TimeSeriesDataContainer\n",
    "\n",
    "print(f\"Loading data from {DATA_PATH}...\")\n",
    "container = TimeSeriesDataContainer.from_parquet_dir(\n",
    "    DATA_PATH,\n",
    "    horizon=HORIZON,\n",
    "    exclude_invalid_labels=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded: {container}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Train Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train Selected Models\n",
    "from notebooks.colab_setup import get_trainer_for_colab\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directories\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(MODELS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not MODELS_TO_TRAIN:\n",
    "    print(\"No models selected! Go back to Configuration and select at least one model.\")\n",
    "else:\n",
    "    trained_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    for i, model_name in enumerate(MODELS_TO_TRAIN):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[{i+1}/{len(MODELS_TO_TRAIN)}] Training: {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            trainer, result = get_trainer_for_colab(\n",
    "                model_name=model_name,\n",
    "                horizon=HORIZON,\n",
    "                data_path=DATA_PATH,\n",
    "                output_dir=OUTPUT_DIR,\n",
    "            )\n",
    "            \n",
    "            trained_models[model_name] = trainer\n",
    "            results[model_name] = result\n",
    "            \n",
    "            # Save to Drive\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_path = Path(MODELS_DIR) / f\"{model_name}_{SYMBOL}_h{HORIZON}_{timestamp}\"\n",
    "            trainer.model.save(model_path)\n",
    "            print(f\"Saved to: {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title View Results Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    if \"error\" in result:\n",
    "        print(f\"  Status: FAILED\")\n",
    "        print(f\"  Error: {result['error']}\")\n",
    "    else:\n",
    "        val_metrics = result.get('evaluation_metrics', {})\n",
    "        test_metrics = result.get('test_metrics', {})\n",
    "        \n",
    "        print(f\"  Status: SUCCESS\")\n",
    "        print(f\"  Validation:\")\n",
    "        print(f\"    Accuracy: {val_metrics.get('accuracy', 0):.4f}\")\n",
    "        print(f\"    F1 Score: {val_metrics.get('macro_f1', 0):.4f}\")\n",
    "        \n",
    "        if test_metrics:\n",
    "            print(f\"  Test (one-shot):\")\n",
    "            print(f\"    Accuracy: {test_metrics.get('accuracy', 0):.4f}\")\n",
    "            print(f\"    F1 Score: {test_metrics.get('macro_f1', 0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Models saved to: {MODELS_DIR}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Save Summary to Drive\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary = {\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"horizon\": HORIZON,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"models\": list(results.keys()),\n",
    "}\n",
    "\n",
    "# Add metrics for successful models\n",
    "for model_name, result in results.items():\n",
    "    if \"error\" not in result:\n",
    "        summary[f\"{model_name}_val_f1\"] = result.get('evaluation_metrics', {}).get('macro_f1', 0)\n",
    "        summary[f\"{model_name}_test_f1\"] = result.get('test_metrics', {}).get('macro_f1', 0)\n",
    "\n",
    "summary_path = Path(MODELS_DIR) / f\"summary_{SYMBOL}_h{HORIZON}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary saved to: {summary_path}\")\n",
    "print(\"\\nDone! Check your Google Drive for saved models.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
