{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Tabular Models (XGBoost, LightGBM, CatBoost)\n",
    "\n",
    "**Purpose:** Train tabular models on processed datasets.\n",
    "\n",
    "**Models:**\n",
    "- XGBoost (gradient boosting)\n",
    "- LightGBM (fast gradient boosting)\n",
    "- CatBoost (categorical boosting)\n",
    "\n",
    "**Expected Runtime:** 20-40 minutes per model\n",
    "\n",
    "**Key Features:**\n",
    "- Auto-checkpointing every 30 minutes\n",
    "- W&B experiment tracking\n",
    "- Resume from last epoch on disconnect\n",
    "- GPU acceleration (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup Colab environment\n# Note: You can use either the scaffolded utils or the working notebooks/colab_setup.py\n\n# Option 1: Use the working colab_setup module\nimport sys\nsys.path.insert(0, '/content/research')\n\nfrom notebooks.colab_setup import setup_colab_environment, is_colab\n\nenv_info = setup_colab_environment(\n    mount_drive=True,\n    use_gpu=True,\n)\n\nprint(f\"\\nüìä Environment Info:\")\nprint(f\"  Running in Colab: {env_info.get('is_colab', False)}\")\nprint(f\"  GPU available: {env_info.get('gpu_available', False)}\")\nprint(f\"  Drive mounted: {env_info.get('drive_mounted', False)}\")\n\n# Estimate remaining time (Colab sessions are ~12 hours)\nimport time\nsession_start = time.time()\nprint(f\"\\n‚è±Ô∏è  Session started - you have approximately 12 hours\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Checkpoint Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nSYMBOL = \"MES\"\nHORIZON = 20\nMODEL = \"xgboost\"  # or \"lightgbm\", \"catboost\"\n\n# Simple checkpoint manager class (no external dependencies)\nclass SimpleCheckpointManager:\n    \"\"\"Minimal checkpoint manager that works without W&B.\"\"\"\n    \n    def __init__(self, drive_path):\n        self.drive_path = Path(drive_path)\n        self.drive_path.mkdir(parents=True, exist_ok=True)\n        self.wandb_run = None\n        \n    def log_metrics(self, metrics, step=None):\n        \"\"\"Log metrics (print if W&B not available).\"\"\"\n        if self.wandb_run:\n            import wandb\n            wandb.log(metrics, step=step)\n        else:\n            print(f\"Metrics: {metrics}\")\n    \n    def finish_wandb_run(self):\n        \"\"\"Finish W&B run if active.\"\"\"\n        if self.wandb_run:\n            self.wandb_run.finish()\n\n# Initialize simple checkpoint manager\nckpt_mgr = SimpleCheckpointManager(\n    drive_path=\"/content/drive/MyDrive/ml_factory/checkpoints\"\n)\n\nprint(f\"‚úÖ Checkpoint manager initialized\")\nprint(f\"   Checkpoint dir: {ckpt_mgr.drive_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Datasets from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "DRIVE_DATA = Path(\"/content/drive/MyDrive/ml_factory/data/processed\") / SYMBOL\n",
    "LOCAL_DATA = Path(\"/content/data/splits/scaled\")\n",
    "LOCAL_DATA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy data from Drive to local (faster I/O during training)\n",
    "import shutil\n",
    "if not (LOCAL_DATA / \"X_train.npy\").exists():\n",
    "    print(\"Copying data from Drive to local disk...\")\n",
    "    shutil.copytree(DRIVE_DATA, LOCAL_DATA, dirs_exist_ok=True)\n",
    "    print(\"‚úÖ Data copied to local disk\")\n",
    "\n",
    "# Load data\n",
    "X_train = np.load(LOCAL_DATA / \"X_train.npy\")\n",
    "y_train = np.load(LOCAL_DATA / \"y_train.npy\")\n",
    "X_val = np.load(LOCAL_DATA / \"X_val.npy\")\n",
    "y_val = np.load(LOCAL_DATA / \"y_val.npy\")\n",
    "X_test = np.load(LOCAL_DATA / \"X_test.npy\")\n",
    "y_test = np.load(LOCAL_DATA / \"y_test.npy\")\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Val: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Existing Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for existing trained model (to resume/skip training)\nfrom pathlib import Path\n\nmodel_checkpoint_dir = Path(\"/content/drive/MyDrive/ml_factory/checkpoints/models\")\nmodel_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\ncheckpoint_path = model_checkpoint_dir / f\"{MODEL}_{SYMBOL}_h{HORIZON}_checkpoint.pkl\"\n\nif checkpoint_path.exists():\n    print(f\"\\n‚úÖ Found model checkpoint: {checkpoint_path}\")\n    print(\"You can load this model and skip training, or retrain from scratch\")\n    resume_training = False  # Set to True if you want to use the saved model\nelse:\n    print(\"\\nüÜï No checkpoint found - will train from scratch\")\n    resume_training = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with Auto-Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.models import ModelRegistry\n\n# Initialize model\nmodel_class = ModelRegistry.get(MODEL)\nmodel = model_class()\n\n# Load from checkpoint if resuming\nif resume_training and checkpoint_path.exists():\n    model.load(checkpoint_path)\n    print(f\"‚úÖ Model loaded from checkpoint: {checkpoint_path}\")\n    print(\"Skipping training - model already trained\")\nelse:\n    # Training configuration\n    config = {\n        \"n_estimators\": 1000,\n        \"learning_rate\": 0.05,\n        \"max_depth\": 8,\n        \"early_stopping_rounds\": 50,\n        \"use_gpu\": env_info.get('gpu_info', {}).get('available', False),\n    }\n    \n    # Train model\n    # Note: BaseModel.fit() does NOT support callbacks parameter\n    # Checkpointing must be done manually after training completes\n    try:\n        print(f\"\\nüöÄ Training {MODEL}...\")\n        training_metrics = model.fit(\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n            config=config,\n        )\n        \n        print(\"\\n‚úÖ Training completed successfully!\")\n        print(f\"Training metrics: {training_metrics}\")\n        \n        # Save checkpoint to Drive immediately after training\n        model.save(checkpoint_path)\n        print(f\"üíæ Model checkpoint saved to: {checkpoint_path}\")\n        \n        # Log to W&B if available\n        if hasattr(ckpt_mgr, 'log_metrics'):\n            ckpt_mgr.log_metrics({\n                \"train_loss\": training_metrics.train_loss,\n                \"val_loss\": training_metrics.val_loss,\n                \"val_accuracy\": training_metrics.val_accuracy,\n            })\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Training failed: {e}\")\n        raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Generate predictions\n# model.predict() returns a PredictionOutput object with:\n# - predictions: class predictions (alias: class_predictions)\n# - probabilities: class probabilities\n# - confidence: prediction confidence scores\npred_output = model.predict(X_test)\ny_pred = pred_output.class_predictions  # or pred_output.predictions\ny_proba = pred_output.probabilities\n\n# Calculate metrics\ntest_metrics = {\n    \"test_accuracy\": accuracy_score(y_test, y_pred),\n    \"test_precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n    \"test_recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n    \"test_f1\": f1_score(y_test, y_pred, average='weighted', zero_division=0),\n}\n\nprint(\"\\nTest Set Metrics:\")\nfor metric, value in test_metrics.items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Log to W&B if available\nif hasattr(ckpt_mgr, 'log_metrics'):\n    ckpt_mgr.log_metrics(test_metrics)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model to Drive and W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define paths\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"{MODEL}_{SYMBOL}_h{HORIZON}_{timestamp}\"\n",
    "\n",
    "DRIVE_MODELS = Path(\"/content/drive/MyDrive/ml_factory/models\")\n",
    "DRIVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = DRIVE_MODELS / f\"{model_name}.pkl\"\n",
    "\n",
    "# Save model to Drive\n",
    "model.save(model_path)\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "\n",
    "# Upload to W&B\n",
    "if ckpt_mgr.wandb_run:\n",
    "    artifact = ckpt_mgr.wandb.Artifact(\n",
    "        name=model_name,\n",
    "        type=\"model\",\n",
    "        metadata={\"symbol\": SYMBOL, \"horizon\": HORIZON, \"model\": MODEL, **test_metrics},\n",
    "    )\n",
    "    artifact.add_file(str(model_path))\n",
    "    ckpt_mgr.wandb_run.log_artifact(artifact)\n",
    "    print(\"‚òÅÔ∏è  Model uploaded to W&B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish W&B Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_mgr.finish_wandb_run()\n",
    "print(\"\\n‚úÖ Training complete! Model saved and logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Train other tabular models (LightGBM, CatBoost)\n",
    "2. Proceed to sequence models (LSTM, GRU, TCN)\n",
    "3. Build heterogeneous ensemble with stacking meta-learner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}