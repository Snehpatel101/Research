{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline (Phases 1-5)\n",
    "\n",
    "**Purpose:** Run the complete data pipeline from raw OHLCV to model-ready datasets.\n",
    "\n",
    "**Phases:**\n",
    "1. Ingestion: Load raw 1-min OHLCV\n",
    "2. MTF Upscaling: Resample to 8 intraday timeframes\n",
    "3. Features: 180+ indicators (momentum, wavelets, microstructure)\n",
    "4. Labeling: Triple-barrier with Optuna optimization\n",
    "5. Adapters: Model-family data preparation (2D, 3D)\n",
    "\n",
    "**Outputs:** Processed datasets saved to Google Drive\n",
    "\n",
    "**Expected Runtime:** 30-60 minutes (depending on symbol and data size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup Colab environment\nimport sys\nsys.path.insert(0, '/content/research')\n\nfrom notebooks.colab_setup import setup_colab_environment, is_colab\n\nenv_info = setup_colab_environment(\n    mount_drive=True,\n    use_gpu=True,\n)\n\nprint(f\"\\nüìä Environment Info:\")\nprint(f\"  Running in Colab: {env_info.get('is_colab', False)}\")\nprint(f\"  GPU available: {env_info.get('gpu_available', False)}\")\nprint(f\"  Drive mounted: {env_info.get('drive_mounted', False)}\")\n\n# Check disk space\nimport shutil\ntotal, used, free = shutil.disk_usage(\"/content\")\nprint(f\"\\nüíæ Disk Space:\")\nprint(f\"  Total: {total // (1024**3)} GB\")\nprint(f\"  Used: {used // (1024**3)} GB\")\nprint(f\"  Free: {free // (1024**3)} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Checkpoint Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple checkpoint tracking (Pipeline has built-in checkpointing)\nfrom pathlib import Path\n\n# Define checkpoint directory on Drive for persistence\nCHECKPOINT_DIR = Path(\"/content/drive/MyDrive/ml_factory/checkpoints\")\nCHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Simple metrics logger\nclass SimpleLogger:\n    def log_metrics(self, metrics):\n        print(f\"üìä Metrics: {metrics}\")\n    \n    def finish_wandb_run(self):\n        pass\n\nckpt_mgr = SimpleLogger()\nprint(f\"‚úÖ Checkpoint directory: {CHECKPOINT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "SYMBOL = \"MES\"\n",
    "DRIVE_DATA_PATH = Path(\"/content/drive/MyDrive/ml_factory/data/raw\")\n",
    "LOCAL_DATA_PATH = Path(\"/content/data/raw\")\n",
    "LOCAL_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy data from Drive to local disk (faster I/O)\n",
    "raw_data_file = DRIVE_DATA_PATH / f\"{SYMBOL}_1m.parquet\"\n",
    "local_data_file = LOCAL_DATA_PATH / f\"{SYMBOL}_1m.parquet\"\n",
    "\n",
    "if not local_data_file.exists():\n",
    "    print(f\"Copying data from Drive to local disk...\")\n",
    "    import shutil\n",
    "    shutil.copy(raw_data_file, local_data_file)\n",
    "    print(f\"‚úÖ Data copied to {local_data_file}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Data already exists at {local_data_file}\")\n",
    "\n",
    "# Load data\n",
    "df_raw = pd.read_parquet(local_data_file)\n",
    "print(f\"\\nRaw data shape: {df_raw.shape}\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Existing Checkpoint (Resume if Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Note: PipelineRunner has built-in checkpointing\n# It automatically saves state after each stage to config.checkpoint_dir\n# When you create PipelineRunner with resume=True, it loads the last checkpoint\n\n# For manual checkpoint inspection:\nfrom pathlib import Path\nimport json\n\ncheckpoint_file = Path(\"/content/output/pipeline_state.json\")\nif checkpoint_file.exists():\n    with open(checkpoint_file) as f:\n        checkpoint = json.load(f)\n    print(f\"\\n‚úÖ Found checkpoint from: {checkpoint.get('timestamp', 'unknown')}\")\n    print(f\"Completed stages: {checkpoint.get('completed_stages', [])}\")\n    last_completed_phase = len(checkpoint.get('completed_stages', []))\nelse:\n    print(\"\\nüÜï No checkpoint found - starting from scratch\")\n    last_completed_phase = 0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline (with Checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import pipeline runner\nfrom src.pipeline.runner import PipelineRunner\nfrom src.pipeline.config import PipelineConfig\n\n# Configure pipeline\nconfig = PipelineConfig(\n    symbols=[SYMBOL],\n    data_dir=Path(\"/content/data\"),\n    output_dir=Path(\"/content/output\"),\n)\n\n# Initialize pipeline runner (resume=True to continue from last checkpoint)\nrunner = PipelineRunner(config, resume=True)\n\n# Run pipeline\n# Note: Pipeline has built-in checkpointing via _save_state()/_load_state()\n# It automatically saves after each stage and can resume from last completed stage\ntry:\n    # from_stage parameter accepts stage name (string), not phase number\n    # If resuming, leave from_stage=None to auto-resume from last completed stage\n    from_stage = None  # Auto-resume from checkpoint\n    \n    success = runner.run(from_stage=from_stage)\n    \n    if success:\n        print(\"\\n‚úÖ Pipeline completed successfully!\")\n        print(f\"Completed stages: {runner.get_completed_stages()}\")\n    else:\n        print(\"\\n‚ö†Ô∏è Pipeline completed with some issues\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Pipeline failed: {e}\")\n    print(f\"Completed stages before failure: {runner.get_completed_stages()}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Results to Google Drive (Permanent Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "LOCAL_OUTPUT = Path(\"/content/data/splits/scaled\")\n",
    "DRIVE_OUTPUT = Path(\"/content/drive/MyDrive/ml_factory/data/processed\")\n",
    "DRIVE_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy processed datasets to Drive\n",
    "print(\"Copying processed datasets to Google Drive...\")\n",
    "shutil.copytree(LOCAL_OUTPUT, DRIVE_OUTPUT / SYMBOL, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Processed datasets saved to: {DRIVE_OUTPUT / SYMBOL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish W&B Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Log final metrics\nckpt_mgr.log_metrics({\n    \"pipeline_status\": \"completed\",\n    \"num_samples\": len(df_raw),\n    \"symbol\": SYMBOL,\n})\n\nprint(\"\\n‚úÖ Data pipeline complete!\")\nprint(\"üìÅ Processed datasets saved to Google Drive\")\nprint(\"\\nüöÄ Next: Run 02_train_tabular.ipynb to train models\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}