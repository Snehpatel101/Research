{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline (Phases 1-5)\n",
    "\n",
    "**Purpose:** Run the complete data pipeline from raw OHLCV to model-ready datasets.\n",
    "\n",
    "**Phases:**\n",
    "1. Ingestion: Load raw 1-min OHLCV\n",
    "2. MTF Upscaling: Resample to 8 intraday timeframes\n",
    "3. Features: 180+ indicators (momentum, wavelets, microstructure)\n",
    "4. Labeling: Triple-barrier with Optuna optimization\n",
    "5. Adapters: Model-family data preparation (2D, 3D)\n",
    "\n",
    "**Outputs:** Processed datasets saved to Google Drive\n",
    "\n",
    "**Expected Runtime:** 30-60 minutes (depending on symbol and data size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-setup (mount Drive, clone repo, install deps)\n",
    "from utils.colab_setup import setup_colab_environment, check_disk_space, estimate_training_time_remaining\n",
    "\n",
    "env_info = setup_colab_environment(\n",
    "    repo_url=\"https://github.com/yourusername/ml-factory.git\",\n",
    "    drive_mount_point=\"/content/drive\",\n",
    "    wandb_project=\"ohlcv-ml-factory\",\n",
    "    install_extra_deps=True,\n",
    ")\n",
    "\n",
    "# Check resources\n",
    "disk = check_disk_space()\n",
    "print(f\"üíæ Available disk space: {disk.get('available', 'unknown')}\")\n",
    "\n",
    "remaining = estimate_training_time_remaining()\n",
    "print(f\"‚è±Ô∏è  Estimated time remaining: {remaining:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Checkpoint Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.checkpoint_manager import CheckpointManager\n",
    "\n",
    "# Initialize checkpoint manager (saves to Drive every 30 min)\n",
    "ckpt_mgr = CheckpointManager(\n",
    "    drive_path=\"/content/drive/MyDrive/ml_factory/checkpoints\",\n",
    "    wandb_project=\"ohlcv-ml-factory\",\n",
    "    auto_save_interval=1800,  # 30 minutes\n",
    "    max_checkpoints=3,\n",
    ")\n",
    "\n",
    "# Initialize W&B run\n",
    "ckpt_mgr.init_wandb_run(\n",
    "    run_name=\"data_pipeline_MES\",\n",
    "    config={\"symbol\": \"MES\", \"pipeline_version\": \"v1\"},\n",
    "    tags=[\"data_pipeline\", \"MES\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "SYMBOL = \"MES\"\n",
    "DRIVE_DATA_PATH = Path(\"/content/drive/MyDrive/ml_factory/data/raw\")\n",
    "LOCAL_DATA_PATH = Path(\"/content/data/raw\")\n",
    "LOCAL_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy data from Drive to local disk (faster I/O)\n",
    "raw_data_file = DRIVE_DATA_PATH / f\"{SYMBOL}_1m.parquet\"\n",
    "local_data_file = LOCAL_DATA_PATH / f\"{SYMBOL}_1m.parquet\"\n",
    "\n",
    "if not local_data_file.exists():\n",
    "    print(f\"Copying data from Drive to local disk...\")\n",
    "    import shutil\n",
    "    shutil.copy(raw_data_file, local_data_file)\n",
    "    print(f\"‚úÖ Data copied to {local_data_file}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Data already exists at {local_data_file}\")\n",
    "\n",
    "# Load data\n",
    "df_raw = pd.read_parquet(local_data_file)\n",
    "print(f\"\\nRaw data shape: {df_raw.shape}\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Existing Checkpoint (Resume if Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to resume from checkpoint\n",
    "checkpoint = ckpt_mgr.load_latest_checkpoint(phase=\"data_pipeline\")\n",
    "\n",
    "if checkpoint:\n",
    "    print(f\"\\n‚úÖ Resuming from checkpoint: {checkpoint['timestamp']}\")\n",
    "    last_completed_phase = checkpoint['state'].get('last_phase', 0)\n",
    "    print(f\"Last completed phase: {last_completed_phase}\")\n",
    "else:\n",
    "    print(\"\\nüÜï No checkpoint found - starting from scratch\")\n",
    "    last_completed_phase = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline (with Checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline runner\n",
    "from src.pipeline.runner import PipelineRunner\n",
    "from src.pipeline.config import PipelineConfig\n",
    "\n",
    "# Configure pipeline\n",
    "config = PipelineConfig(\n",
    "    symbols=[SYMBOL],\n",
    "    data_dir=Path(\"/content/data\"),\n",
    "    output_dir=Path(\"/content/output\"),\n",
    "    checkpoint_dir=Path(\"/content/drive/MyDrive/ml_factory/checkpoints/pipeline\"),\n",
    ")\n",
    "\n",
    "# Initialize pipeline runner\n",
    "runner = PipelineRunner(config)\n",
    "\n",
    "# Run pipeline with auto-checkpointing\n",
    "try:\n",
    "    result = runner.run(\n",
    "        start_phase=last_completed_phase + 1,  # Resume from next phase\n",
    "        checkpoint_callback=lambda phase, state: ckpt_mgr.save_checkpoint(\n",
    "            phase=\"data_pipeline\",\n",
    "            state={\"last_phase\": phase, **state},\n",
    "            metadata={\"symbol\": SYMBOL},\n",
    "            force=True,  # Force save after each phase\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "    print(f\"Results: {result}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Pipeline failed: {e}\")\n",
    "    # Save checkpoint on failure\n",
    "    ckpt_mgr.save_checkpoint(\n",
    "        phase=\"data_pipeline\",\n",
    "        state={\"error\": str(e), \"last_phase\": runner.current_phase},\n",
    "        metadata={\"symbol\": SYMBOL, \"status\": \"failed\"},\n",
    "        force=True,\n",
    "    )\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Results to Google Drive (Permanent Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "LOCAL_OUTPUT = Path(\"/content/data/splits/scaled\")\n",
    "DRIVE_OUTPUT = Path(\"/content/drive/MyDrive/ml_factory/data/processed\")\n",
    "DRIVE_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy processed datasets to Drive\n",
    "print(\"Copying processed datasets to Google Drive...\")\n",
    "shutil.copytree(LOCAL_OUTPUT, DRIVE_OUTPUT / SYMBOL, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Processed datasets saved to: {DRIVE_OUTPUT / SYMBOL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish W&B Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log final metrics\n",
    "ckpt_mgr.log_metrics({\n",
    "    \"pipeline_status\": \"completed\",\n",
    "    \"num_samples\": len(df_raw),\n",
    "    \"symbol\": SYMBOL,\n",
    "})\n",
    "\n",
    "# Finish W&B run\n",
    "ckpt_mgr.finish_wandb_run()\n",
    "\n",
    "print(\"\\n‚úÖ Data pipeline complete! Proceed to model training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
